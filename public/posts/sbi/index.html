<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Likelihood-free MCMC with Amortized Ratio Estimator | iamtu</title>
<meta name=keywords content="sbi"><meta name=description content="Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper."><meta name=author content="Tu T. Do"><link rel=canonical href=http://localhost:1313/posts/sbi/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/sbi/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PWLR4FLELZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PWLR4FLELZ")}</script><meta property="og:title" content="Likelihood-free MCMC with Amortized Ratio Estimator"><meta property="og:description" content="Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/sbi/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-10-31T00:00:00+00:00"><meta property="article:modified_time" content="2023-10-31T00:00:00+00:00"><meta property="og:site_name" content="iamtu"><meta name=twitter:card content="summary"><meta name=twitter:title content="Likelihood-free MCMC with Amortized Ratio Estimator"><meta name=twitter:description content="Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Likelihood-free MCMC with Amortized Ratio Estimator","item":"http://localhost:1313/posts/sbi/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Likelihood-free MCMC with Amortized Ratio Estimator","name":"Likelihood-free MCMC with Amortized Ratio Estimator","description":"Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper.","keywords":["sbi"],"articleBody":"Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper. In the analogy above, the black box represents the simulator, and the configurations are the simulatorâ€™s parameters.\nThe applicability of SBI has great potential since we can almost reduce any process with defined input and output to a black-box machine 1.\nThis post documents my notes while studying Likelihood-free MCMC with Amortized Ratio Estimator (Hermans et al, 2020); a method developed to address SBI.\nLikelihood-free MCMC with Amortized Ratio Estimator Likelihood ratio is defined as the ratio between the likelihood of the observation between two different hypothesis:\n$$ r(\\mathbf{x} | \\theta_0, \\theta_1) = \\frac{p(\\mathbf{x} | \\theta_0)}{p(\\mathbf{x}|\\theta_1)} $$\nThis quantity then can be used in various methods to draw sample from a distribution. In the paper, the author mention three sampling methods, namely Markov Chain Monte Carlo, Metropolis-Hasting, and HMC. In the following section, I am briefly summarizing those methods.\nBackground Markov Chain Monte Carlo (MCMC) In statistics, the MCMC method is a class of algorithms for sampling from a probability distribution. By constructing a Markov chain with the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the state chain 2.\nAdapting MCMC for SBI task We want to sample from \\(p(\\theta | \\mathbf{x})\\) using MCMC, we need this quantity\n$$ \\begin{equation} \\begin{aligned} \\frac{p(\\theta | \\mathbf{x} )}{p(\\theta_t| \\mathbf{x})} = \\frac{ p(\\theta)p(\\mathbf{x} | \\theta)/p(\\mathbf{x}) }{ p(\\theta_t)p(\\mathbf{x} | \\theta_t)/p(\\mathbf{x}) } = \\frac{p(\\theta)}{p(\\theta_t)}\\times \\frac{p(\\mathbf{x} | \\theta)}{p(\\mathbf{x} | \\theta_t)} = \\frac{p(\\theta)}{p(\\theta_t)} \\times r(\\mathbf{x} | \\theta, \\theta_t) \\end{aligned} \\end{equation} $$\nWe can compute the first term of the equation since we have access to prior \\(p(\\theta)\\). But we can not compute the second term because we do not have access to the likelihood function \\(p(\\mathbf{x} | \\theta)\\). However, we can reframe the problem in the supervised-learning paradigm, so we can use a parameterized discriminator \\(d_\\theta(\\mathbf{x})\\) to estimate the likelihood. The details are described in the Likelihood Ratio Estimator section.\nMetropolis-Hasting (MH) tbd\nHalmitonian Markov Chain(MH) tbd\nLikelihood Ratio Estimator The remaining question is how to estimate the likelihood ratio \\( r(\\mathbf{x} | \\theta_0, \\theta_1)\\). To estimate the ratio, the author employed the Likelihood Ratio Trick, training a discriminator \\(d_\\phi(\\mathbf{x})\\) to classify samples \\( x \\sim p(\\mathbf{x} | \\theta_0)\\) with class label \\(y = 1\\) from \\(\\mathbf{x} \\sim p(\\mathbf{x} | \\theta_1)\\) with class label \\(y = 0\\). The decision function obtained by the trained discrimininator:\n$$ d^*(\\mathbf{x}) = p(y = 1 | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\theta_0)}{p(\\mathbf{x} | \\theta_0) + p(\\mathbf{x} | \\theta_1)} $$\nThen the estimation of likelihood ratio can be computed by:\n$$ \\hat{r}(\\mathbf{x} | \\theta_0, \\theta_1) = \\frac {d^{*}(\\mathbf{x})} {1 - d^{*}(\\mathbf{x})} $$\nHowever, this method required the discriminator to be trained at every pair of \\((\\theta_0, \\theta_1)\\), which is impractical in the context. To overcome this issue, the paper proposed to train the discriminator to classify dependent sample-parameter pairs \\((\\mathbf{x}, \\mathbf{\\theta}) \\sim p(\\mathbf{x}, \\mathbf{\\theta})\\) with label \\(y=1\\) from the independent sample-parameter pairs \\((\\mathbf{x}, \\mathbf{\\theta}) \\sim p(\\mathbf{x})p(\\mathbf{\\theta})\\) with label \\(y=0\\).\n$$ \\begin{equation} \\begin{aligned} d^*(\\mathbf{x}, \\mathbf{\\theta}) \u0026= \\frac {p(\\mathbf{x}, \\mathbf{\\theta})} { p(\\mathbf{x}, \\mathbf{\\theta}) + p(\\mathbf{x}) p(\\mathbf{\\theta}) } \\ \\end{aligned} \\end{equation} $$\nThe likelihood-to-evidence ratio is computed by\n$$ r(\\mathbf{x} | \\theta) = \\frac {p(\\mathbf{x} | \\theta)} {p(x)} = \\frac{p(x, \\theta)}{p(x)p(\\theta)} = \\frac {d^{*}(x, \\theta)} {1 - d^{*}(x, \\theta)} $$\nThen the likelihood ratio for any two hypotheses can be estimated at any point by\n$$ r(x | \\theta_0, \\theta_1) = \\frac{d^{*}(x,\\theta_0)}{d^{*}(x, \\theta_1)} $$\nToy example Setup:\nThe simulator: a function takes 1 parameter \\(\\mu\\), and return a random variable drawn from \\(\\mathcal{N}(\\mu, 1)\\) The observations \\(\\mathbf{x}\\): Observation drawn from the simulator with \\(\\mu = 2.5\\), which is unknown to the algorithm. The discriminator: A fully connected neural network. The prior of the parameters: \\(\\mathcal{N}(0, 1)\\) We want to draw samples from the posterior distribution \\(p(\\theta | \\mathbf{x})\\), where \\(x \\sim \\mathcal{N}(2.5, 1)\\).\nImplementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 import click import numpy as np import torch from matplotlib import pyplot as plt from torch import nn from torch.nn import functional as F from typing import NamedTuple np.random.seed(1) torch.manual_seed(1) def stochastic(func): def __wrapper__(*args, **kwargs): np.random.seed() rs = func(*args, **kwargs) np.random.seed(1) return rs return __wrapper__ class Layer(NamedTuple): h: int # hidden dim a: str # activation def Dense(h_i: int, h_j: int, a : str): if a == \"tanh\": act = nn.Tanh() elif a == \"sigmoid\": act = nn.Sigmoid() elif a == \"relu\": act = nn.ReLU() else: raise NotImplementedError(a) return nn.Sequential( nn.Linear(h_i, h_j), act) def build_mlp(input_dim: int, seq: list[Layer]) -\u003e nn.Module: h0, a0 = seq[0] _seq = [Dense(input_dim, h0, a0)] for j in range(1, len(seq)): h_j, a_j = seq[j] h_i, _ = seq[j - 1] _seq.append(Dense(h_i, h_j, a_j)) return nn.Sequential(*_seq) def train_step( Xpos: torch.Tensor, Xneg: torch.Tensor, d: nn.Module, opt: torch.optim.Optimizer) -\u003e torch.Tensor: \"\"\" Args: - Xpos: (x, theta) - Xneg: (x, theta') - d: classifier Where theta/theta' ~ p, x ~ p(x | theta) \"\"\" for i in range(32): opt.zero_grad() zpos = d(Xpos) zneg = d(Xneg) loss = F.binary_cross_entropy(zpos, torch.ones_like(zpos))\\ + F.binary_cross_entropy(zneg, torch.zeros_like(zneg)) loss.backward() opt.step() return loss.item() def train_d( p: callable, sim: callable, d: nn.Module, m: int, e: int, lr: float): \"\"\" Args: - p : prior - sim: simulator (implicit p(x | theta) - d: parameterized classifier - m: batch_size - e: max epochs - lr: learning rate \"\"\" opt = torch.optim.Adam(d.parameters(), lr=lr) sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt) losses = [] for b in range(e): theta = p(m) theta_prime = p(m) x = sim(theta) # expand dims everything theta = np.expand_dims(theta, -1) theta_prime = np.expand_dims(theta_prime, -1) x = np.expand_dims(x, -1) # construct training sample Xpos = np.concatenate([x, theta], -1) Xneg = np.concatenate([x, theta_prime], -1) Xpos, Xneg = torch.tensor(Xpos, dtype=torch.float),\\ torch.tensor(Xneg, dtype=torch.float) loss = train_step(Xpos, Xneg, d, opt) losses.append(loss) if b%50 == 49: sch.step(loss) return d, losses @stochastic def mcmc(lp: callable, obs: np.ndarray, d: nn.Module, n_samples: int, step_size: float): \"\"\" Amortized MCMC likelihood free \"\"\" # proposal distribution: q = lambda theta: np.random.normal(theta, step_size) # initialize theta theta = 0. samples = [] obs = np.expand_dims(obs, -1) for i in range(n_samples): theta_prime = q(theta) mu_theta = np.ones_like(obs) * theta mu_theta_prime = np.ones_like(obs) * theta_prime # construct input vector X = np.concatenate([obs, mu_theta], -1) Xp = np.concatenate([obs, mu_theta_prime], -1) X, Xp= torch.tensor(X, dtype=torch.float),\\ torch.tensor(Xp, dtype=torch.float) # Compute the decision function d_theta = d(X).detach().mean().numpy() d_theta_prime = d(Xp).detach().mean().numpy() r_theta = d_theta / (1 - d_theta) r_theta_prime = d_theta_prime / (1- d_theta_prime) H = r_theta_prime / r_theta H = lp(theta_prime) / lp(theta) * H H = 1 if H \u003e 1 else H u = np.random.uniform() if u \u003c H: # accept theta_prime samples.append(theta_prime) theta = theta_prime return samples def main( batch_size: int, max_iter: int, lr: float, n_obs: int, n_samples: int, step_size: float): # PROBLEM SETUP # -------------------------------------------------- # prior theta p = lambda m: np.random.normal(0, 1, size=m) lp = lambda x: np.exp(-0.5 * x**2)#likelihood function # simulator: unknown sim = lambda mu: np.random.normal(mu, np.ones_like(mu) * .25) # parmeterized classifier d = build_mlp( 2, [Layer(4, 'relu'), Layer(2, 'relu'), Layer(1, 'sigmoid')]) # TRAINING the classifier # -------------------------------------------------- d, losses = train_d(p, sim, d, m=batch_size, e=max_iter, lr=lr) # inference # -------------------------------------------------- MU = 2.5 #unknown obs = sim(np.ones(n_obs) * MU) # Posterior sample: sample p(theta | obs) samples = mcmc(lp, obs, d, n_samples, step_size) Result References The frontier of simulation-based inference Likelihood-free MCMC with Amortized Ratio Estimator I am a black-box machine, you are a black-box machine, everyone is a black-box machine as long as we donâ€™t care enough about the person.Â â†©ï¸Ž\nShamelessly copied from Wikipedia.Â â†©ï¸Ž\n","wordCount":"1498","inLanguage":"en","datePublished":"2023-10-31T00:00:00Z","dateModified":"2023-10-31T00:00:00Z","author":{"@type":"Person","name":"Tu T. Do"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/sbi/"},"publisher":{"@type":"Organization","name":"iamtu","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="iamtu (Alt + H)">iamtu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/ title=home><span>home</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;Â»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class=post-title>Likelihood-free MCMC with Amortized Ratio Estimator</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#simulation-based-inference aria-label="Simulation Based Inference">Simulation Based Inference</a></li><li><a href=#likelihood-free-mcmc-with-amortized-ratio-estimator aria-label="Likelihood-free MCMC with Amortized Ratio Estimator">Likelihood-free MCMC with Amortized Ratio Estimator</a><ul><li><a href=#background aria-label=Background>Background</a><ul><li><a href=#markov-chain-monte-carlo-mcmc aria-label="Markov Chain Monte Carlo (MCMC)">Markov Chain Monte Carlo (MCMC)</a><ul><li><a href=#adapting-mcmc-for-sbi-task aria-label="Adapting MCMC for SBI task">Adapting MCMC for SBI task</a></li></ul></li><li><a href=#metropolis-hasting-mh aria-label="Metropolis-Hasting (MH)">Metropolis-Hasting (MH)</a></li><li><a href=#halmitonian-markov-chainmh aria-label="Halmitonian Markov Chain(MH)">Halmitonian Markov Chain(MH)</a></li></ul></li><li><a href=#likelihood-ratio-estimator aria-label="Likelihood Ratio Estimator">Likelihood Ratio Estimator</a></li></ul></li><li><a href=#toy-example aria-label="Toy example">Toy example</a><ul><li><a href=#result aria-label=Result>Result</a></li></ul></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=simulation-based-inference>Simulation Based Inference<a hidden class=anchor aria-hidden=true href=#simulation-based-inference>#</a></h2><p>Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see <a href=https://www.pnas.org/doi/full/10.1073/pnas.1912789117>this paper</a>. In the analogy above, the black box represents the simulator, and the configurations are the simulatorâ€™s parameters.</p><p>The applicability of SBI has great potential since we can almost reduce any process with defined input and output to a black-box machine <sup id=fnref:1><a href=#fn:1 class=footnote-ref role=doc-noteref>1</a></sup>.</p><p>This post documents my notes while studying <a href=https://proceedings.mlr.press/v119/hermans20a/hermans20a.pdf>Likelihood-free MCMC with Amortized Ratio Estimator</a> (Hermans et al, 2020); a method developed to address SBI.</p><h2 id=likelihood-free-mcmc-with-amortized-ratio-estimator>Likelihood-free MCMC with Amortized Ratio Estimator<a hidden class=anchor aria-hidden=true href=#likelihood-free-mcmc-with-amortized-ratio-estimator>#</a></h2><p>Likelihood ratio is defined as the ratio between the likelihood of the observation between two different hypothesis:</p><p>$$
r(\mathbf{x} | \theta_0, \theta_1) = \frac{p(\mathbf{x} | \theta_0)}{p(\mathbf{x}|\theta_1)}
$$</p><p>This quantity then can be used in various methods to draw sample from a distribution. In the paper, the author mention three sampling methods, namely <strong>Markov Chain Monte Carlo</strong>, <strong>Metropolis-Hasting</strong>, and HMC. In the following section, I am briefly summarizing those methods.</p><h3 id=background>Background<a hidden class=anchor aria-hidden=true href=#background>#</a></h3><h4 id=markov-chain-monte-carlo-mcmc>Markov Chain Monte Carlo (MCMC)<a hidden class=anchor aria-hidden=true href=#markov-chain-monte-carlo-mcmc>#</a></h4><p>In statistics, the MCMC method is a class of algorithms for sampling from a probability distribution. By constructing a Markov chain with the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the state chain <sup id=fnref:2><a href=#fn:2 class=footnote-ref role=doc-noteref>2</a></sup>.</p><h5 id=adapting-mcmc-for-sbi-task>Adapting MCMC for SBI task<a hidden class=anchor aria-hidden=true href=#adapting-mcmc-for-sbi-task>#</a></h5><p>We want to sample from \(p(\theta | \mathbf{x})\) using MCMC, we need this quantity</p><p>$$
\begin{equation}
\begin{aligned}
\frac{p(\theta | \mathbf{x} )}{p(\theta_t| \mathbf{x})} =
\frac{
p(\theta)p(\mathbf{x} | \theta)/p(\mathbf{x})
}{
p(\theta_t)p(\mathbf{x} | \theta_t)/p(\mathbf{x})
} =
\frac{p(\theta)}{p(\theta_t)}\times
\frac{p(\mathbf{x} | \theta)}{p(\mathbf{x} | \theta_t)}
= \frac{p(\theta)}{p(\theta_t)} \times r(\mathbf{x} | \theta, \theta_t)
\end{aligned}
\end{equation}
$$</p><p>We can compute the first term of the equation since we have access to prior \(p(\theta)\). But we can not compute the second term because we do not have access to the likelihood function \(p(\mathbf{x} | \theta)\). However, we can reframe the problem in the supervised-learning paradigm, so we can use a parameterized discriminator \(d_\theta(\mathbf{x})\) to estimate the likelihood. The details are described in the <strong>Likelihood Ratio Estimator</strong> section.</p><h4 id=metropolis-hasting-mh>Metropolis-Hasting (MH)<a hidden class=anchor aria-hidden=true href=#metropolis-hasting-mh>#</a></h4><p><code>tbd</code></p><h4 id=halmitonian-markov-chainmh>Halmitonian Markov Chain(MH)<a hidden class=anchor aria-hidden=true href=#halmitonian-markov-chainmh>#</a></h4><p><code>tbd</code></p><h3 id=likelihood-ratio-estimator>Likelihood Ratio Estimator<a hidden class=anchor aria-hidden=true href=#likelihood-ratio-estimator>#</a></h3><p>The remaining question is how to estimate the likelihood ratio \( r(\mathbf{x} | \theta_0, \theta_1)\). To estimate the ratio, the author employed the Likelihood Ratio Trick, training a discriminator \(d_\phi(\mathbf{x})\) to classify samples \( x \sim p(\mathbf{x} | \theta_0)\) with class label \(y = 1\) from \(\mathbf{x} \sim p(\mathbf{x} | \theta_1)\) with class label \(y = 0\). The decision function obtained by the trained discrimininator:</p><p>$$
d^*(\mathbf{x}) = p(y = 1 | \mathbf{x}) = \frac{p(\mathbf{x} | \theta_0)}{p(\mathbf{x} | \theta_0) + p(\mathbf{x} | \theta_1)}
$$</p><p>Then the estimation of likelihood ratio can be computed by:</p><p>$$
\hat{r}(\mathbf{x} | \theta_0, \theta_1) = \frac
{d^{*}(\mathbf{x})}
{1 - d^{*}(\mathbf{x})}
$$</p><p>However, this method required the discriminator to be trained at every pair of \((\theta_0, \theta_1)\), which is impractical in the context. To overcome this issue, the paper proposed to train the discriminator to classify dependent sample-parameter pairs \((\mathbf{x}, \mathbf{\theta}) \sim p(\mathbf{x}, \mathbf{\theta})\) with label \(y=1\) from the independent sample-parameter pairs \((\mathbf{x}, \mathbf{\theta}) \sim p(\mathbf{x})p(\mathbf{\theta})\) with label \(y=0\).</p><p>$$
\begin{equation}
\begin{aligned}
d^*(\mathbf{x}, \mathbf{\theta}) &= \frac
{p(\mathbf{x}, \mathbf{\theta})}
{
p(\mathbf{x}, \mathbf{\theta})
+ p(\mathbf{x}) p(\mathbf{\theta})
} \
\end{aligned}
\end{equation}
$$</p><p>The likelihood-to-evidence ratio is computed by</p><p>$$
r(\mathbf{x} | \theta) = \frac
{p(\mathbf{x} | \theta)}
{p(x)} =
\frac{p(x, \theta)}{p(x)p(\theta)} = \frac
{d^{*}(x, \theta)}
{1 - d^{*}(x, \theta)}
$$</p><p>Then the likelihood ratio for any two hypotheses can be estimated at any point by</p><p>$$
r(x | \theta_0, \theta_1) = \frac{d^{*}(x,\theta_0)}{d^{*}(x, \theta_1)}
$$</p><h2 id=toy-example>Toy example<a hidden class=anchor aria-hidden=true href=#toy-example>#</a></h2><p>Setup:</p><ul><li><strong>The simulator</strong>: a function takes 1 parameter \(\mu\), and return a random variable drawn from \(\mathcal{N}(\mu, 1)\)</li><li><strong>The observations</strong> \(\mathbf{x}\): Observation drawn from the simulator with \(\mu = 2.5\), which is unknown to the algorithm.</li><li><strong>The discriminator</strong>: A fully connected neural network.</li><li><strong>The prior</strong> of the parameters: \(\mathcal{N}(0, 1)\)</li></ul><p>We want to draw samples from the posterior distribution \(p(\theta | \mathbf{x})\), where \(x \sim \mathcal{N}(2.5, 1)\).</p><p><details><summary markdown=span>Implementation</summary><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>  1
</span><span class=lnt>  2
</span><span class=lnt>  3
</span><span class=lnt>  4
</span><span class=lnt>  5
</span><span class=lnt>  6
</span><span class=lnt>  7
</span><span class=lnt>  8
</span><span class=lnt>  9
</span><span class=lnt> 10
</span><span class=lnt> 11
</span><span class=lnt> 12
</span><span class=lnt> 13
</span><span class=lnt> 14
</span><span class=lnt> 15
</span><span class=lnt> 16
</span><span class=lnt> 17
</span><span class=lnt> 18
</span><span class=lnt> 19
</span><span class=lnt> 20
</span><span class=lnt> 21
</span><span class=lnt> 22
</span><span class=lnt> 23
</span><span class=lnt> 24
</span><span class=lnt> 25
</span><span class=lnt> 26
</span><span class=lnt> 27
</span><span class=lnt> 28
</span><span class=lnt> 29
</span><span class=lnt> 30
</span><span class=lnt> 31
</span><span class=lnt> 32
</span><span class=lnt> 33
</span><span class=lnt> 34
</span><span class=lnt> 35
</span><span class=lnt> 36
</span><span class=lnt> 37
</span><span class=lnt> 38
</span><span class=lnt> 39
</span><span class=lnt> 40
</span><span class=lnt> 41
</span><span class=lnt> 42
</span><span class=lnt> 43
</span><span class=lnt> 44
</span><span class=lnt> 45
</span><span class=lnt> 46
</span><span class=lnt> 47
</span><span class=lnt> 48
</span><span class=lnt> 49
</span><span class=lnt> 50
</span><span class=lnt> 51
</span><span class=lnt> 52
</span><span class=lnt> 53
</span><span class=lnt> 54
</span><span class=lnt> 55
</span><span class=lnt> 56
</span><span class=lnt> 57
</span><span class=lnt> 58
</span><span class=lnt> 59
</span><span class=lnt> 60
</span><span class=lnt> 61
</span><span class=lnt> 62
</span><span class=lnt> 63
</span><span class=lnt> 64
</span><span class=lnt> 65
</span><span class=lnt> 66
</span><span class=lnt> 67
</span><span class=lnt> 68
</span><span class=lnt> 69
</span><span class=lnt> 70
</span><span class=lnt> 71
</span><span class=lnt> 72
</span><span class=lnt> 73
</span><span class=lnt> 74
</span><span class=lnt> 75
</span><span class=lnt> 76
</span><span class=lnt> 77
</span><span class=lnt> 78
</span><span class=lnt> 79
</span><span class=lnt> 80
</span><span class=lnt> 81
</span><span class=lnt> 82
</span><span class=lnt> 83
</span><span class=lnt> 84
</span><span class=lnt> 85
</span><span class=lnt> 86
</span><span class=lnt> 87
</span><span class=lnt> 88
</span><span class=lnt> 89
</span><span class=lnt> 90
</span><span class=lnt> 91
</span><span class=lnt> 92
</span><span class=lnt> 93
</span><span class=lnt> 94
</span><span class=lnt> 95
</span><span class=lnt> 96
</span><span class=lnt> 97
</span><span class=lnt> 98
</span><span class=lnt> 99
</span><span class=lnt>100
</span><span class=lnt>101
</span><span class=lnt>102
</span><span class=lnt>103
</span><span class=lnt>104
</span><span class=lnt>105
</span><span class=lnt>106
</span><span class=lnt>107
</span><span class=lnt>108
</span><span class=lnt>109
</span><span class=lnt>110
</span><span class=lnt>111
</span><span class=lnt>112
</span><span class=lnt>113
</span><span class=lnt>114
</span><span class=lnt>115
</span><span class=lnt>116
</span><span class=lnt>117
</span><span class=lnt>118
</span><span class=lnt>119
</span><span class=lnt>120
</span><span class=lnt>121
</span><span class=lnt>122
</span><span class=lnt>123
</span><span class=lnt>124
</span><span class=lnt>125
</span><span class=lnt>126
</span><span class=lnt>127
</span><span class=lnt>128
</span><span class=lnt>129
</span><span class=lnt>130
</span><span class=lnt>131
</span><span class=lnt>132
</span><span class=lnt>133
</span><span class=lnt>134
</span><span class=lnt>135
</span><span class=lnt>136
</span><span class=lnt>137
</span><span class=lnt>138
</span><span class=lnt>139
</span><span class=lnt>140
</span><span class=lnt>141
</span><span class=lnt>142
</span><span class=lnt>143
</span><span class=lnt>144
</span><span class=lnt>145
</span><span class=lnt>146
</span><span class=lnt>147
</span><span class=lnt>148
</span><span class=lnt>149
</span><span class=lnt>150
</span><span class=lnt>151
</span><span class=lnt>152
</span><span class=lnt>153
</span><span class=lnt>154
</span><span class=lnt>155
</span><span class=lnt>156
</span><span class=lnt>157
</span><span class=lnt>158
</span><span class=lnt>159
</span><span class=lnt>160
</span><span class=lnt>161
</span><span class=lnt>162
</span><span class=lnt>163
</span><span class=lnt>164
</span><span class=lnt>165
</span><span class=lnt>166
</span><span class=lnt>167
</span><span class=lnt>168
</span><span class=lnt>169
</span><span class=lnt>170
</span><span class=lnt>171
</span><span class=lnt>172
</span><span class=lnt>173
</span><span class=lnt>174
</span><span class=lnt>175
</span><span class=lnt>176
</span><span class=lnt>177
</span><span class=lnt>178
</span><span class=lnt>179
</span><span class=lnt>180
</span><span class=lnt>181
</span><span class=lnt>182
</span><span class=lnt>183
</span><span class=lnt>184
</span><span class=lnt>185
</span><span class=lnt>186
</span><span class=lnt>187
</span><span class=lnt>188
</span><span class=lnt>189
</span><span class=lnt>190
</span><span class=lnt>191
</span><span class=lnt>192
</span><span class=lnt>193
</span><span class=lnt>194
</span><span class=lnt>195
</span><span class=lnt>196
</span><span class=lnt>197
</span><span class=lnt>198
</span><span class=lnt>199
</span><span class=lnt>200
</span><span class=lnt>201
</span><span class=lnt>202
</span><span class=lnt>203
</span><span class=lnt>204
</span><span class=lnt>205
</span><span class=lnt>206
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>click</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>torch</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>matplotlib</span> <span class=kn>import</span> <span class=n>pyplot</span> <span class=k>as</span> <span class=n>plt</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch</span> <span class=kn>import</span> <span class=n>nn</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>torch.nn</span> <span class=kn>import</span> <span class=n>functional</span> <span class=k>as</span> <span class=n>F</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>typing</span> <span class=kn>import</span> <span class=n>NamedTuple</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>torch</span><span class=o>.</span><span class=n>manual_seed</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>stochastic</span><span class=p>(</span><span class=n>func</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>__wrapper__</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>rs</span> <span class=o>=</span> <span class=n>func</span><span class=p>(</span><span class=o>*</span><span class=n>args</span><span class=p>,</span> <span class=o>**</span><span class=n>kwargs</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>seed</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>rs</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>__wrapper__</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>Layer</span><span class=p>(</span><span class=n>NamedTuple</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>h</span><span class=p>:</span> <span class=nb>int</span> <span class=c1># hidden dim</span>
</span></span><span class=line><span class=cl>    <span class=n>a</span><span class=p>:</span> <span class=nb>str</span> <span class=c1># activation</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>Dense</span><span class=p>(</span><span class=n>h_i</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>h_j</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>a</span> <span class=p>:</span> <span class=nb>str</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>a</span> <span class=o>==</span> <span class=s2>&#34;tanh&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>act</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Tanh</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>a</span> <span class=o>==</span> <span class=s2>&#34;sigmoid&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>act</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sigmoid</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>elif</span> <span class=n>a</span> <span class=o>==</span> <span class=s2>&#34;relu&#34;</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>act</span> <span class=o>=</span> <span class=n>nn</span><span class=o>.</span><span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>raise</span> <span class=ne>NotImplementedError</span><span class=p>(</span><span class=n>a</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>nn</span><span class=o>.</span><span class=n>Linear</span><span class=p>(</span><span class=n>h_i</span><span class=p>,</span> <span class=n>h_j</span><span class=p>),</span>
</span></span><span class=line><span class=cl>            <span class=n>act</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>build_mlp</span><span class=p>(</span><span class=n>input_dim</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>seq</span><span class=p>:</span> <span class=nb>list</span><span class=p>[</span><span class=n>Layer</span><span class=p>])</span> <span class=o>-&gt;</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>h0</span><span class=p>,</span> <span class=n>a0</span> <span class=o>=</span> <span class=n>seq</span><span class=p>[</span><span class=mi>0</span><span class=p>]</span>
</span></span><span class=line><span class=cl>    <span class=n>_seq</span> <span class=o>=</span> <span class=p>[</span><span class=n>Dense</span><span class=p>(</span><span class=n>input_dim</span><span class=p>,</span> <span class=n>h0</span><span class=p>,</span> <span class=n>a0</span><span class=p>)]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>j</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=nb>len</span><span class=p>(</span><span class=n>seq</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=n>h_j</span><span class=p>,</span> <span class=n>a_j</span> <span class=o>=</span> <span class=n>seq</span><span class=p>[</span><span class=n>j</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>h_i</span><span class=p>,</span> <span class=n>_</span> <span class=o>=</span> <span class=n>seq</span><span class=p>[</span><span class=n>j</span> <span class=o>-</span> <span class=mi>1</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>_seq</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>Dense</span><span class=p>(</span><span class=n>h_i</span><span class=p>,</span> <span class=n>h_j</span><span class=p>,</span> <span class=n>a_j</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>nn</span><span class=o>.</span><span class=n>Sequential</span><span class=p>(</span><span class=o>*</span><span class=n>_seq</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_step</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>Xpos</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>Xneg</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>d</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> 
</span></span><span class=line><span class=cl>        <span class=n>opt</span><span class=p>:</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Optimizer</span><span class=p>)</span> <span class=o>-&gt;</span> <span class=n>torch</span><span class=o>.</span><span class=n>Tensor</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>    - Xpos: (x, theta)
</span></span></span><span class=line><span class=cl><span class=s2>    - Xneg: (x, theta&#39;)
</span></span></span><span class=line><span class=cl><span class=s2>    - d: classifier 
</span></span></span><span class=line><span class=cl><span class=s2>
</span></span></span><span class=line><span class=cl><span class=s2>    Where theta/theta&#39; ~ p, x ~ p(x | theta)
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=mi>32</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>opt</span><span class=o>.</span><span class=n>zero_grad</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=n>zpos</span> <span class=o>=</span> <span class=n>d</span><span class=p>(</span><span class=n>Xpos</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>zneg</span> <span class=o>=</span> <span class=n>d</span><span class=p>(</span><span class=n>Xneg</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>binary_cross_entropy</span><span class=p>(</span><span class=n>zpos</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>zpos</span><span class=p>))</span>\
</span></span><span class=line><span class=cl>                <span class=o>+</span> <span class=n>F</span><span class=o>.</span><span class=n>binary_cross_entropy</span><span class=p>(</span><span class=n>zneg</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros_like</span><span class=p>(</span><span class=n>zneg</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>loss</span><span class=o>.</span><span class=n>backward</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>opt</span><span class=o>.</span><span class=n>step</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>loss</span><span class=o>.</span><span class=n>item</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>train_d</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>p</span><span class=p>:</span> <span class=n>callable</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>sim</span><span class=p>:</span> <span class=n>callable</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>d</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>m</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>e</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>lr</span><span class=p>:</span> <span class=nb>float</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Args:
</span></span></span><span class=line><span class=cl><span class=s2>    - p : prior
</span></span></span><span class=line><span class=cl><span class=s2>    - sim: simulator (implicit p(x | theta)
</span></span></span><span class=line><span class=cl><span class=s2>    - d: parameterized classifier
</span></span></span><span class=line><span class=cl><span class=s2>    - m: batch_size
</span></span></span><span class=line><span class=cl><span class=s2>    - e: max epochs
</span></span></span><span class=line><span class=cl><span class=s2>    - lr: learning rate
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>opt</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>d</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>lr_scheduler</span><span class=o>.</span><span class=n>ReduceLROnPlateau</span><span class=p>(</span><span class=n>opt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>losses</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>b</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>e</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>theta</span> <span class=o>=</span> <span class=n>p</span><span class=p>(</span><span class=n>m</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>theta_prime</span> <span class=o>=</span> <span class=n>p</span><span class=p>(</span><span class=n>m</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>sim</span><span class=p>(</span><span class=n>theta</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># expand dims everything</span>
</span></span><span class=line><span class=cl>        <span class=n>theta</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>theta_prime</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>theta_prime</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># construct training sample</span>
</span></span><span class=line><span class=cl>        <span class=n>Xpos</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>x</span><span class=p>,</span> <span class=n>theta</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>Xneg</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>x</span><span class=p>,</span> <span class=n>theta_prime</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>Xpos</span><span class=p>,</span> <span class=n>Xneg</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>Xpos</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>),</span>\
</span></span><span class=line><span class=cl>                <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>Xneg</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>train_step</span><span class=p>(</span><span class=n>Xpos</span><span class=p>,</span> <span class=n>Xneg</span><span class=p>,</span> <span class=n>d</span><span class=p>,</span> <span class=n>opt</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>losses</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>b</span><span class=o>%</span><span class=mi>50</span> <span class=o>==</span> <span class=mi>49</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>sch</span><span class=o>.</span><span class=n>step</span><span class=p>(</span><span class=n>loss</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>d</span><span class=p>,</span> <span class=n>losses</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=nd>@stochastic</span>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>mcmc</span><span class=p>(</span><span class=n>lp</span><span class=p>:</span> <span class=n>callable</span><span class=p>,</span> <span class=n>obs</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>ndarray</span><span class=p>,</span> <span class=n>d</span><span class=p>:</span> <span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span> <span class=n>step_size</span><span class=p>:</span> <span class=nb>float</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;&#34;&#34;
</span></span></span><span class=line><span class=cl><span class=s2>    Amortized MCMC likelihood free
</span></span></span><span class=line><span class=cl><span class=s2>    &#34;&#34;&#34;</span>
</span></span><span class=line><span class=cl>    <span class=c1># proposal distribution:</span>
</span></span><span class=line><span class=cl>    <span class=n>q</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>theta</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>theta</span><span class=p>,</span> <span class=n>step_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># initialize theta</span>
</span></span><span class=line><span class=cl>    <span class=n>theta</span> <span class=o>=</span> <span class=mf>0.</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>samples</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>obs</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>expand_dims</span><span class=p>(</span><span class=n>obs</span><span class=p>,</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n_samples</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>theta_prime</span> <span class=o>=</span> <span class=n>q</span><span class=p>(</span><span class=n>theta</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>mu_theta</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>obs</span><span class=p>)</span> <span class=o>*</span> <span class=n>theta</span>
</span></span><span class=line><span class=cl>        <span class=n>mu_theta_prime</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>obs</span><span class=p>)</span> <span class=o>*</span> <span class=n>theta_prime</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># construct input vector</span>
</span></span><span class=line><span class=cl>        <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>obs</span><span class=p>,</span> <span class=n>mu_theta</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>Xp</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>concatenate</span><span class=p>([</span><span class=n>obs</span><span class=p>,</span> <span class=n>mu_theta_prime</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>X</span><span class=p>,</span> <span class=n>Xp</span><span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>),</span>\
</span></span><span class=line><span class=cl>                <span class=n>torch</span><span class=o>.</span><span class=n>tensor</span><span class=p>(</span><span class=n>Xp</span><span class=p>,</span> <span class=n>dtype</span><span class=o>=</span><span class=n>torch</span><span class=o>.</span><span class=n>float</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># Compute the decision function</span>
</span></span><span class=line><span class=cl>        <span class=n>d_theta</span> <span class=o>=</span> <span class=n>d</span><span class=p>(</span><span class=n>X</span><span class=p>)</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>d_theta_prime</span> <span class=o>=</span> <span class=n>d</span><span class=p>(</span><span class=n>Xp</span><span class=p>)</span><span class=o>.</span><span class=n>detach</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span><span class=o>.</span><span class=n>numpy</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>r_theta</span> <span class=o>=</span> <span class=n>d_theta</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>d_theta</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>r_theta_prime</span> <span class=o>=</span> <span class=n>d_theta_prime</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span><span class=o>-</span> <span class=n>d_theta_prime</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>H</span> <span class=o>=</span> <span class=n>r_theta_prime</span> <span class=o>/</span> <span class=n>r_theta</span> 
</span></span><span class=line><span class=cl>        <span class=n>H</span> <span class=o>=</span> <span class=n>lp</span><span class=p>(</span><span class=n>theta_prime</span><span class=p>)</span> <span class=o>/</span> <span class=n>lp</span><span class=p>(</span><span class=n>theta</span><span class=p>)</span> <span class=o>*</span> <span class=n>H</span>
</span></span><span class=line><span class=cl>        <span class=n>H</span> <span class=o>=</span> <span class=mi>1</span> <span class=k>if</span> <span class=n>H</span> <span class=o>&gt;</span> <span class=mi>1</span> <span class=k>else</span> <span class=n>H</span>
</span></span><span class=line><span class=cl>        <span class=n>u</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>uniform</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>u</span> <span class=o>&lt;</span> <span class=n>H</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># accept theta_prime</span>
</span></span><span class=line><span class=cl>            <span class=n>samples</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>theta_prime</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>theta</span> <span class=o>=</span> <span class=n>theta_prime</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>samples</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>main</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>batch_size</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>max_iter</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>lr</span><span class=p>:</span> <span class=nb>float</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>n_obs</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>n_samples</span><span class=p>:</span> <span class=nb>int</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>step_size</span><span class=p>:</span> <span class=nb>float</span><span class=p>):</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># PROBLEM SETUP</span>
</span></span><span class=line><span class=cl>    <span class=c1># --------------------------------------------------</span>
</span></span><span class=line><span class=cl>    <span class=c1># prior theta</span>
</span></span><span class=line><span class=cl>    <span class=n>p</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>m</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=mi>0</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>size</span><span class=o>=</span><span class=n>m</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>lp</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>x</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=o>-</span><span class=mf>0.5</span> <span class=o>*</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span><span class=c1>#likelihood function</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># simulator: unknown</span>
</span></span><span class=line><span class=cl>    <span class=n>sim</span> <span class=o>=</span> <span class=k>lambda</span> <span class=n>mu</span><span class=p>:</span> <span class=n>np</span><span class=o>.</span><span class=n>random</span><span class=o>.</span><span class=n>normal</span><span class=p>(</span><span class=n>mu</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>ones_like</span><span class=p>(</span><span class=n>mu</span><span class=p>)</span> <span class=o>*</span> <span class=mf>.25</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># parmeterized classifier</span>
</span></span><span class=line><span class=cl>    <span class=n>d</span> <span class=o>=</span> <span class=n>build_mlp</span><span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=mi>2</span><span class=p>,</span> <span class=p>[</span><span class=n>Layer</span><span class=p>(</span><span class=mi>4</span><span class=p>,</span> <span class=s1>&#39;relu&#39;</span><span class=p>),</span> <span class=n>Layer</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=s1>&#39;relu&#39;</span><span class=p>),</span> <span class=n>Layer</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=s1>&#39;sigmoid&#39;</span><span class=p>)])</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># TRAINING the classifier </span>
</span></span><span class=line><span class=cl>    <span class=c1># --------------------------------------------------</span>
</span></span><span class=line><span class=cl>    <span class=n>d</span><span class=p>,</span> <span class=n>losses</span> <span class=o>=</span> <span class=n>train_d</span><span class=p>(</span><span class=n>p</span><span class=p>,</span> <span class=n>sim</span><span class=p>,</span> <span class=n>d</span><span class=p>,</span> <span class=n>m</span><span class=o>=</span><span class=n>batch_size</span><span class=p>,</span> <span class=n>e</span><span class=o>=</span><span class=n>max_iter</span><span class=p>,</span> <span class=n>lr</span><span class=o>=</span><span class=n>lr</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># inference</span>
</span></span><span class=line><span class=cl>    <span class=c1># --------------------------------------------------</span>
</span></span><span class=line><span class=cl>    <span class=n>MU</span> <span class=o>=</span> <span class=mf>2.5</span> <span class=c1>#unknown</span>
</span></span><span class=line><span class=cl>    <span class=n>obs</span> <span class=o>=</span> <span class=n>sim</span><span class=p>(</span><span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>(</span><span class=n>n_obs</span><span class=p>)</span> <span class=o>*</span> <span class=n>MU</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1># Posterior sample: sample p(theta | obs)</span>
</span></span><span class=line><span class=cl>    <span class=n>samples</span> <span class=o>=</span> <span class=n>mcmc</span><span class=p>(</span><span class=n>lp</span><span class=p>,</span> <span class=n>obs</span><span class=p>,</span> <span class=n>d</span><span class=p>,</span> <span class=n>n_samples</span><span class=p>,</span> <span class=n>step_size</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div></details></p><h3 id=result>Result<a hidden class=anchor aria-hidden=true href=#result>#</a></h3><p><img loading=lazy src=/images/amcmc.png alt=img></p><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li><a href=https://www.pnas.org/doi/full/10.1073/pnas.1912789117>The frontier of simulation-based inference</a></li><li><a href=https://proceedings.mlr.press/v119/hermans20a/hermans20a.pdf>Likelihood-free MCMC with Amortized Ratio Estimator</a></li></ul><div class=footnotes role=doc-endnotes><hr><ol><li id=fn:1><p>I am a black-box machine, you are a black-box machine, everyone is a black-box machine as long as we don&rsquo;t care enough about the person.&#160;<a href=#fnref:1 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li><li id=fn:2><p>Shamelessly copied from <a href=https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo>Wikipedia</a>.&#160;<a href=#fnref:2 class=footnote-backref role=doc-backlink>&#8617;&#xfe0e;</a></p></li></ol></div></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/sbi/>Sbi</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/closed-form-kl-gaussian/><span class=title>Â« Prev</span><br><span>Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution</span>
</a><a class=next href=http://localhost:1313/posts/miscellanous/><span class=title>Next Â»</span><br><span>Miscellanous</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Likelihood-free MCMC with Amortized Ratio Estimator on twitter" href="https://twitter.com/intent/tweet/?text=Likelihood-free%20MCMC%20with%20Amortized%20Ratio%20Estimator&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fsbi%2f&amp;hashtags=sbi"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Likelihood-free MCMC with Amortized Ratio Estimator on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fsbi%2f&amp;title=Likelihood-free%20MCMC%20with%20Amortized%20Ratio%20Estimator&amp;summary=Likelihood-free%20MCMC%20with%20Amortized%20Ratio%20Estimator&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2fsbi%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Likelihood-free MCMC with Amortized Ratio Estimator on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2fsbi%2f&title=Likelihood-free%20MCMC%20with%20Amortized%20Ratio%20Estimator"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Likelihood-free MCMC with Amortized Ratio Estimator on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2fsbi%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Likelihood-free MCMC with Amortized Ratio Estimator on whatsapp" href="https://api.whatsapp.com/send?text=Likelihood-free%20MCMC%20with%20Amortized%20Ratio%20Estimator%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2fsbi%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Likelihood-free MCMC with Amortized Ratio Estimator on telegram" href="https://telegram.me/share/url?text=Likelihood-free%20MCMC%20with%20Amortized%20Ratio%20Estimator&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2fsbi%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a><a target=_blank rel="noopener noreferrer" aria-label="share Likelihood-free MCMC with Amortized Ratio Estimator on ycombinator" href="https://news.ycombinator.com/submitlink?t=Likelihood-free%20MCMC%20with%20Amortized%20Ratio%20Estimator&u=http%3a%2f%2flocalhost%3a1313%2fposts%2fsbi%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></div></footer><div id=disqus_thread></div><script>window.disqus_config=function(){},function(){if(["localhost","127.0.0.1"].indexOf(window.location.hostname)!=-1){document.getElementById("disqus_thread").innerHTML="Disqus comments not available by default when the website is previewed locally.";return}var t=document,e=t.createElement("script");e.async=!0,e.src="//iamtu-dev.disqus.com.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><a href=https://disqus.com class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>iamtu</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>