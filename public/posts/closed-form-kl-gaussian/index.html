<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution | iamtu</title>
<meta name=keywords content="learning,stat"><meta name=description content="The closed form of KL divergence used in Variational Auto Encoder.
Univariate case Let
\(p(x) = \mathcal{N}(\mu_1, \sigma_1) = (2\pi\sigma_1^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_1^2}(x-\mu_1)^2]\) \(q(x) = \mathcal{N}(\mu_1, \sigma_2) = (2\pi\sigma_2^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_2^2}(x-\mu_2)^2]\) KL divergence between \(p\) and \(q\) is defined as:
$$ \begin{aligned} \text{KL}(p\parallel q) &= -\int_{x}{p(x)\log{\frac{q(x)}{p(x)}}dx} \\ &= -\int_x p(x) [\log{q(x)} - \log{p(x)}]dx \\ &= \underbrace{ \int_x{p(x)\log p(x) dx}}_A - \underbrace{ \int_x{p(x)\log q(x) dx}}_B \end{aligned} $$
First quantity \(A\):
$$ \begin{aligned} A &= \int_x{p(x)\log p(x) dx} \\ &= \int_x{p(x)\big[ -\frac{1}{2}\log{2\pi\sigma_1^2 - \frac{1}{2\sigma_1^2}(x - \mu_1)^2} \big]dx}\\ &= -\frac{1}{2}\log{2\pi\sigma_1^2}\int_x{p(x)dx} - \frac{1}{2\sigma_1^2} \underbrace{\int_x{p(x)(x-\mu_1)^2dx}}_{\text{var(x)}}\\ &= -\frac{1}{2}\log{2\pi} - \log\sigma_1-\frac{1}{2} \end{aligned} $$"><meta name=author content="Tu T. Do"><link rel=canonical href=http://localhost:1313/posts/closed-form-kl-gaussian/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/closed-form-kl-gaussian/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PWLR4FLELZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PWLR4FLELZ")}</script><meta property="og:title" content="Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution"><meta property="og:description" content="The closed form of KL divergence used in Variational Auto Encoder.
Univariate case Let
\(p(x) = \mathcal{N}(\mu_1, \sigma_1) = (2\pi\sigma_1^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_1^2}(x-\mu_1)^2]\) \(q(x) = \mathcal{N}(\mu_1, \sigma_2) = (2\pi\sigma_2^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_2^2}(x-\mu_2)^2]\) KL divergence between \(p\) and \(q\) is defined as:
$$ \begin{aligned} \text{KL}(p\parallel q) &= -\int_{x}{p(x)\log{\frac{q(x)}{p(x)}}dx} \\ &= -\int_x p(x) [\log{q(x)} - \log{p(x)}]dx \\ &= \underbrace{ \int_x{p(x)\log p(x) dx}}_A - \underbrace{ \int_x{p(x)\log q(x) dx}}_B \end{aligned} $$
First quantity \(A\):
$$ \begin{aligned} A &= \int_x{p(x)\log p(x) dx} \\ &= \int_x{p(x)\big[ -\frac{1}{2}\log{2\pi\sigma_1^2 - \frac{1}{2\sigma_1^2}(x - \mu_1)^2} \big]dx}\\ &= -\frac{1}{2}\log{2\pi\sigma_1^2}\int_x{p(x)dx} - \frac{1}{2\sigma_1^2} \underbrace{\int_x{p(x)(x-\mu_1)^2dx}}_{\text{var(x)}}\\ &= -\frac{1}{2}\log{2\pi} - \log\sigma_1-\frac{1}{2} \end{aligned} $$"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/closed-form-kl-gaussian/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-11-12T00:00:00+00:00"><meta property="article:modified_time" content="2023-11-12T00:00:00+00:00"><meta property="og:site_name" content="iamtu"><meta name=twitter:card content="summary"><meta name=twitter:title content="Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution"><meta name=twitter:description content="The closed form of KL divergence used in Variational Auto Encoder.
Univariate case Let
\(p(x) = \mathcal{N}(\mu_1, \sigma_1) = (2\pi\sigma_1^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_1^2}(x-\mu_1)^2]\) \(q(x) = \mathcal{N}(\mu_1, \sigma_2) = (2\pi\sigma_2^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_2^2}(x-\mu_2)^2]\) KL divergence between \(p\) and \(q\) is defined as:
$$ \begin{aligned} \text{KL}(p\parallel q) &= -\int_{x}{p(x)\log{\frac{q(x)}{p(x)}}dx} \\ &= -\int_x p(x) [\log{q(x)} - \log{p(x)}]dx \\ &= \underbrace{ \int_x{p(x)\log p(x) dx}}_A - \underbrace{ \int_x{p(x)\log q(x) dx}}_B \end{aligned} $$
First quantity \(A\):
$$ \begin{aligned} A &= \int_x{p(x)\log p(x) dx} \\ &= \int_x{p(x)\big[ -\frac{1}{2}\log{2\pi\sigma_1^2 - \frac{1}{2\sigma_1^2}(x - \mu_1)^2} \big]dx}\\ &= -\frac{1}{2}\log{2\pi\sigma_1^2}\int_x{p(x)dx} - \frac{1}{2\sigma_1^2} \underbrace{\int_x{p(x)(x-\mu_1)^2dx}}_{\text{var(x)}}\\ &= -\frac{1}{2}\log{2\pi} - \log\sigma_1-\frac{1}{2} \end{aligned} $$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution","item":"http://localhost:1313/posts/closed-form-kl-gaussian/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution","name":"Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution","description":"The closed form of KL divergence used in Variational Auto Encoder.\nUnivariate case Let\n\\(p(x) = \\mathcal{N}(\\mu_1, \\sigma_1) = (2\\pi\\sigma_1^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2]\\) \\(q(x) = \\mathcal{N}(\\mu_1, \\sigma_2) = (2\\pi\\sigma_2^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2]\\) KL divergence between \\(p\\) and \\(q\\) is defined as:\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= -\\int_{x}{p(x)\\log{\\frac{q(x)}{p(x)}}dx} \\\\ \u0026amp;= -\\int_x p(x) [\\log{q(x)} - \\log{p(x)}]dx \\\\ \u0026amp;= \\underbrace{ \\int_x{p(x)\\log p(x) dx}}_A - \\underbrace{ \\int_x{p(x)\\log q(x) dx}}_B \\end{aligned} $$\nFirst quantity \\(A\\):\n$$ \\begin{aligned} A \u0026amp;= \\int_x{p(x)\\log p(x) dx} \\\\ \u0026amp;= \\int_x{p(x)\\big[ -\\frac{1}{2}\\log{2\\pi\\sigma_1^2 - \\frac{1}{2\\sigma_1^2}(x - \\mu_1)^2} \\big]dx}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi\\sigma_1^2}\\int_x{p(x)dx} - \\frac{1}{2\\sigma_1^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)^2dx}}_{\\text{var(x)}}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi} - \\log\\sigma_1-\\frac{1}{2} \\end{aligned} $$","keywords":["learning","stat"],"articleBody":"The closed form of KL divergence used in Variational Auto Encoder.\nUnivariate case Let\n\\(p(x) = \\mathcal{N}(\\mu_1, \\sigma_1) = (2\\pi\\sigma_1^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2]\\) \\(q(x) = \\mathcal{N}(\\mu_1, \\sigma_2) = (2\\pi\\sigma_2^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2]\\) KL divergence between \\(p\\) and \\(q\\) is defined as:\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026= -\\int_{x}{p(x)\\log{\\frac{q(x)}{p(x)}}dx} \\\\ \u0026= -\\int_x p(x) [\\log{q(x)} - \\log{p(x)}]dx \\\\ \u0026= \\underbrace{ \\int_x{p(x)\\log p(x) dx}}_A - \\underbrace{ \\int_x{p(x)\\log q(x) dx}}_B \\end{aligned} $$\nFirst quantity \\(A\\):\n$$ \\begin{aligned} A \u0026= \\int_x{p(x)\\log p(x) dx} \\\\ \u0026= \\int_x{p(x)\\big[ -\\frac{1}{2}\\log{2\\pi\\sigma_1^2 - \\frac{1}{2\\sigma_1^2}(x - \\mu_1)^2} \\big]dx}\\\\ \u0026= -\\frac{1}{2}\\log{2\\pi\\sigma_1^2}\\int_x{p(x)dx} - \\frac{1}{2\\sigma_1^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)^2dx}}_{\\text{var(x)}}\\\\ \u0026= -\\frac{1}{2}\\log{2\\pi} - \\log\\sigma_1-\\frac{1}{2} \\end{aligned} $$\nThe second quantity \\(B\\):\n$$ \\begin{aligned} B =\u0026 \\int_x{p(x)\\big[ -\\frac{1}{2}\\log2\\pi\\sigma_2^2 - \\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2 \\big]dx}\\\\ =\u0026 -\\frac{1}{2}\\log2\\pi\\sigma_2^2 - \\frac{1}{2\\sigma_2^2}\\int_x{ p(x)\\big[ (x - \\mu_1)^2 + 2(x-\\mu_1)(\\mu_1 - \\mu_2) + (\\mu_1 -\\mu_2)^2 \\big]dx} \\\\ =\u0026 -\\frac{1}{2}\\log2\\pi\\sigma_2^2 \\\\ \u0026 - \\frac{1}{2\\sigma_2^2}\\underbrace{\\int_x{p(x)(x-\\mu_1)^2}}_{\\text{var}(x)}\\\\ \u0026 - \\frac{2(\\mu_1 -\\mu_2)}{2\\sigma_2^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)dx}}_0 \\\\ \u0026 - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} \\\\ =\u0026 -\\frac{1}{2}\\log2\\pi -\\log\\sigma_2 - \\frac{\\sigma_1^2}{2\\sigma_2^2} - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} \\end{aligned} $$\nFinally, we obtained the KL divergence for univariate case\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026= A - B \\\\ \u0026= (-\\frac{1}{2}\\log2\\pi - \\log\\sigma_1 - \\frac{1}{2}) - ( -\\frac{1}{2}\\log2\\pi -\\log\\sigma_2 - \\frac{\\sigma_1^2}{2\\sigma_2^2} - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}) \\\\ \u0026= \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2}{2\\sigma_2^2} + \\frac{(\\mu_1 -\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2} \\end{aligned} $$\nMultivariate case tbd\nReference https://gregorygundersen.com/blog/ ","wordCount":"195","inLanguage":"en","datePublished":"2023-11-12T00:00:00Z","dateModified":"2023-11-12T00:00:00Z","author":{"@type":"Person","name":"Tu T. Do"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/closed-form-kl-gaussian/"},"publisher":{"@type":"Organization","name":"iamtu","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="iamtu (Alt + H)">iamtu</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/ title=home><span>home</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class=post-title>Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution</h1></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#univariate-case>Univariate case</a></li><li><a href=#multivariate-case>Multivariate case</a></li><li><a href=#reference>Reference</a></li></ul></nav></div></details></div><div class=post-content><p>The closed form of KL divergence used in Variational Auto Encoder.</p><h2 id=univariate-case>Univariate case<a hidden class=anchor aria-hidden=true href=#univariate-case>#</a></h2><p>Let</p><ul><li>\(p(x) = \mathcal{N}(\mu_1, \sigma_1) = (2\pi\sigma_1^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_1^2}(x-\mu_1)^2]\)</li><li>\(q(x) = \mathcal{N}(\mu_1, \sigma_2) = (2\pi\sigma_2^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_2^2}(x-\mu_2)^2]\)</li></ul><p>KL divergence between \(p\) and \(q\) is defined as:</p><p>$$
\begin{aligned}
\text{KL}(p\parallel q) &= -\int_{x}{p(x)\log{\frac{q(x)}{p(x)}}dx} \\
&= -\int_x p(x) [\log{q(x)} - \log{p(x)}]dx \\
&=
\underbrace{
\int_x{p(x)\log p(x) dx}}_A
- \underbrace{
\int_x{p(x)\log q(x) dx}}_B
\end{aligned}
$$</p><p>First quantity \(A\):</p><p>$$
\begin{aligned}
A &= \int_x{p(x)\log p(x) dx} \\
&= \int_x{p(x)\big[
-\frac{1}{2}\log{2\pi\sigma_1^2
- \frac{1}{2\sigma_1^2}(x - \mu_1)^2}
\big]dx}\\
&= -\frac{1}{2}\log{2\pi\sigma_1^2}\int_x{p(x)dx}
- \frac{1}{2\sigma_1^2}
\underbrace{\int_x{p(x)(x-\mu_1)^2dx}}_{\text{var(x)}}\\
&= -\frac{1}{2}\log{2\pi} - \log\sigma_1-\frac{1}{2}
\end{aligned}
$$</p><p>The second quantity \(B\):</p><p>$$
\begin{aligned}
B =& \int_x{p(x)\big[
-\frac{1}{2}\log2\pi\sigma_2^2
- \frac{1}{2\sigma_2^2}(x-\mu_2)^2
\big]dx}\\
=& -\frac{1}{2}\log2\pi\sigma_2^2
- \frac{1}{2\sigma_2^2}\int_x{
p(x)\big[
(x - \mu_1)^2 + 2(x-\mu_1)(\mu_1 - \mu_2) + (\mu_1 -\mu_2)^2
\big]dx} \\
=& -\frac{1}{2}\log2\pi\sigma_2^2 \\
& - \frac{1}{2\sigma_2^2}\underbrace{\int_x{p(x)(x-\mu_1)^2}}_{\text{var}(x)}\\
& - \frac{2(\mu_1 -\mu_2)}{2\sigma_2^2} \underbrace{\int_x{p(x)(x-\mu_1)dx}}_0 \\
& - \frac{(\mu_1-\mu_2)^2}{2\sigma_2^2} \\
=& -\frac{1}{2}\log2\pi -\log\sigma_2 - \frac{\sigma_1^2}{2\sigma_2^2} - \frac{(\mu_1-\mu_2)^2}{2\sigma_2^2}
\end{aligned}
$$</p><p>Finally, we obtained the KL divergence for univariate case</p><p>$$
\begin{aligned}
\text{KL}(p\parallel q) &= A - B \\
&= (-\frac{1}{2}\log2\pi - \log\sigma_1 - \frac{1}{2}) - ( -\frac{1}{2}\log2\pi -\log\sigma_2 - \frac{\sigma_1^2}{2\sigma_2^2} - \frac{(\mu_1-\mu_2)^2}{2\sigma_2^2}) \\
&= \log\frac{\sigma_2}{\sigma_1} + \frac{\sigma_1^2}{2\sigma_2^2} + \frac{(\mu_1 -\mu_2)^2}{2\sigma_2^2} - \frac{1}{2}
\end{aligned}
$$</p><h2 id=multivariate-case>Multivariate case<a hidden class=anchor aria-hidden=true href=#multivariate-case>#</a></h2><p><code>tbd</code></p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><ul><li><a href=https://gregorygundersen.com/blog/>https://gregorygundersen.com/blog/</a></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/learning/>Learning</a></li><li><a href=http://localhost:1313/tags/stat/>Stat</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/diff-under-integral-sign/><span class=title>« Prev</span><br><span>Differentiation under integral sign</span>
</a><a class=next href=http://localhost:1313/posts/sbi/><span class=title>Next »</span><br><span>Likelihood-free MCMC with Amortized Ratio Estimator</span></a></nav></footer><div id=disqus_thread></div><script type=text/javascript>(function(){var t,e=document.createElement("script");e.type="text/javascript",e.async=!0,t="iamtu-dev.disqus.com",e.src="//"+t+".disqus.com/embed.js",(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(e)})()</script><noscript>Please enable JavaScript to view the <a href=http://disqus.com/?ref_noscript>comments powered by
Disqus.</a></noscript><a href=http://disqus.com/ class=dsq-brlink>comments powered by <span class=logo-disqus>Disqus</span></a></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>iamtu</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>