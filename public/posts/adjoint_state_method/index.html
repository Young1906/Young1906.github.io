<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Neural Odinary Differential Equation - NeuralODE | iamtu</title>
<meta name=keywords content="learn"><meta name=description content="Problem $$ \begin{equation} \begin{aligned} \text{ODE:}\quad &\begin{cases} \frac{du}{dt} &= f(u, t; \theta) &\text{(Dynamic function)}\\ u(t=0) &= u_0 & \text{(Initial value)} \end{cases} \end{aligned} \end{equation} $$
Minimize loss functional:
$$ \begin{equation} \begin{aligned} \mathcal{J}[\theta] = \int_{0}^T{g(u; \theta)} dt \end{aligned} \end{equation} $$
Formulating as an optimization with ODE constraint: $$ \begin{equation} \begin{aligned} \min_\theta \mathcal{J}(\theta) = \min_\theta \int_0^Tg(u; \theta) dt \\ \text{such that: } \ \frac{du}{dt} - f(u, t;\theta) = 0 \end{aligned} \end{equation} $$
Derivation of the adjoint-state The Lagrangian of the optimization problem"><meta name=author content="Tu T. Do"><link rel=canonical href=http://localhost:1313/posts/adjoint_state_method/><meta name=google-site-verification content="G-PWLR4FLELZ"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/adjoint_state_method/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PWLR4FLELZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PWLR4FLELZ")}</script><meta property="og:title" content="Neural Odinary Differential Equation - NeuralODE"><meta property="og:description" content="Problem $$ \begin{equation} \begin{aligned} \text{ODE:}\quad &\begin{cases} \frac{du}{dt} &= f(u, t; \theta) &\text{(Dynamic function)}\\ u(t=0) &= u_0 & \text{(Initial value)} \end{cases} \end{aligned} \end{equation} $$
Minimize loss functional:
$$ \begin{equation} \begin{aligned} \mathcal{J}[\theta] = \int_{0}^T{g(u; \theta)} dt \end{aligned} \end{equation} $$
Formulating as an optimization with ODE constraint: $$ \begin{equation} \begin{aligned} \min_\theta \mathcal{J}(\theta) = \min_\theta \int_0^Tg(u; \theta) dt \\ \text{such that: } \ \frac{du}{dt} - f(u, t;\theta) = 0 \end{aligned} \end{equation} $$
Derivation of the adjoint-state The Lagrangian of the optimization problem"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/adjoint_state_method/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-01T00:00:00+00:00"><meta property="og:site_name" content="iamtu"><meta name=twitter:card content="summary"><meta name=twitter:title content="Neural Odinary Differential Equation - NeuralODE"><meta name=twitter:description content="Problem $$ \begin{equation} \begin{aligned} \text{ODE:}\quad &\begin{cases} \frac{du}{dt} &= f(u, t; \theta) &\text{(Dynamic function)}\\ u(t=0) &= u_0 & \text{(Initial value)} \end{cases} \end{aligned} \end{equation} $$
Minimize loss functional:
$$ \begin{equation} \begin{aligned} \mathcal{J}[\theta] = \int_{0}^T{g(u; \theta)} dt \end{aligned} \end{equation} $$
Formulating as an optimization with ODE constraint: $$ \begin{equation} \begin{aligned} \min_\theta \mathcal{J}(\theta) = \min_\theta \int_0^Tg(u; \theta) dt \\ \text{such that: } \ \frac{du}{dt} - f(u, t;\theta) = 0 \end{aligned} \end{equation} $$
Derivation of the adjoint-state The Lagrangian of the optimization problem"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Neural Odinary Differential Equation - NeuralODE","item":"http://localhost:1313/posts/adjoint_state_method/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Neural Odinary Differential Equation - NeuralODE","name":"Neural Odinary Differential Equation - NeuralODE","description":"Problem $$ \\begin{equation} \\begin{aligned} \\text{ODE:}\\quad \u0026amp;\\begin{cases} \\frac{du}{dt} \u0026amp;= f(u, t; \\theta) \u0026amp;\\text{(Dynamic function)}\\\\ u(t=0) \u0026amp;= u_0 \u0026amp; \\text{(Initial value)} \\end{cases} \\end{aligned} \\end{equation} $$\nMinimize loss functional:\n$$ \\begin{equation} \\begin{aligned} \\mathcal{J}[\\theta] = \\int_{0}^T{g(u; \\theta)} dt \\end{aligned} \\end{equation} $$\nFormulating as an optimization with ODE constraint: $$ \\begin{equation} \\begin{aligned} \\min_\\theta \\mathcal{J}(\\theta) = \\min_\\theta \\int_0^Tg(u; \\theta) dt \\\\ \\text{such that: } \\ \\frac{du}{dt} - f(u, t;\\theta) = 0 \\end{aligned} \\end{equation} $$\nDerivation of the adjoint-state The Lagrangian of the optimization problem","keywords":["learn"],"articleBody":"Problem $$ \\begin{equation} \\begin{aligned} \\text{ODE:}\\quad \u0026\\begin{cases} \\frac{du}{dt} \u0026= f(u, t; \\theta) \u0026\\text{(Dynamic function)}\\\\ u(t=0) \u0026= u_0 \u0026 \\text{(Initial value)} \\end{cases} \\end{aligned} \\end{equation} $$\nMinimize loss functional:\n$$ \\begin{equation} \\begin{aligned} \\mathcal{J}[\\theta] = \\int_{0}^T{g(u; \\theta)} dt \\end{aligned} \\end{equation} $$\nFormulating as an optimization with ODE constraint: $$ \\begin{equation} \\begin{aligned} \\min_\\theta \\mathcal{J}(\\theta) = \\min_\\theta \\int_0^Tg(u; \\theta) dt \\\\ \\text{such that: } \\ \\frac{du}{dt} - f(u, t;\\theta) = 0 \\end{aligned} \\end{equation} $$\nDerivation of the adjoint-state The Lagrangian of the optimization problem\n$$ \\begin{equation} \\begin{aligned} \\mathcal{L}(u, \\lambda;\\theta) \u0026 = \\mathcal{J}(\\theta) + \\underbrace{ \\int_0^T{\\lambda^\\top(t)(f - \\frac{du}{dt}) dt} }_{=0 \\text{ due to the ODE}} \\\\ \u0026 = \\int_0^T{ \\big[g(u;\\theta) + \\lambda^\\top(t)(f - \\frac{du}{dt})\\big]dt } \\end{aligned} \\end{equation} $$\nWe need to compute the the total derivative of loss functional with respect to (w.r.t) \\(\\theta\\):\n$$ \\begin{equation} \\begin{aligned} \\frac{d\\mathcal{L}}{d\\theta} \u0026 = \\frac{d}{d\\theta}\\bigg(\\int_0^T{ \\big(g(u;\\theta) + \\lambda^\\top(t)(f - \\frac{du}{dt})\\big)dt }\\bigg)\\\\ \u0026 = \\int_0^T{ \\frac{\\partial}{\\partial\\theta} \\big(g(u;\\theta) + \\lambda^\\top(t)(f - \\frac{du}{dt})\\big)dt } \\quad \\text{(Leibniz Integral Rule)}\\\\ \u0026 = \\int_0^T {\\big( \\frac{\\partial g}{\\partial \\theta} + \\frac{\\partial g}{\\partial u}\\frac{du}{d\\theta} + \\lambda^\\top(t)( \\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial f}{\\partial u}\\frac{du}{d\\theta} - \\frac{d}{d\\theta}\\frac{du}{dt} ) \\big)dt} \\\\ \u0026 = \\int_0^T{\\big( \\frac{\\partial g}{\\partial\\theta} + \\lambda^\\top(t) \\frac{\\partial f}{\\partial \\theta} + (\\frac{\\partial g}{\\partial u} + \\lambda^\\top(t)\\frac{\\partial f}{\\partial u})\\frac{du}{d\\theta} - \\lambda^\\top(t) \\frac{d}{dt}\\frac{du}{d\\theta} \\big)dt}\\\\ \u0026 = \\int_0^T{\\big( \\frac{\\partial g}{\\partial\\theta} + \\lambda^\\top(t) \\frac{\\partial f}{\\partial \\theta} + (\\frac{\\partial g}{\\partial u} + \\lambda^\\top(t)\\frac{\\partial f}{\\partial u})\\frac{du}{d\\theta} \\red{ - \\lambda^\\top(t) \\frac{d}{dt}\\frac{du}{d\\theta} } \\big)dt} \\end{aligned} \\end{equation} $$\nConsider the integration of the last term:\n$$ \\begin{equation} \\begin{aligned} \\int_0^T { -\\lambda^\\top(t) \\frac{d}{dt}\\frac{du}{d\\theta} dt } \u0026= \\bigg[-\\lambda^\\top(t)\\frac{du}{d\\theta}\\bigg]_0^T + \\int_0^T{\\frac{d\\lambda}{dt}^\\top \\frac{du}{d\\theta}dt} \\\\ \u0026= \\lambda^\\top(0) \\frac{du}{d\\theta}\\big\\vert_{t=0} - \\lambda^\\top(T)\\frac{du)}{d\\theta}\\big\\vert_{t=T} + \\int_0^T{\\frac{d\\lambda}{dt}^\\top \\frac{du}{d\\theta}dt} \\\\ \\end{aligned} \\end{equation} $$\nReplacing result from equation (6) into equation (5):\n$$ \\begin{equation} \\begin{aligned} \\frac{d\\mathcal{L}}{d\\theta} \u0026= \\int_0^T {\\big( \\frac{\\partial g}{\\partial\\theta} + \\lambda^\\top(t)\\frac{\\partial f}{\\partial \\theta} + \\underbrace{ (\\frac{\\partial g}{\\partial u} + \\lambda^\\top(t) \\frac{\\partial f}{\\partial u} - \\frac{d\\lambda}{dt}^\\top)\\frac{du}{d\\theta} }_{A} \\big)dt} \\\\ \u0026 + \\lambda^\\top(0)\\frac{du}{d\\theta}\\big\\vert_{t=0} - \\underbrace{\\lambda^\\top(T)\\frac{du}{d\\theta}\\big\\vert_{t=T}}_B \\end{aligned} \\end{equation} $$\nBecause the Jacobian \\(\\frac{du}{d\\theta}\\) is computationally expensive, we can choose \\(\\lambda(t)\\) such that \\(A\\) and \\(B\\) vanish from equation (7) and compute \\(\\lambda^\\top(0)\\) by solving terminal value ODE:\n$$ \\begin{equation} \\begin{aligned} \u0026 \\begin{cases} \\frac{\\partial g}{\\partial u} + \\lambda^\\top(t) \\frac{\\partial f}{\\partial u} - \\frac{d\\lambda}{dt}^\\top = 0 \\\\ \\lambda^\\top(T) = 0 \\end{cases} \\\\ \\implies \u0026 \\begin{cases} \\frac{d\\lambda}{dt}^\\top = \\frac{\\partial g}{\\partial u} + \\lambda^\\top(t) \\frac{\\partial f}{\\partial u} \\\\ \\lambda^\\top(T) = 0 \\end{cases} \u0026 \\text{\\small(Rearrange)}\\\\ \\implies \u0026 \\begin{cases} \\frac{d\\lambda}{dt} = \\frac{\\partial g}{\\partial u}^\\top + \\frac{\\partial f}{\\partial u}^\\top\\lambda \\\\ \\lambda(T) = 0 \\end{cases} \u0026 \\text{\\small(Transposition both L.H.S and R.H.S)} \\end{aligned} \\end{equation} $$\nThen the gradient of loss functional w.r.t network parameters becames:\n$$ \\begin{equation} \\begin{aligned} \\frac{d\\mathcal{L}}{d\\theta} \u0026= \\int_0^T {\\big( \\frac{\\partial g}{\\partial_\\theta} + \\lambda^\\top(t)\\frac{\\partial f}{\\partial \\theta} \\big)dt} + \\lambda^\\top(0) \\frac{du}{d\\theta}\\big\\vert_{t=0} \\end{aligned} \\end{equation} $$\n\\(\\lambda(t)\\) is called the adjoint-state.\nAppendix Table of notations\n\\(\\vec{u}(t) \\in \\mathbb{R}^N \\) Solution function to initial value ODE (1) \\(g(t): \\mathbb{R}^N \\rightarrow \\mathbb{R} \\) Some loss function (i.e, MSE) \\(f(u, t; \\theta): \\mathbb{R}^{N \\times P} \\rightarrow \\mathbb{R}^N \\) Dynamic function parameterized by \\(\\theta\\), describes the gradient field of state \\(u\\) given its current location \\(\\mathcal{J}: \\mathcal{F}\\rightarrow \\mathbb{R}\\) Loss functional, mapping from loss function onto real number line. ","wordCount":"501","inLanguage":"en","datePublished":"2024-10-01T00:00:00Z","dateModified":"2024-10-01T00:00:00Z","author":{"@type":"Person","name":"Tu T. Do"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/adjoint_state_method/"},"publisher":{"@type":"Organization","name":"iamtu","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="iamtu (Alt + H)">iamtu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/ title=home><span>home</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class=post-title>Neural Odinary Differential Equation - NeuralODE<sup><span class=entry-isdraft>&nbsp;&nbsp;[draft]</span></sup></h1><div class=post-meta><span title='2024-10-01 00:00:00 +0000 UTC'>October 1, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;501 words&nbsp;·&nbsp;Tu T. Do</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#problem aria-label=Problem>Problem</a><ul><li><a href=#formulating-as-an-optimization-with-ode-constraint aria-label="Formulating as an optimization with ODE constraint:">Formulating as an optimization with ODE constraint:</a></li></ul></li><li><a href=#derivation-of-the-adjoint-state aria-label="Derivation of the adjoint-state">Derivation of the adjoint-state</a></li><li><a href=#appendix aria-label=Appendix>Appendix</a></li></ul></div></details></div><div class=post-content><h1 id=problem>Problem<a hidden class=anchor aria-hidden=true href=#problem>#</a></h1><p>$$
\begin{equation}
\begin{aligned}
\text{ODE:}\quad &\begin{cases} \frac{du}{dt} &= f(u, t; \theta) &\text{(Dynamic function)}\\
u(t=0) &= u_0 & \text{(Initial value)}
\end{cases}
\end{aligned}
\end{equation}
$$</p><p>Minimize loss functional:</p><p>$$
\begin{equation}
\begin{aligned}
\mathcal{J}[\theta] = \int_{0}^T{g(u; \theta)} dt
\end{aligned}
\end{equation}
$$</p><h2 id=formulating-as-an-optimization-with-ode-constraint>Formulating as an optimization with ODE constraint:<a hidden class=anchor aria-hidden=true href=#formulating-as-an-optimization-with-ode-constraint>#</a></h2><p>$$
\begin{equation}
\begin{aligned}
\min_\theta \mathcal{J}(\theta) = \min_\theta \int_0^Tg(u; \theta) dt \\
\text{such that: } \
\frac{du}{dt} - f(u, t;\theta) = 0
\end{aligned}
\end{equation}
$$</p><h1 id=derivation-of-the-adjoint-state>Derivation of the adjoint-state<a hidden class=anchor aria-hidden=true href=#derivation-of-the-adjoint-state>#</a></h1><p>The Lagrangian of the optimization problem</p><p>$$
\begin{equation}
\begin{aligned}
\mathcal{L}(u, \lambda;\theta) & =
\mathcal{J}(\theta) +
\underbrace{
\int_0^T{\lambda^\top(t)(f - \frac{du}{dt}) dt}
}_{=0 \text{ due to the ODE}} \\
& = \int_0^T{
\big[g(u;\theta) + \lambda^\top(t)(f - \frac{du}{dt})\big]dt
}
\end{aligned}
\end{equation}
$$</p><p>We need to compute the the total derivative of loss functional with respect to (w.r.t) \(\theta\):</p><p>$$
\begin{equation}
\begin{aligned}
\frac{d\mathcal{L}}{d\theta} & = \frac{d}{d\theta}\bigg(\int_0^T{
\big(g(u;\theta) + \lambda^\top(t)(f - \frac{du}{dt})\big)dt
}\bigg)\\
& = \int_0^T{
\frac{\partial}{\partial\theta}
\big(g(u;\theta) + \lambda^\top(t)(f - \frac{du}{dt})\big)dt
} \quad \text{(Leibniz Integral Rule)}\\
& = \int_0^T {\big(
\frac{\partial g}{\partial \theta} + \frac{\partial g}{\partial u}\frac{du}{d\theta}
+ \lambda^\top(t)(
\frac{\partial f}{\partial \theta}
+ \frac{\partial f}{\partial u}\frac{du}{d\theta}
- \frac{d}{d\theta}\frac{du}{dt}
)
\big)dt} \\
& = \int_0^T{\big(
\frac{\partial g}{\partial\theta} + \lambda^\top(t) \frac{\partial f}{\partial \theta}
+ (\frac{\partial g}{\partial u} + \lambda^\top(t)\frac{\partial f}{\partial u})\frac{du}{d\theta}
- \lambda^\top(t) \frac{d}{dt}\frac{du}{d\theta}
\big)dt}\\
& = \int_0^T{\big(
\frac{\partial g}{\partial\theta} + \lambda^\top(t) \frac{\partial f}{\partial \theta}
+ (\frac{\partial g}{\partial u} + \lambda^\top(t)\frac{\partial f}{\partial u})\frac{du}{d\theta}
\red{
- \lambda^\top(t) \frac{d}{dt}\frac{du}{d\theta}
}
\big)dt}
\end{aligned}
\end{equation}
$$</p><p>Consider the integration of the last term:</p><p>$$
\begin{equation}
\begin{aligned}
\int_0^T {
-\lambda^\top(t) \frac{d}{dt}\frac{du}{d\theta} dt
} &= \bigg[-\lambda^\top(t)\frac{du}{d\theta}\bigg]_0^T + \int_0^T{\frac{d\lambda}{dt}^\top \frac{du}{d\theta}dt} \\
&= \lambda^\top(0) \frac{du}{d\theta}\big\vert_{t=0} - \lambda^\top(T)\frac{du)}{d\theta}\big\vert_{t=T} + \int_0^T{\frac{d\lambda}{dt}^\top \frac{du}{d\theta}dt} \\
\end{aligned}
\end{equation}
$$</p><p>Replacing result from equation (6) into equation (5):</p><p>$$
\begin{equation}
\begin{aligned}
\frac{d\mathcal{L}}{d\theta}
&= \int_0^T {\big(
\frac{\partial g}{\partial\theta} + \lambda^\top(t)\frac{\partial f}{\partial \theta}
+ \underbrace{
(\frac{\partial g}{\partial u} + \lambda^\top(t) \frac{\partial f}{\partial u} - \frac{d\lambda}{dt}^\top)\frac{du}{d\theta}
}_{A}
\big)dt} \\
& + \lambda^\top(0)\frac{du}{d\theta}\big\vert_{t=0} -
\underbrace{\lambda^\top(T)\frac{du}{d\theta}\big\vert_{t=T}}_B
\end{aligned}
\end{equation}
$$</p><p>Because the Jacobian \(\frac{du}{d\theta}\) is computationally expensive, we can choose \(\lambda(t)\) such that \(A\) and \(B\) vanish from equation (7) and compute \(\lambda^\top(0)\) by solving terminal value ODE:</p><p>$$
\begin{equation}
\begin{aligned}
& \begin{cases}
\frac{\partial g}{\partial u} + \lambda^\top(t) \frac{\partial f}{\partial u} - \frac{d\lambda}{dt}^\top = 0 \\
\lambda^\top(T) = 0
\end{cases} \\
\implies & \begin{cases}
\frac{d\lambda}{dt}^\top = \frac{\partial g}{\partial u} + \lambda^\top(t) \frac{\partial f}{\partial u} \\
\lambda^\top(T) = 0
\end{cases} & \text{\small(Rearrange)}\\
\implies & \begin{cases}
\frac{d\lambda}{dt} = \frac{\partial g}{\partial u}^\top + \frac{\partial f}{\partial u}^\top\lambda \\
\lambda(T) = 0
\end{cases} & \text{\small(Transposition both L.H.S and R.H.S)}
\end{aligned}
\end{equation}
$$</p><p>Then the gradient of loss functional w.r.t network parameters becames:</p><p>$$
\begin{equation}
\begin{aligned}
\frac{d\mathcal{L}}{d\theta} &= \int_0^T {\big(
\frac{\partial g}{\partial_\theta} + \lambda^\top(t)\frac{\partial f}{\partial \theta}
\big)dt} + \lambda^\top(0) \frac{du}{d\theta}\big\vert_{t=0}
\end{aligned}
\end{equation}
$$</p><p>\(\lambda(t)\) is called the adjoint-state.</p><h1 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h1><p><strong>Table of notations</strong></p><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>\(\vec{u}(t) \in \mathbb{R}^N \)</td><td>Solution function to initial value ODE (1)</td></tr><tr><td>\(g(t): \mathbb{R}^N \rightarrow \mathbb{R} \)</td><td>Some loss function (i.e, MSE)</td></tr><tr><td>\(f(u, t; \theta): \mathbb{R}^{N \times P} \rightarrow \mathbb{R}^N \)</td><td>Dynamic function parameterized by \(\theta\), describes the gradient field of state \(u\) given its current location</td></tr><tr><td>\(\mathcal{J}: \mathcal{F}\rightarrow \mathbb{R}\)</td><td>Loss functional, mapping from loss function onto real number line.</td></tr></tbody></table></div><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://iamtu-dev.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/learn/>Learn</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/ode_solver/><span class=title>Next »</span><br><span>Numerical Integrations</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>iamtu</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>