<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding Adjoint sensitivity method | iamtu</title>
<meta name=keywords content="learn"><meta name=description content="Note
$$
\red{\text{==================editing====================}}
$$
Suppose we have a dataset \({(t_i, u_{t_i})}_{i=0\cdots N-1}, \quad u_{t_i} \in \mathbb{R}^N\) is the observed state of a dynamical system given by ODE
\begin{equation}
\begin{aligned}
\begin{cases}
u(t=t_i) = u_{t_i} \
\frac{du}{dt} = f(u, t, \theta)
\end{cases}
\end{aligned}
\end{equation}
For simplicity, assume we only have 2 observed states \((t_0, u_0), (t_1, u_1)\). So that we can write \(u_1\) in term of \(u_0\) and the dynamic \(f\)
$$
\begin{equation}
\begin{aligned}
u(t_1) = u_0 + \int_{t_0}^{t_1}{f(u, t, \theta) dt}
\end{aligned}
\end{equation}
$$"><meta name=author content="Tu T. Do"><link rel=canonical href=http://localhost:1313/posts/adjoint_state_method/><meta name=google-site-verification content="G-PWLR4FLELZ"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/adjoint_state_method/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PWLR4FLELZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PWLR4FLELZ")}</script><meta property="og:title" content="Understanding Adjoint sensitivity method"><meta property="og:description" content="Note
$$
\red{\text{==================editing====================}}
$$
Suppose we have a dataset \({(t_i, u_{t_i})}_{i=0\cdots N-1}, \quad u_{t_i} \in \mathbb{R}^N\) is the observed state of a dynamical system given by ODE
\begin{equation}
\begin{aligned}
\begin{cases}
u(t=t_i) = u_{t_i} \
\frac{du}{dt} = f(u, t, \theta)
\end{cases}
\end{aligned}
\end{equation}
For simplicity, assume we only have 2 observed states \((t_0, u_0), (t_1, u_1)\). So that we can write \(u_1\) in term of \(u_0\) and the dynamic \(f\)
$$
\begin{equation}
\begin{aligned}
u(t_1) = u_0 + \int_{t_0}^{t_1}{f(u, t, \theta) dt}
\end{aligned}
\end{equation}
$$"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/adjoint_state_method/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-01T00:00:00+00:00"><meta property="og:site_name" content="iamtu"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding Adjoint sensitivity method"><meta name=twitter:description content="Note
$$
\red{\text{==================editing====================}}
$$
Suppose we have a dataset \({(t_i, u_{t_i})}_{i=0\cdots N-1}, \quad u_{t_i} \in \mathbb{R}^N\) is the observed state of a dynamical system given by ODE
\begin{equation}
\begin{aligned}
\begin{cases}
u(t=t_i) = u_{t_i} \
\frac{du}{dt} = f(u, t, \theta)
\end{cases}
\end{aligned}
\end{equation}
For simplicity, assume we only have 2 observed states \((t_0, u_0), (t_1, u_1)\). So that we can write \(u_1\) in term of \(u_0\) and the dynamic \(f\)
$$
\begin{equation}
\begin{aligned}
u(t_1) = u_0 + \int_{t_0}^{t_1}{f(u, t, \theta) dt}
\end{aligned}
\end{equation}
$$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Understanding Adjoint sensitivity method","item":"http://localhost:1313/posts/adjoint_state_method/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding Adjoint sensitivity method","name":"Understanding Adjoint sensitivity method","description":"Note $$ \\red{\\text{==================editing====================}} $$\nSuppose we have a dataset \\({(t_i, u_{t_i})}_{i=0\\cdots N-1}, \\quad u_{t_i} \\in \\mathbb{R}^N\\) is the observed state of a dynamical system given by ODE\n\\begin{equation} \\begin{aligned} \\begin{cases} u(t=t_i) = u_{t_i} \\ \\frac{du}{dt} = f(u, t, \\theta) \\end{cases} \\end{aligned} \\end{equation}\nFor simplicity, assume we only have 2 observed states \\((t_0, u_0), (t_1, u_1)\\). So that we can write \\(u_1\\) in term of \\(u_0\\) and the dynamic \\(f\\)\n$$ \\begin{equation} \\begin{aligned} u(t_1) = u_0 + \\int_{t_0}^{t_1}{f(u, t, \\theta) dt} \\end{aligned} \\end{equation} $$\n","keywords":["learn"],"articleBody":"Note $$ \\red{\\text{==================editing====================}} $$\nSuppose we have a dataset \\({(t_i, u_{t_i})}_{i=0\\cdots N-1}, \\quad u_{t_i} \\in \\mathbb{R}^N\\) is the observed state of a dynamical system given by ODE\n\\begin{equation} \\begin{aligned} \\begin{cases} u(t=t_i) = u_{t_i} \\ \\frac{du}{dt} = f(u, t, \\theta) \\end{cases} \\end{aligned} \\end{equation}\nFor simplicity, assume we only have 2 observed states \\((t_0, u_0), (t_1, u_1)\\). So that we can write \\(u_1\\) in term of \\(u_0\\) and the dynamic \\(f\\)\n$$ \\begin{equation} \\begin{aligned} u(t_1) = u_0 + \\int_{t_0}^{t_1}{f(u, t, \\theta) dt} \\end{aligned} \\end{equation} $$\nMinimize the loss function \\(g: \\mathbb{R}^N \\rightarrow \\mathbb{R}; g(u(t_1))\\) such that \\(\\frac{du}{dt} = f;\\quad \\forall t \\in [t_0, t_1]\\)\nWe can write the constrainted optimization problem as unconstrainted one using Lagrangian multiplier $\\lambda(t)$\n$$ \\begin{equation} \\begin{aligned} L(u(t_1), \\lambda, \\theta) = g(u(t_1)) + \\int_{t_0}^{t_1}{\\lambda(t)(f - \\frac{du}{dt} )dt} \\end{aligned} \\end{equation} $$\nWe need to compute the total derivative $\\frac{dL}{d\\theta}$ in order to minimze functional $L$\n$$ \\begin{equation} \\begin{aligned} \\frac{dL}{d\\theta} \u0026 = \\frac{d}{d\\theta}\\bigg( g(u(t_1)) + \\int_{t_0}^{t_1}{\\lambda(t)(f - \\frac{du}{dt} )dt} \\bigg) \\\\ \u0026 = \\frac{d}{d\\theta} g(u(t_1), \\theta) + \\int_{t_0}^{t_1}{\\frac{d}{d\\theta}\\big( \\lambda(t)(f - \\frac{du}{dt}) \\big) dt} \\\\ \u0026 = \\frac{\\partial g}{\\partial u(t_1)} \\frac{d u(t_1)}{d\\theta} + \\int_{t_0}^{t_1}{\\lambda(t)\\frac{d}{d\\theta}\\big( f - \\frac{du}{dt} \\big) dt} \\\\ \u0026 = \\frac{\\partial g}{\\partial u(t_1)} \\frac{d}{d\\theta}\\bigg( u_0 + \\int_{t_0}^{t_1}{f(u, t, \\theta) dt} \\bigg) + \\int_{t_0}^{t_1}{\\lambda(t)\\frac{d}{d\\theta}\\big( f - \\frac{du}{dt} \\big) dt} \\\\ \u0026 = \\frac{\\partial g}{\\partial u(t_1)} \\int_{t_0}^{t_1}{\\big( \\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial f}{\\partial u} \\frac{du}{d\\theta} \\big)dt} + \\int_{t_0}^{t_1}{\\lambda(t)\\big( \\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial f}{\\partial u} \\frac{du}{d\\theta} - \\frac{d}{d\\theta}\\frac{du}{dt} \\big) dt} \\\\ \u0026 = \\int_{t_0}^{t_1}{ \\bigg( \\frac{\\partial g}{\\partial u(t_1)} \\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial g}{\\partial u(t_1)} \\frac{\\partial f}{\\partial u}\\frac{du}{d\\theta} + \\lambda(t) \\frac{\\partial f}{\\partial \\theta} + \\lambda(t)\\frac{\\partial f}{\\partial u}\\frac{du}{d\\theta} \\underbrace{ - \\lambda(t) \\frac{d}{dt}\\frac{du}{d\\theta} }_{A} \\bigg) dt } \\end{aligned} \\end{equation} $$\nConsider the integral of term $A$\n$$ \\begin{equation} \\begin{aligned} \\int_{t_0}^{t_1}{- \\lambda(t) \\frac{d}{dt}\\frac{du}{d\\theta} dt} = [\\lambda(t)\\frac{du}{d\\theta}]_{t_1}^{t_0} + \\int_{t_0}^{t_1}{\\frac{d\\lambda}{dt}\\frac{du}{d\\theta}dt} \\end{aligned} \\end{equation} $$\nReplace eq.(\\ref{eq:term_a}) into eq.(\\ref{eq:sensitivity}) we have:\n$$ \\begin{equation} \\begin{aligned} \\frac{dL}{d\\theta} \u0026= \\int_{t_0}^{t_1} {\\bigg( \\frac{\\partial g}{\\partial u(t_1)} + \\lambda(t) \\bigg)\\frac{\\partial f}{\\partial \\theta}dt} \\\\ \u0026 + \\int_{t_0}^{t_1} {\\bigg( \\frac{\\partial g}{\\partial u(t_1)}\\frac{\\partial f}{\\partial u} + \\lambda(t) \\frac{\\partial f}{\\partial u} + \\frac{d\\lambda}{dt} \\bigg)\\red{\\frac{du}{d\\theta}}dt} \\\\ \u0026 + \\lambda(t_0)\\frac{du(t_0)}{d\\theta} - \\red{\\lambda(t_1)\\frac{du(t_1)}{d\\theta}} \\end{aligned} \\end{equation} $$\nBecause $\\frac{du}{d\\theta}$ is computationally expensive, we can choose $\\lambda(t)$ so that the second and forth term vanish so that we have another ODE\n$$ \\begin{equation} \\begin{aligned} \\begin{cases} \\frac{d\\lambda}{dt} = -\\big( \\frac{\\partial g}{\\partial u(t_1)} + \\lambda(t) \\big)\\frac{\\partial f}{\\partial u} \\\\ \\lambda(t_1) = \\vec{0} \\end{cases} \\end{aligned} \\end{equation} $$\nLet $a(t) = \\lambda(t) + \\frac{\\partial g}{\\partial u(t_1)}$, eq.(\\ref{eq:adjoint_ode}) becomes:\n$$ \\begin{equation} \\begin{aligned} \\begin{cases} \\frac{da}{dt} = -a(t)\\frac{\\partial f}{\\partial u} \\\\ a(t_1) = \\frac{\\partial g}{\\partial u(t_1)} \\end{cases} \\end{aligned} \\end{equation} $$\nAnd eq.(\\ref{eq:sensitivity_2}) becomes:\n\\begin{equation} \\begin{aligned} \\frac{dL}{d\\theta} = \\big(a(t_0) - \\frac{\\partial g}{\\partial u(t_1)}\\big)\\frac{du(t_0)}{d\\theta} + \\int_{t_0}^{t_1}{a(t)\\frac{\\partial f}{\\partial\\theta} dt} \\end{aligned} \\end{equation}\nReferences Efficient gradient computation for dynamical models ","wordCount":"445","inLanguage":"en","datePublished":"2024-10-01T00:00:00Z","dateModified":"2024-10-01T00:00:00Z","author":{"@type":"Person","name":"Tu T. Do"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/adjoint_state_method/"},"publisher":{"@type":"Organization","name":"iamtu","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="iamtu (Alt + H)">iamtu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/ title=home><span>home</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class=post-title>Understanding Adjoint sensitivity method</h1><div class=post-meta><span title='2024-10-01 00:00:00 +0000 UTC'>October 1, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;445 words&nbsp;·&nbsp;Tu T. Do</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#note aria-label=Note>Note</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h1 id=note>Note<a hidden class=anchor aria-hidden=true href=#note>#</a></h1><p>$$
\red{\text{==================editing====================}}
$$</p><p>Suppose we have a dataset \({(t_i, u_{t_i})}_{i=0\cdots N-1}, \quad u_{t_i} \in \mathbb{R}^N\) is the observed state of a dynamical system given by ODE</p><p>\begin{equation}
\begin{aligned}
\begin{cases}
u(t=t_i) = u_{t_i} \
\frac{du}{dt} = f(u, t, \theta)
\end{cases}
\end{aligned}
\end{equation}</p><p>For simplicity, assume we only have 2 observed states \((t_0, u_0), (t_1, u_1)\). So that we can write \(u_1\) in term of \(u_0\) and the dynamic \(f\)</p><p>$$
\begin{equation}
\begin{aligned}
u(t_1) = u_0 + \int_{t_0}^{t_1}{f(u, t, \theta) dt}
\end{aligned}
\end{equation}
$$</p><p>Minimize the loss function \(g: \mathbb{R}^N \rightarrow \mathbb{R}; g(u(t_1))\) such that \(\frac{du}{dt} = f;\quad \forall t \in [t_0, t_1]\)</p><p>We can write the constrainted optimization problem as unconstrainted one using Lagrangian multiplier $\lambda(t)$</p><p>$$
\begin{equation}
\begin{aligned}
L(u(t_1), \lambda, \theta) = g(u(t_1)) + \int_{t_0}^{t_1}{\lambda(t)(f - \frac{du}{dt} )dt}
\end{aligned}
\end{equation}
$$</p><p>We need to compute the total derivative $\frac{dL}{d\theta}$ in order to minimze functional $L$</p><p>$$
\begin{equation}
\begin{aligned}
\frac{dL}{d\theta} & = \frac{d}{d\theta}\bigg(
g(u(t_1)) + \int_{t_0}^{t_1}{\lambda(t)(f - \frac{du}{dt} )dt}
\bigg) \\
& = \frac{d}{d\theta} g(u(t_1), \theta) + \int_{t_0}^{t_1}{\frac{d}{d\theta}\big(
\lambda(t)(f - \frac{du}{dt})
\big) dt} \\
& = \frac{\partial g}{\partial u(t_1)} \frac{d u(t_1)}{d\theta}
+ \int_{t_0}^{t_1}{\lambda(t)\frac{d}{d\theta}\big(
f - \frac{du}{dt}
\big) dt} \\
& = \frac{\partial g}{\partial u(t_1)} \frac{d}{d\theta}\bigg(
u_0 + \int_{t_0}^{t_1}{f(u, t, \theta) dt}
\bigg) + \int_{t_0}^{t_1}{\lambda(t)\frac{d}{d\theta}\big(
f - \frac{du}{dt}
\big) dt} \\
& = \frac{\partial g}{\partial u(t_1)} \int_{t_0}^{t_1}{\big(
\frac{\partial f}{\partial \theta} + \frac{\partial f}{\partial u} \frac{du}{d\theta}
\big)dt} +
\int_{t_0}^{t_1}{\lambda(t)\big(
\frac{\partial f}{\partial \theta} + \frac{\partial f}{\partial u} \frac{du}{d\theta}
- \frac{d}{d\theta}\frac{du}{dt}
\big) dt} \\
& = \int_{t_0}^{t_1}{
\bigg(
\frac{\partial g}{\partial u(t_1)} \frac{\partial f}{\partial \theta}
+ \frac{\partial g}{\partial u(t_1)} \frac{\partial f}{\partial u}\frac{du}{d\theta}
+ \lambda(t) \frac{\partial f}{\partial \theta}
+ \lambda(t)\frac{\partial f}{\partial u}\frac{du}{d\theta}
\underbrace{
- \lambda(t) \frac{d}{dt}\frac{du}{d\theta}
}_{A}
\bigg) dt
}
\end{aligned}
\end{equation}
$$</p><p>Consider the integral of term $A$</p><p>$$
\begin{equation}
\begin{aligned}
\int_{t_0}^{t_1}{- \lambda(t) \frac{d}{dt}\frac{du}{d\theta} dt}
= [\lambda(t)\frac{du}{d\theta}]_{t_1}^{t_0} + \int_{t_0}^{t_1}{\frac{d\lambda}{dt}\frac{du}{d\theta}dt}
\end{aligned}
\end{equation}
$$</p><p>Replace eq.(\ref{eq:term_a}) into eq.(\ref{eq:sensitivity}) we have:</p><p>$$
\begin{equation}
\begin{aligned}
\frac{dL}{d\theta} &=
\int_{t_0}^{t_1} {\bigg(
\frac{\partial g}{\partial u(t_1)} + \lambda(t)
\bigg)\frac{\partial f}{\partial \theta}dt} \\
& + \int_{t_0}^{t_1} {\bigg(
\frac{\partial g}{\partial u(t_1)}\frac{\partial f}{\partial u}
+ \lambda(t) \frac{\partial f}{\partial u} + \frac{d\lambda}{dt}
\bigg)\red{\frac{du}{d\theta}}dt} \\
& + \lambda(t_0)\frac{du(t_0)}{d\theta} - \red{\lambda(t_1)\frac{du(t_1)}{d\theta}}
\end{aligned}
\end{equation}
$$</p><p>Because $\frac{du}{d\theta}$ is computationally expensive, we can choose $\lambda(t)$ so that the second and forth term vanish so that we have another ODE</p><p>$$
\begin{equation}
\begin{aligned}
\begin{cases}
\frac{d\lambda}{dt} = -\big(
\frac{\partial g}{\partial u(t_1)} + \lambda(t)
\big)\frac{\partial f}{\partial u} \\
\lambda(t_1) = \vec{0}
\end{cases}
\end{aligned}
\end{equation}
$$</p><p>Let $a(t) = \lambda(t) + \frac{\partial g}{\partial u(t_1)}$, eq.(\ref{eq:adjoint_ode}) becomes:</p><p>$$
\begin{equation}
\begin{aligned}
\begin{cases}
\frac{da}{dt} = -a(t)\frac{\partial f}{\partial u} \\
a(t_1) = \frac{\partial g}{\partial u(t_1)}
\end{cases}
\end{aligned}
\end{equation}
$$</p><p>And eq.(\ref{eq:sensitivity_2}) becomes:</p><p>\begin{equation}
\begin{aligned}
\frac{dL}{d\theta} =
\big(a(t_0) - \frac{\partial g}{\partial u(t_1)}\big)\frac{du(t_0)}{d\theta}
+ \int_{t_0}^{t_1}{a(t)\frac{\partial f}{\partial\theta} dt}
\end{aligned}
\end{equation}</p><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><ul><li><a href=https://www.sciencedirect.com/science/article/pii/S1053811914003097>Efficient gradient computation for dynamical models</a></li></ul></div><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://iamtu-dev.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/learn/>Learn</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/ode_solver/><span class=title>Next »</span><br><span>Numerical Integrations</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>iamtu</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>