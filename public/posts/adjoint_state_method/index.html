<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding NeuralODE | iamtu</title>
<meta name=keywords content="learn"><meta name=description content="Motivation This blog post is my note taken when studying Neural Oridinary Differential Equation (NeuralODE), which was proposed in Neural Ordinary Differential Equations. The goal of this note is to understand the formulation, some mathematical derivations, and some techincal difficulties encounter when implementing NeuralODE in JAX.
The entire learning process is quite fascinating and introduced me to some new mathematical concepts such as Euler-Lagrange equation, Continuous Lagrangian Multiplier. NeuralODE is a important entry point to Physics Informed Machine Learning and I struggled for quite sometime to understand."><meta name=author content="Tu T. Do"><link rel=canonical href=http://localhost:1313/posts/adjoint_state_method/><meta name=google-site-verification content="G-PWLR4FLELZ"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=http://localhost:1313/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/adjoint_state_method/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PWLR4FLELZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PWLR4FLELZ")}</script><meta property="og:title" content="Understanding NeuralODE"><meta property="og:description" content="Motivation This blog post is my note taken when studying Neural Oridinary Differential Equation (NeuralODE), which was proposed in Neural Ordinary Differential Equations. The goal of this note is to understand the formulation, some mathematical derivations, and some techincal difficulties encounter when implementing NeuralODE in JAX.
The entire learning process is quite fascinating and introduced me to some new mathematical concepts such as Euler-Lagrange equation, Continuous Lagrangian Multiplier. NeuralODE is a important entry point to Physics Informed Machine Learning and I struggled for quite sometime to understand."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/adjoint_state_method/"><meta property="og:image" content="http://localhost:1313/images/neural_ode_rs.png"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-11-13T00:00:00+00:00"><meta property="article:modified_time" content="2024-11-13T00:00:00+00:00"><meta property="og:site_name" content="iamtu"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/neural_ode_rs.png"><meta name=twitter:title content="Understanding NeuralODE"><meta name=twitter:description content="Motivation This blog post is my note taken when studying Neural Oridinary Differential Equation (NeuralODE), which was proposed in Neural Ordinary Differential Equations. The goal of this note is to understand the formulation, some mathematical derivations, and some techincal difficulties encounter when implementing NeuralODE in JAX.
The entire learning process is quite fascinating and introduced me to some new mathematical concepts such as Euler-Lagrange equation, Continuous Lagrangian Multiplier. NeuralODE is a important entry point to Physics Informed Machine Learning and I struggled for quite sometime to understand."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Understanding NeuralODE","item":"http://localhost:1313/posts/adjoint_state_method/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding NeuralODE","name":"Understanding NeuralODE","description":"Motivation This blog post is my note taken when studying Neural Oridinary Differential Equation (NeuralODE), which was proposed in Neural Ordinary Differential Equations. The goal of this note is to understand the formulation, some mathematical derivations, and some techincal difficulties encounter when implementing NeuralODE in JAX.\nThe entire learning process is quite fascinating and introduced me to some new mathematical concepts such as Euler-Lagrange equation, Continuous Lagrangian Multiplier. NeuralODE is a important entry point to Physics Informed Machine Learning and I struggled for quite sometime to understand.","keywords":["learn"],"articleBody":" Motivation This blog post is my note taken when studying Neural Oridinary Differential Equation (NeuralODE), which was proposed in Neural Ordinary Differential Equations. The goal of this note is to understand the formulation, some mathematical derivations, and some techincal difficulties encounter when implementing NeuralODE in JAX.\nThe entire learning process is quite fascinating and introduced me to some new mathematical concepts such as Euler-Lagrange equation, Continuous Lagrangian Multiplier. NeuralODE is a important entry point to Physics Informed Machine Learning and I struggled for quite sometime to understand. So I hope this post might save sometime for people who are also trying to understand NeuralODE.\nNeuralODE Formulation Many systems can be written as a ODE’s initial value problem:\n$$ \\begin{equation} \\begin{cases} \\frac{d\\mathbf{u}}{dt} = f(\\mathbf{u}, t) \\\\ \\mathbf{u}(t=0) = \\mathbf{u}_0 \\end{cases} \\end{equation} $$\nWhere:\n\\(\\mathbf{u} \\in \\mathbb{R}^N\\) describes the state of the system. \\(\\mathbf{u}(t)\\) is the state of the system at time \\(t\\). \\(f: \\mathbb{R}^N \\rightarrow \\mathbb{R}^N\\) is a dynamic function that characterizes the system. Which means, at any given state of the system, \\(f\\) tells us the rate of change for that state \\(\\frac{d\\mathbf{u}}{dt}\\). When \\(f\\) is given, this system of equations can be solved by numerical methods such as Euler, Predictor Corrector Method, or RK45 (see my previous post on ODE Integrator).\nNeuralODE solves the inverse problem, given some observations of the system state \\(\\{(\\mathbf{u}_i, t_i)\\}_{i=1\\cdots N}\\) (which can be irregularly sampled), find the dynamic function \\(f\\). NeuralODE parameterize \\(f\\) by a neural-network \\(f_\\theta\\), where \\(\\theta \\in \\mathbb{R}^P\\) is the set of parameters of the network. For simplicity, assume that we have only two observations at \\(t=t_0\\) and \\(t=t_1\\). NeuralODE’s prediction of the system state at \\(t=t_1\\) are given by:\n$$ \\begin{equation} \\hat{\\mathbf{u}}_1 = \\mathbf{u}_0 + \\int_{t_0}^{t_1}{f(\\mathbf{u}, t;\\theta)dt} \\end{equation} $$\nNeuralODE learns the parameter \\(\\theta\\) by minimizing the difference between its prediction and groundtruth \\(\\mathbf{u}_1\\):\n$$ L(\\hat{\\mathbf{u}}_1, \\mathbf{u}_1) = \\text{MSE}(\\mathbf{u}_1, \\mathbf{\\hat{u}}_1) $$\nFinally, NeuralODE can be formulated as a constrained optimization problem:\n$$ \\begin{equation} \\begin{aligned} \u0026 \\min_\\theta L(\\hat{\\mathbf{u}}_1, \\mathbf{u}_1) \\\\ \\text{such that: } \u0026 \\frac{d\\mathbf{u}}{dt} = f(\\mathbf{u}, t;\\theta)\\quad \\forall t_0\\leq t \\leq t_1 \\end{aligned} \\end{equation} $$\nIn order to solve the optimization problem in equation 3), we need to compute the sensitivity \\(\\frac{dL}{d\\theta}\\). The following sections will discuss the method of computing this quantity.\nThe forward sensitivity method The forward sensitivity method is a straight forward method of computing \\(\\frac{dL}{d\\theta}\\):\n$$ \\begin{equation} \\begin{aligned} \\frac{dL}{d\\theta} \u0026 = \\frac{d}{d\\theta} L(\\mathbf{u}_1, \\mathbf{\\hat{u}}_1) \\\\ \u0026 = \\underbrace{\\frac{\\partial L}{\\partial \\mathbf{\\hat{u}}_1}}_{1\\times N} \\underbrace{\\frac{d\\mathbf{\\hat{u}_1}}{d\\theta}}_{N\\times P} \\end{aligned} \\end{equation} $$\nBy differentiating both L.H.S and R.H.S of equation (1) with respect to \\(\\theta\\), we have:\n$$ \\begin{equation} \\begin{aligned} \u0026 \\frac{d}{d\\theta}(\\frac{d\\mathbf{u}}{dt}) = \\frac{d}{d\\theta}f(\\mathbf{u}, t; \\theta) \\\\ \\iff \u0026 \\frac{d}{dt}(\\frac{d\\mathbf{u}}{d\\theta}) = \\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial \\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta} \\end{aligned} \\end{equation} $$\nEquation (5) gaves us a system of ODE initial value problem, which consists of \\(N\\times P\\) individual ODE. We can see that by denoting \\(A = \\frac{d\\mathbf{u}}{d\\theta} \\in \\mathbb{R}^N\\) is the Jacobian of state \\(u\\) with respect to parameter \\(\\theta\\), \\(A_{ij} = \\frac{\\partial u_i}{\\partial\\theta_j}\\). To solve for \\(\\frac{d\\mathbf{\\hat{u}_1}}{d\\theta}\\), we solve \\(N\\times P\\) individual ODE initial value problems with initial value is \\(0\\) (since \\(\\mathbf{u}_0\\) doesn’t depend on \\(\\theta\\)). The forward sensitivity method is computationally prohibited for the medium to large neural networks with thousands of parameters.\nEquiped with this understanding, we can fully appreciate the adjoint sensitivity method, which is the key to understand NeuralODE.\nThe adjoint state method We can write the constrainted optimization problem in equation (3) as unconstrainted one using continuous Lagrangian multiplier \\(\\lambda(t)\\):\n$$ \\begin{equation} \\begin{aligned} J(\\mathbf{\\hat{u}_1}, \\lambda, \\theta) = L(\\mathbf{\\hat{u}_1}) + \\int_{t_0}^{t_1}{\\lambda(t)(f - \\frac{d\\mathbf{u}}{dt} )dt} \\end{aligned} \\end{equation} $$\nWhere \\(\\lambda \\in \\mathbb{R}^N\\) is the Lagrangian multiplier in the form of function of time. Take the derivative with respect to \\(\\theta\\) for L.H.S and R.H.S in equation (6):\n$$ \\begin{equation} \\begin{aligned} \\frac{dJ}{d\\theta} \u0026 = \\frac{d}{d\\theta}\\bigg( L(\\mathbf{\\hat{u}_1}) + \\int_{t_0}^{t_1}{\\lambda(t)(f - \\frac{d\\mathbf{u}}{dt} )dt} \\bigg) \\\\ \u0026 = \\frac{d}{d\\theta} L(\\mathbf{\\hat{u}_1}) + \\int_{t_0}^{t_1}{ \\frac{d}{d\\theta} \\lambda(f - \\frac{d\\mathbf{u}}{dt}) dt} \\end{aligned} \\end{equation} $$\nConsidering the first term in R.H.S of equation (7):\n$$ \\begin{equation} \\begin{aligned} \\frac{d}{d\\theta}L(\\mathbf{\\hat{u}}_1) \u0026= \\frac{\\partial L}{\\partial \\mathbf{\\hat{u}_1}} \\frac{d}{d\\theta}( \\mathbf{u}_0 + \\int_{t_0}^{t_1}{f(\\mathbf{u}, t; \\theta)dt}) \\\\ \u0026 = \\frac{\\partial L}{\\partial \\mathbf{\\hat{u}_1}} \\int_{t_0}^{t_1}{ \\bigg(\\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta}\\bigg)dt } \\end{aligned} \\end{equation} $$\nAnd considering the second term of the R.H.S of equation (7):\n$$ \\begin{equation} \\begin{aligned} \u0026 \\int_{t_0}^{t_1}{ \\frac{d}{d\\theta}\\lambda \\bigg( f(\\mathbf{u}, t;\\theta) - \\frac{d\\mathbf{u}}{dt} \\bigg) dt }\\\\ = \u0026 \\int_{t_0}^{t_1}{ \\lambda \\bigg( \\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta} - \\frac{d}{d\\theta}(\\frac{d\\mathbf{u}}{dt}) \\bigg) dt }\\\\ = \u0026 \\int_{t_0}^{t_1}{ \\lambda \\bigg( \\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta} \\bigg) dt } - \\blue{ \\int_{t_0}^{t_1}{ \\lambda\\frac{d}{d\\theta}(\\frac{d\\mathbf{u}}{dt}) dt } } \\end{aligned} \\end{equation} $$\nThe final term in equation (9) can be evaluated by integration by parts after swapping the order of derivation:\n$$ \\begin{equation} \\begin{aligned} \u0026 \\int_{t_0}^{t_1}{ \\lambda\\frac{d}{d\\theta}(\\frac{d\\mathbf{u}}{dt}) dt } \\\\ = \u0026 \\int_{t_0}^{t_1}{ \\lambda\\frac{d}{dt}(\\frac{d\\mathbf{u}}{d\\theta}) dt } \\\\ = \u0026 \\bigg[\\lambda \\frac{d\\mathbf{u}}{d\\theta}\\bigg]_{t_0}^{t_1} - \\int_{t_0}^{t_1}{\\frac{d\\lambda}{dt}\\frac{d\\mathbf{u}}{d\\theta}dt}\\\\ = \u0026 \\lambda(t_1) \\frac{d\\mathbf{u}}{d\\theta}\\bigg\\vert_{t_1} - \\underbrace{\\cancel{ \\lambda(t_0)\\frac{d\\mathbf{u}}{d\\theta}\\bigg\\vert_{t_0} }}_{=0} - \\int_{t_0}^{t_1}{\\frac{d\\lambda}{dt}\\frac{d\\mathbf{u}}{d\\theta}dt}\\\\ \\end{aligned} \\end{equation} $$\nThe second term cancelled out due to state \\(u(t=0)\\) doesn’t depend on \\(\\theta\\). Replacing equation (10) back into equation (9):\n$$ \\begin{equation} \\begin{aligned} \u0026 \\int_{t_0}^{t_1}{ \\frac{d}{d\\theta}\\lambda \\bigg( f(\\mathbf{u}, t;\\theta) - \\frac{d\\mathbf{u}}{dt} \\bigg) dt }\\\\ = \u0026 \\int_{t_0}^{t_1}{ \\lambda \\bigg( \\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta} + \\frac{d\\lambda}{dt}\\frac{d\\mathbf{u}}{d\\theta} \\bigg) dt } - \\lambda(t_1) \\frac{d\\mathbf{u}}{d\\theta}\\bigg\\vert_{t_1} \\end{aligned} \\end{equation} $$\nReplacing result from equation (8) and (11) into equation (7):\n$$ \\begin{equation} \\begin{aligned} \\frac{dJ}{d\\theta} \u0026= \\frac{\\partial L}{\\partial \\mathbf{\\hat{u}_1}} \\int_{t_0}^{t_1}{ \\bigg(\\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta}\\bigg)dt }\\\\ \u0026 + \\int_{t_0}^{t_1}{ \\lambda \\bigg( \\frac{\\partial f}{\\partial\\theta} + \\frac{\\partial f}{\\partial\\mathbf{u}}\\frac{d\\mathbf{u}}{d\\theta} + \\frac{d\\lambda}{dt}\\frac{d\\mathbf{u}}{d\\theta} \\bigg) dt }\\\\ \u0026 - \\lambda(t_1) \\frac{d\\mathbf{u}}{d\\theta}\\bigg\\vert_{t_1} \\end{aligned} \\end{equation} $$\nRearranging equation (12):\n$$ \\begin{equation} \\begin{aligned} \\frac{dJ}{d\\theta} \u0026= \\int_{t_0}^{t_1}{ \\big(\\frac{\\partial L}{\\partial\\mathbf{\\hat{u}}_1} + \\lambda\\big)\\frac{\\partial f}{\\partial\\theta}dt }\\\\ \u0026 + \\int_{t_0}^{t_1}{ \\big( \\frac{\\partial L}{\\partial\\mathbf{\\hat{u}}_1}\\frac{\\partial f}{\\partial\\mathbf{u}} + \\lambda \\frac{\\partial f}{\\partial\\mathbf{u}} + \\frac{d\\lambda}{dt} \\big)\\frac{d\\mathbf{u}}{d\\theta} }\\\\ \u0026 - \\lambda(t_1) \\frac{d\\mathbf{u}}{d\\theta}\\bigg\\vert_{t_1} \\end{aligned} \\end{equation} $$\nFrom the forward sensitivity method we know that \\(\\frac{d\\mathbf{u}}{d\\theta}\\) is prohibitively expensive, we can choose the Lagrangian \\(\\lambda\\) such that the last two terms in equation (13) vanish. Specifically:\n$$ \\begin{equation} \\begin{aligned} \u0026 \\begin{cases} \\frac{\\partial L}{\\partial\\mathbf{\\hat{u}}_1}\\frac{\\partial f}{\\partial\\mathbf{u}} + \\lambda \\frac{\\partial f}{\\partial\\mathbf{u}} + \\frac{d\\lambda}{dt} = 0 \\\\ \\lambda(t_1) = 0 \\end{cases} \\\\ \\iff \u0026 \\begin{cases} \\frac{d\\lambda}{dt} = -\\big(\\frac{\\partial L}{\\partial\\mathbf{\\hat{u}}_1} + \\lambda\\big) \\frac{\\partial f}{\\partial\\mathbf{u}}\\\\ \\lambda(t_1) = 0 \\end{cases} \\end{aligned} \\end{equation} $$\nDenoting \\(\\mathbf{a}(t)=\\lambda + \\frac{\\partial L}{\\partial\\mathbf{\\hat{u}_1}}\\), equation (14) became:\n$$ \\begin{equation} \\begin{aligned} \\begin{cases} \\frac{d\\mathbf{a}}{dt} = -\\mathbf{a}(t)\\frac{\\partial f}{\\partial \\mathbf{u}}\\\\ \\mathbf{a}(t_1) = \\frac{\\partial L}{\\partial\\mathbf{\\hat{u}}_1} \\end{cases} \\end{aligned} \\end{equation} $$\nEquation (15) is a ODE terminal value problem, which can be solved by any ODE solver. The sensitivity \\(\\frac{dJ}{d\\theta}\\) in equation (13) became:\n$$ \\begin{equation} \\frac{dJ}{d\\theta} = \\int_{t_0}^{t_1}{\\mathbf{a}(t)\\frac{\\partial f}{\\partial \\theta}dt} \\end{equation} $$\n\\(\\mathbf{a}(t)\\) is exactly the adjoint state that mentioned in the original paper. In the paper, the authors went with alternative proof using Taylor Expansion.\nSummary strategy of computing the sensitivity \\(\\frac{dJ}{d\\theta}\\): In forward pass, \\(\\mathbf{\\hat{u}}_1 = \\text{ODESolve}(f_\\theta, \\mathbf{u}_0, t_0, t_1)\\), where dynamic is specified by neural-network \\(f_\\theta\\) Solve ODE terminal value problem specified by equation (15) for adjoint state \\(\\mathbf{a}(t)\\) Compute sensitivity \\(\\frac{dJ}{d\\theta}\\) Implementation Git to my version of implementation References Patric Kridge’s thesis On Neural Differential Equation. Efficient gradient computation for dynamical models ","wordCount":"1155","inLanguage":"en","image":"http://localhost:1313/images/neural_ode_rs.png","datePublished":"2024-11-13T00:00:00Z","dateModified":"2024-11-13T00:00:00Z","author":{"@type":"Person","name":"Tu T. Do"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/adjoint_state_method/"},"publisher":{"@type":"Organization","name":"iamtu","logo":{"@type":"ImageObject","url":"http://localhost:1313/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="iamtu (Alt + H)">iamtu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/ title=home><span>home</span></a></li><li><a href=http://localhost:1313/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=http://localhost:1313/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li><li><a href=http://localhost:1313/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:1313/posts/>Posts</a></div><h1 class=post-title>Understanding NeuralODE</h1><div class=post-meta><span title='2024-11-13 00:00:00 +0000 UTC'>November 13, 2024</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1155 words&nbsp;·&nbsp;Tu T. Do</div></header><figure class=entry-cover><img loading=lazy src=http://localhost:1313/images/neural_ode_rs.png alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#motivation aria-label=Motivation>Motivation</a></li><li><a href=#neuralode aria-label=NeuralODE>NeuralODE</a><ul><li><a href=#formulation aria-label=Formulation>Formulation</a></li><li><a href=#the-forward-sensitivity-method aria-label="The forward sensitivity method">The forward sensitivity method</a></li><li><a href=#the-adjoint-state-method aria-label="The adjoint state method">The adjoint state method</a></li><li><a href=#summary-strategy-of-computing-the-sensitivity-fracdjdtheta aria-label="Summary strategy of computing the sensitivity \(\frac{dJ}{d\theta}\):">Summary strategy of computing the sensitivity \(\frac{dJ}{d\theta}\):</a></li></ul></li><li><a href=#implementation aria-label=Implementation>Implementation</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h2 id=motivation>Motivation<a hidden class=anchor aria-hidden=true href=#motivation>#</a></h2><p>This blog post is my note taken when studying <em>Neural Oridinary Differential Equation</em> (NeuralODE), which was proposed in <a href=https://arxiv.org/abs/1806.07366>Neural Ordinary Differential Equations</a>. The goal of this note is to understand the formulation, some mathematical derivations, and some techincal difficulties encounter when implementing NeuralODE in JAX.</p><p>The entire learning process is quite fascinating and introduced me to some new mathematical concepts such as Euler-Lagrange equation, Continuous Lagrangian Multiplier. NeuralODE is a important entry point to Physics Informed Machine Learning and I struggled for quite sometime to understand. So I hope this post might save sometime for people who are also trying to understand NeuralODE.</p><h2 id=neuralode>NeuralODE<a hidden class=anchor aria-hidden=true href=#neuralode>#</a></h2><h3 id=formulation>Formulation<a hidden class=anchor aria-hidden=true href=#formulation>#</a></h3><p>Many systems can be written as a ODE&rsquo;s initial value problem:</p><p>$$
\begin{equation}
\begin{cases}
\frac{d\mathbf{u}}{dt} = f(\mathbf{u}, t) \\
\mathbf{u}(t=0) = \mathbf{u}_0
\end{cases}
\end{equation}
$$</p><p>Where:</p><ul><li>\(\mathbf{u} \in \mathbb{R}^N\) describes the state of the system.</li><li>\(\mathbf{u}(t)\) is the state of the system at time \(t\).</li><li>\(f: \mathbb{R}^N \rightarrow \mathbb{R}^N\) is a dynamic function that characterizes the system. Which means, at any given state of the system, \(f\) tells us the rate of change for that state \(\frac{d\mathbf{u}}{dt}\).</li></ul><p>When \(f\) is given, this system of equations can be solved by numerical methods such as Euler, Predictor Corrector Method, or RK45 (see my previous post on ODE Integrator).</p><p>NeuralODE solves the inverse problem, given some observations of the system state \(\{(\mathbf{u}_i, t_i)\}_{i=1\cdots N}\) (which can be irregularly sampled), find the dynamic function \(f\). NeuralODE parameterize \(f\) by a neural-network \(f_\theta\), where \(\theta \in \mathbb{R}^P\) is the set of parameters of the network. For simplicity, assume that we have only two observations at \(t=t_0\) and \(t=t_1\). NeuralODE&rsquo;s prediction of the system state at \(t=t_1\) are given by:</p><p>$$
\begin{equation}
\hat{\mathbf{u}}_1 = \mathbf{u}_0 + \int_{t_0}^{t_1}{f(\mathbf{u}, t;\theta)dt}
\end{equation}
$$</p><p>NeuralODE learns the parameter \(\theta\) by minimizing the difference between its prediction and groundtruth \(\mathbf{u}_1\):</p><p>$$
L(\hat{\mathbf{u}}_1, \mathbf{u}_1) = \text{MSE}(\mathbf{u}_1, \mathbf{\hat{u}}_1)
$$</p><p>Finally, NeuralODE can be formulated as a constrained optimization problem:</p><p>$$
\begin{equation}
\begin{aligned}
& \min_\theta L(\hat{\mathbf{u}}_1, \mathbf{u}_1) \\
\text{such that: } &
\frac{d\mathbf{u}}{dt} = f(\mathbf{u}, t;\theta)\quad \forall t_0\leq t \leq t_1
\end{aligned}
\end{equation}
$$</p><p>In order to solve the optimization problem in equation 3), we need to compute the sensitivity \(\frac{dL}{d\theta}\). The following sections will discuss the method of computing this quantity.</p><h3 id=the-forward-sensitivity-method>The forward sensitivity method<a hidden class=anchor aria-hidden=true href=#the-forward-sensitivity-method>#</a></h3><p>The forward sensitivity method is a straight forward method of computing \(\frac{dL}{d\theta}\):</p><p>$$
\begin{equation}
\begin{aligned}
\frac{dL}{d\theta} & = \frac{d}{d\theta} L(\mathbf{u}_1, \mathbf{\hat{u}}_1) \\
& =
\underbrace{\frac{\partial L}{\partial \mathbf{\hat{u}}_1}}_{1\times N}
\underbrace{\frac{d\mathbf{\hat{u}_1}}{d\theta}}_{N\times P}
\end{aligned}
\end{equation}
$$</p><p>By differentiating both L.H.S and R.H.S of equation (1) with respect to \(\theta\), we have:</p><p>$$
\begin{equation}
\begin{aligned}
& \frac{d}{d\theta}(\frac{d\mathbf{u}}{dt}) = \frac{d}{d\theta}f(\mathbf{u}, t; \theta) \\
\iff & \frac{d}{dt}(\frac{d\mathbf{u}}{d\theta}) = \frac{\partial f}{\partial\theta} + \frac{\partial f}{\partial \mathbf{u}}\frac{d\mathbf{u}}{d\theta}
\end{aligned}
\end{equation}
$$</p><p>Equation (5) gaves us a system of ODE initial value problem, which consists of \(N\times P\) individual ODE. We can see that by denoting \(A = \frac{d\mathbf{u}}{d\theta} \in \mathbb{R}^N\) is the Jacobian of state \(u\) with respect to parameter \(\theta\), \(A_{ij} = \frac{\partial u_i}{\partial\theta_j}\). To solve for \(\frac{d\mathbf{\hat{u}_1}}{d\theta}\), we solve \(N\times P\) individual ODE initial value problems with initial value is \(0\) (since \(\mathbf{u}_0\) doesn&rsquo;t depend on \(\theta\)). The forward sensitivity method is computationally prohibited for the medium to large neural networks with thousands of parameters.</p><p>Equiped with this understanding, we can fully appreciate the adjoint sensitivity method, which is the key to understand NeuralODE.</p><h3 id=the-adjoint-state-method>The adjoint state method<a hidden class=anchor aria-hidden=true href=#the-adjoint-state-method>#</a></h3><p>We can write the constrainted optimization problem in equation (3) as unconstrainted one using continuous Lagrangian multiplier \(\lambda(t)\):</p><p>$$
\begin{equation}
\begin{aligned}
J(\mathbf{\hat{u}_1}, \lambda, \theta) = L(\mathbf{\hat{u}_1}) + \int_{t_0}^{t_1}{\lambda(t)(f - \frac{d\mathbf{u}}{dt} )dt}
\end{aligned}
\end{equation}
$$</p><p>Where \(\lambda \in \mathbb{R}^N\) is the Lagrangian multiplier in the form of function of time. Take the derivative with respect to \(\theta\) for L.H.S and R.H.S in equation (6):</p><p>$$
\begin{equation}
\begin{aligned}
\frac{dJ}{d\theta} & = \frac{d}{d\theta}\bigg(
L(\mathbf{\hat{u}_1}) + \int_{t_0}^{t_1}{\lambda(t)(f - \frac{d\mathbf{u}}{dt} )dt}
\bigg) \\
& = \frac{d}{d\theta} L(\mathbf{\hat{u}_1})
+ \int_{t_0}^{t_1}{
\frac{d}{d\theta} \lambda(f - \frac{d\mathbf{u}}{dt})
dt}
\end{aligned}
\end{equation}
$$</p><p>Considering the first term in R.H.S of equation (7):</p><p>$$
\begin{equation}
\begin{aligned}
\frac{d}{d\theta}L(\mathbf{\hat{u}}_1) &= \frac{\partial L}{\partial \mathbf{\hat{u}_1}} \frac{d}{d\theta}(
\mathbf{u}_0 + \int_{t_0}^{t_1}{f(\mathbf{u}, t; \theta)dt}) \\
& = \frac{\partial L}{\partial \mathbf{\hat{u}_1}} \int_{t_0}^{t_1}{
\bigg(\frac{\partial f}{\partial\theta} + \frac{\partial f}{\partial\mathbf{u}}\frac{d\mathbf{u}}{d\theta}\bigg)dt
}
\end{aligned}
\end{equation}
$$</p><p>And considering the second term of the R.H.S of equation (7):</p><p>$$
\begin{equation}
\begin{aligned}
& \int_{t_0}^{t_1}{
\frac{d}{d\theta}\lambda \bigg(
f(\mathbf{u}, t;\theta) - \frac{d\mathbf{u}}{dt}
\bigg) dt
}\\
= & \int_{t_0}^{t_1}{
\lambda \bigg(
\frac{\partial f}{\partial\theta}
+ \frac{\partial f}{\partial\mathbf{u}}\frac{d\mathbf{u}}{d\theta}
- \frac{d}{d\theta}(\frac{d\mathbf{u}}{dt})
\bigg) dt
}\\
= & \int_{t_0}^{t_1}{
\lambda \bigg(
\frac{\partial f}{\partial\theta}
+ \frac{\partial f}{\partial\mathbf{u}}\frac{d\mathbf{u}}{d\theta}
\bigg) dt
} -
\blue{
\int_{t_0}^{t_1}{
\lambda\frac{d}{d\theta}(\frac{d\mathbf{u}}{dt}) dt
}
}
\end{aligned}
\end{equation}
$$</p><p>The final term in equation (9) can be evaluated by integration by parts after swapping the order of derivation:</p><p>$$
\begin{equation}
\begin{aligned}
& \int_{t_0}^{t_1}{
\lambda\frac{d}{d\theta}(\frac{d\mathbf{u}}{dt}) dt
} \\
= & \int_{t_0}^{t_1}{
\lambda\frac{d}{dt}(\frac{d\mathbf{u}}{d\theta}) dt
} \\
= & \bigg[\lambda \frac{d\mathbf{u}}{d\theta}\bigg]_{t_0}^{t_1} - \int_{t_0}^{t_1}{\frac{d\lambda}{dt}\frac{d\mathbf{u}}{d\theta}dt}\\
= & \lambda(t_1) \frac{d\mathbf{u}}{d\theta}\bigg\vert_{t_1}
- \underbrace{\cancel{
\lambda(t_0)\frac{d\mathbf{u}}{d\theta}\bigg\vert_{t_0}
}}_{=0}
- \int_{t_0}^{t_1}{\frac{d\lambda}{dt}\frac{d\mathbf{u}}{d\theta}dt}\\
\end{aligned}
\end{equation}
$$</p><p>The second term cancelled out due to state \(u(t=0)\) doesn&rsquo;t depend on \(\theta\). Replacing equation (10) back into equation (9):</p><p>$$
\begin{equation}
\begin{aligned}
& \int_{t_0}^{t_1}{
\frac{d}{d\theta}\lambda \bigg(
f(\mathbf{u}, t;\theta) - \frac{d\mathbf{u}}{dt}
\bigg) dt
}\\
= & \int_{t_0}^{t_1}{
\lambda \bigg(
\frac{\partial f}{\partial\theta}
+ \frac{\partial f}{\partial\mathbf{u}}\frac{d\mathbf{u}}{d\theta}
+ \frac{d\lambda}{dt}\frac{d\mathbf{u}}{d\theta}
\bigg) dt
} - \lambda(t_1) \frac{d\mathbf{u}}{d\theta}\bigg\vert_{t_1}
\end{aligned}
\end{equation}
$$</p><p>Replacing result from equation (8) and (11) into equation (7):</p><p>$$
\begin{equation}
\begin{aligned}
\frac{dJ}{d\theta}
&= \frac{\partial L}{\partial \mathbf{\hat{u}_1}} \int_{t_0}^{t_1}{
\bigg(\frac{\partial f}{\partial\theta} + \frac{\partial f}{\partial\mathbf{u}}\frac{d\mathbf{u}}{d\theta}\bigg)dt
}\\
& + \int_{t_0}^{t_1}{
\lambda \bigg(
\frac{\partial f}{\partial\theta}
+ \frac{\partial f}{\partial\mathbf{u}}\frac{d\mathbf{u}}{d\theta}
+ \frac{d\lambda}{dt}\frac{d\mathbf{u}}{d\theta}
\bigg) dt
}\\
& - \lambda(t_1) \frac{d\mathbf{u}}{d\theta}\bigg\vert_{t_1}
\end{aligned}
\end{equation}
$$</p><p>Rearranging equation (12):</p><p>$$
\begin{equation}
\begin{aligned}
\frac{dJ}{d\theta}
&= \int_{t_0}^{t_1}{
\big(\frac{\partial L}{\partial\mathbf{\hat{u}}_1} + \lambda\big)\frac{\partial f}{\partial\theta}dt
}\\
& + \int_{t_0}^{t_1}{
\big(
\frac{\partial L}{\partial\mathbf{\hat{u}}_1}\frac{\partial f}{\partial\mathbf{u}}
+ \lambda \frac{\partial f}{\partial\mathbf{u}}
+ \frac{d\lambda}{dt}
\big)\frac{d\mathbf{u}}{d\theta}
}\\
& - \lambda(t_1) \frac{d\mathbf{u}}{d\theta}\bigg\vert_{t_1}
\end{aligned}
\end{equation}
$$</p><p>From the forward sensitivity method we know that \(\frac{d\mathbf{u}}{d\theta}\) is prohibitively expensive, we can choose the Lagrangian \(\lambda\) such that the last two terms in equation (13) vanish. Specifically:</p><p>$$
\begin{equation}
\begin{aligned}
& \begin{cases}
\frac{\partial L}{\partial\mathbf{\hat{u}}_1}\frac{\partial f}{\partial\mathbf{u}}
+ \lambda \frac{\partial f}{\partial\mathbf{u}}
+ \frac{d\lambda}{dt} = 0 \\
\lambda(t_1) = 0
\end{cases} \\
\iff & \begin{cases}
\frac{d\lambda}{dt} = -\big(\frac{\partial L}{\partial\mathbf{\hat{u}}_1}
+ \lambda\big) \frac{\partial f}{\partial\mathbf{u}}\\
\lambda(t_1) = 0
\end{cases}
\end{aligned}
\end{equation}
$$</p><p>Denoting \(\mathbf{a}(t)=\lambda + \frac{\partial L}{\partial\mathbf{\hat{u}_1}}\), equation (14) became:</p><p>$$
\begin{equation}
\begin{aligned}
\begin{cases}
\frac{d\mathbf{a}}{dt} = -\mathbf{a}(t)\frac{\partial f}{\partial \mathbf{u}}\\
\mathbf{a}(t_1) = \frac{\partial L}{\partial\mathbf{\hat{u}}_1}
\end{cases}
\end{aligned}
\end{equation}
$$</p><p>Equation (15) is a ODE terminal value problem, which can be solved by any ODE solver. The sensitivity \(\frac{dJ}{d\theta}\) in equation (13) became:</p><p>$$
\begin{equation}
\frac{dJ}{d\theta} = \int_{t_0}^{t_1}{\mathbf{a}(t)\frac{\partial f}{\partial \theta}dt}
\end{equation}
$$</p><p>\(\mathbf{a}(t)\) is exactly the adjoint state that mentioned in the original paper. In the paper, the authors went with alternative proof using Taylor Expansion.</p><h3 id=summary-strategy-of-computing-the-sensitivity-fracdjdtheta>Summary strategy of computing the sensitivity \(\frac{dJ}{d\theta}\):<a hidden class=anchor aria-hidden=true href=#summary-strategy-of-computing-the-sensitivity-fracdjdtheta>#</a></h3><ul><li>In forward pass, \(\mathbf{\hat{u}}_1 = \text{ODESolve}(f_\theta, \mathbf{u}_0, t_0, t_1)\), where dynamic is specified by neural-network \(f_\theta\)</li><li>Solve ODE terminal value problem specified by equation (15) for adjoint state \(\mathbf{a}(t)\)</li><li>Compute sensitivity \(\frac{dJ}{d\theta}\)</li></ul><h2 id=implementation>Implementation<a hidden class=anchor aria-hidden=true href=#implementation>#</a></h2><ul><li><a href=https://github.com/Young1906/jax_neural_ode>Git</a> to my version of implementation</li></ul><h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2><ul><li>Patric Kridge&rsquo;s thesis <a href=http://arxiv.org/abs/2202.02435>On Neural Differential Equation</a>.</li><li><a href=https://www.sciencedirect.com/science/article/pii/S1053811914003097>Efficient gradient computation for dynamical models</a></li></ul></div><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://iamtu-dev.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/learn/>Learn</a></li></ul><nav class=paginav><a class=next href=http://localhost:1313/posts/ode_solver/><span class=title>Next »</span><br><span>Numerical Integrations</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=http://localhost:1313/>iamtu</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>