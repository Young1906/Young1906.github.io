<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Pinn on iamtu</title><link>http://localhost:40633/tags/pinn/</link><description>Recent content in Pinn on iamtu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 24 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:40633/tags/pinn/index.xml" rel="self" type="application/rss+xml"/><item><title>My calculus of variations crash course</title><link>http://localhost:40633/posts/variational_calculus/</link><pubDate>Tue, 24 Sep 2024 00:00:00 +0000</pubDate><guid>http://localhost:40633/posts/variational_calculus/</guid><description>&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>My goal for this post is to have a basic understanding of Calculus of Variations, so that I can be more comfortable with mathematics in &lt;a href="https://arxiv.org/abs/1806.07366">NeuralODE paper&lt;/a>, where the problem can be formulated as a optimization of a functional with ODE constraint (&lt;a href="https://www.youtube.com/watch?v=k6s2G5MZv-I&amp;amp;t=556s">Adjoint State Method for an ODE&lt;/a>).&lt;/p>
&lt;p>My first encounter with Calculus of Variation is one of my homework where we try to derive probablity density function of some distribution by the principle of maximum entropy. This is my note of a more thorough investigation of the topic and it is heavily based on the content of &lt;a href="https://www.open.edu/openlearn/8b/91/8b919cebaa524d141308761405438be36126c07d?response-content-disposition=inline%3Bfilename%3D%22Introduction%20to%20the%20calculus%20of%20variations_ms327.pdf%22&amp;amp;response-content-type=application%2Fpdf&amp;amp;Expires=1727281020&amp;amp;Signature=Y5qPgt8T-FL6MrViTgisHOaDvOZrnyRTlog5CKdcx3O3IaYDepGQ43r2ZdUf2rX3tGPulZYzYbf1H1RhB6Xv2V80TZtQThFlBZLMgHWWNDR3AZKxqL8FD3Xq2vlRwft~grD5fGpioT6G57~zDUKl0TQ7bR3PDv4DzUakrkhDg-NWehqo4dIk7v5PnyFEeki7qsvbz6Dtkqo9MwEn8Z-CDWg2IUwZ5ocNFSts7Hj8ABQpljFITa8C36XvZBgy8lcmPCAhAr4gFi26Y5PXnytW7~3FCqnlLJd6pA7yK-t9A5UMW7dooi6el4bvbuMVrbrZ~sLTNXeM9SXOryke07jTBw__&amp;amp;Key-Pair-Id=K87HJKWMK329B">this tutorial&lt;/a>.&lt;/p></description></item><item><title>Learning to solve heat equation</title><link>http://localhost:40633/posts/heat/</link><pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate><guid>http://localhost:40633/posts/heat/</guid><description>&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>Surveying numerical methods (finite difference methods) and physics-informed neural networks to solve a 1D heat equation. This post was heavily inspired by:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>(Book) &lt;a href="https://www.amazon.com/Differential-Equations-Scientists-Engineers-Mathematics/dp/048667620X">Partial Differential Equations for Scientists and Engineers - Standley J. Farlow&lt;/a>&lt;/strong> for deriving closed-form solution.&lt;/li>
&lt;li>&lt;strong>(Article) &lt;a href="http://dma.dima.uniroma1.it/users/lsa_adn/MATERIALE/FDheat.pdf">Finite-Difference Approximations to the Heat Equation&lt;/a>&lt;/strong>&lt;/li>
&lt;li>&lt;strong>(Course) &lt;a href="https://www.youtube.com/watch?v=IDIv92Z6Qvc&amp;amp;list=PLJkYEExhe7rYY5HjpIJbgo-tDZ3bIAqAm&amp;amp;index=5">ETH Zurich | Deep Learning for Scientific Computing 2023&lt;/a>&lt;/strong> for Theory and Implementation of Physics-Informed Neural Network.&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>&lt;em>Physics-Informed Machine Learning&lt;/em> (PIML) is an exciting subfield of Machine Learning that aims to incorporate physical laws and/or constraints into statistical machine learning. The representations of the laws and constraints can be categorized into three groups (with decreasing strength of inductive bias):&lt;/p></description></item><item><title>Numerical methods for PDE</title><link>http://localhost:40633/posts/pde/</link><pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate><guid>http://localhost:40633/posts/pde/</guid><description>&lt;p>Outline&lt;/p>
&lt;ul>
&lt;li>ODE with initial value problem&lt;/li>
&lt;li>Reduction of orders&lt;/li>
&lt;li>Euler method&lt;/li>
&lt;/ul></description></item><item><title>Learning physics-informed Neural Networks (PINN)</title><link>http://localhost:40633/posts/pinn/</link><pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate><guid>http://localhost:40633/posts/pinn/</guid><description>&lt;p>My goal is to familiarize myself with physics-informed neural network(PINN), consist of &amp;hellip; Having no formal training on solving PDE, the first section will outline basic concepts in solving partial differential equations (PDEs), an example of a PDE, and its closed-form solution.&lt;/p>
&lt;p>Outline:&lt;/p>
&lt;ul>
&lt;li>PDE&lt;/li>
&lt;li>Closed-formed solution&lt;/li>
&lt;li>Generating data&lt;/li>
&lt;li>PINN&lt;/li>
&lt;/ul>
&lt;h2 id="pde-and-the-legendre-differential-equation">PDE and the Legendre Differential Equation&lt;/h2>
&lt;p>This section is written with instruction from &lt;a href="https://web.math.ucsb.edu/~moore/pde.pdf">this introduction&lt;/a> by JD Moore.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>&lt;strong>Definition&lt;/strong> (Power series)&lt;/p></description></item></channel></rss>