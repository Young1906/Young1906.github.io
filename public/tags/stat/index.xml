<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Stat on iamtu</title><link>http://localhost:53049/tags/stat/</link><description>Recent content in Stat on iamtu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 12 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="http://localhost:53049/tags/stat/index.xml" rel="self" type="application/rss+xml"/><item><title>Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution</title><link>http://localhost:53049/posts/closed-form-kl-gaussian/</link><pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate><guid>http://localhost:53049/posts/closed-form-kl-gaussian/</guid><description>The closed form of KL divergence used in Variational Auto Encoder.
Univariate case Let
\(p(x) = \mathcal{N}(\mu_1, \sigma_1) = (2\pi\sigma_1^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_1^2}(x-\mu_1)^2]\) \(q(x) = \mathcal{N}(\mu_1, \sigma_2) = (2\pi\sigma_2^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_2^2}(x-\mu_2)^2]\) KL divergence between \(p\) and \(q\) is defined as:
$$ \begin{aligned} \text{KL}(p\parallel q) &amp;amp;= -\int_{x}{p(x)\log{\frac{q(x)}{p(x)}}dx} \\ &amp;amp;= -\int_x p(x) [\log{q(x)} - \log{p(x)}]dx \\ &amp;amp;= \underbrace{ \int_x{p(x)\log p(x) dx}}_A - \underbrace{ \int_x{p(x)\log q(x) dx}}_B \end{aligned} $$
First quantity \(A\):
$$ \begin{aligned} A &amp;amp;= \int_x{p(x)\log p(x) dx} \\ &amp;amp;= \int_x{p(x)\big[ -\frac{1}{2}\log{2\pi\sigma_1^2 - \frac{1}{2\sigma_1^2}(x - \mu_1)^2} \big]dx}\\ &amp;amp;= -\frac{1}{2}\log{2\pi\sigma_1^2}\int_x{p(x)dx} - \frac{1}{2\sigma_1^2} \underbrace{\int_x{p(x)(x-\mu_1)^2dx}}_{\text{var(x)}}\\ &amp;amp;= -\frac{1}{2}\log{2\pi} - \log\sigma_1-\frac{1}{2} \end{aligned} $$</description></item></channel></rss>