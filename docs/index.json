[{"content":"Motivating example Evaluating following integral\n$$ I = \\int_0^1{\\frac{1 - x^2}{\\ln{x}}dx} $$\nClosed-form results $$ \\begin{equation} \\begin{aligned} F(t) \u0026amp;= \\int_0^1{\\frac{1-x^t}{\\ln(x)}dx} \\\\ \\implies \\frac{d}{dt}F \u0026amp;= \\frac{d}{dt}\\int_0^1{\\frac{1-x^t}{\\ln(x)}dx}\\\\ \u0026amp;= \\int_0^1{ \\frac{\\partial}{\\partial t} \\frac{1-x^t}{\\ln(x)}dx }\\\\ \u0026amp;= \\int_0^1{ \\frac{-\\ln(x)x^t}{ln(x)} dx} \\\\ \u0026amp;= \\bigg[-\\frac{x^{t+1}}{t+1}\\bigg]_0^1\\\\ \u0026amp;= -\\frac{1}{t+1}\\\\ \\implies F(t) \u0026amp;= -\\ln({t+1}) \\\\ \\implies I \u0026amp;= f(2) = -\\ln3 \\end{aligned} \\end{equation} $$\nNumerical approximation Code to produce the figure import numpy as np from matplotlib import pyplot as plt def I(): g = lambda x: (1 - x**2)/np.log(x) vG = np.vectorize(g) x = np.random.uniform(0, 1, 10000) return vG(x).mean() ","permalink":"https://iamtu.dev/posts/diff-under-integral-sign/","summary":"Motivating example Evaluating following integral\n$$ I = \\int_0^1{\\frac{1 - x^2}{\\ln{x}}dx} $$\nClosed-form results $$ \\begin{equation} \\begin{aligned} F(t) \u0026amp;= \\int_0^1{\\frac{1-x^t}{\\ln(x)}dx} \\\\ \\implies \\frac{d}{dt}F \u0026amp;= \\frac{d}{dt}\\int_0^1{\\frac{1-x^t}{\\ln(x)}dx}\\\\ \u0026amp;= \\int_0^1{ \\frac{\\partial}{\\partial t} \\frac{1-x^t}{\\ln(x)}dx }\\\\ \u0026amp;= \\int_0^1{ \\frac{-\\ln(x)x^t}{ln(x)} dx} \\\\ \u0026amp;= \\bigg[-\\frac{x^{t+1}}{t+1}\\bigg]_0^1\\\\ \u0026amp;= -\\frac{1}{t+1}\\\\ \\implies F(t) \u0026amp;= -\\ln({t+1}) \\\\ \\implies I \u0026amp;= f(2) = -\\ln3 \\end{aligned} \\end{equation} $$\nNumerical approximation Code to produce the figure import numpy as np from matplotlib import pyplot as plt def I(): g = lambda x: (1 - x**2)/np.","title":"Differentiation under integral sign"},{"content":"The closed form of KL divergence used in Variational Auto Encoder.\nUnivariate case Let\n\\(p(x) = \\mathcal{N}(\\mu_1, \\sigma_1) = (2\\pi\\sigma_1^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2]\\) \\(q(x) = \\mathcal{N}(\\mu_1, \\sigma_2) = (2\\pi\\sigma_2^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2]\\) KL divergence between \\(p\\) and \\(q\\) is defined as:\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= -\\int_{x}{p(x)\\log{\\frac{q(x)}{p(x)}}dx} \\\\ \u0026amp;= -\\int_x p(x) [\\log{q(x)} - \\log{p(x)}]dx \\\\ \u0026amp;= \\underbrace{ \\int_x{p(x)\\log p(x) dx}}_A - \\underbrace{ \\int_x{p(x)\\log q(x) dx}}_B \\end{aligned} $$\nFirst quantity \\(A\\):\n$$ \\begin{aligned} A \u0026amp;= \\int_x{p(x)\\log p(x) dx} \\\\ \u0026amp;= \\int_x{p(x)\\big[ -\\frac{1}{2}\\log{2\\pi\\sigma_1^2 - \\frac{1}{2\\sigma_1^2}(x - \\mu_1)^2} \\big]dx}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi\\sigma_1^2}\\int_x{p(x)dx} - \\frac{1}{2\\sigma_1^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)^2dx}}_{\\text{var(x)}}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi} - \\log\\sigma_1-\\frac{1}{2} \\end{aligned} $$\nThe second quantity \\(B\\):\n$$ \\begin{aligned} B =\u0026amp; \\int_x{p(x)\\big[ -\\frac{1}{2}\\log2\\pi\\sigma_2^2 - \\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2 \\big]dx}\\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi\\sigma_2^2 - \\frac{1}{2\\sigma_2^2}\\int_x{ p(x)\\big[ (x - \\mu_1)^2 + 2(x-\\mu_1)(\\mu_1 - \\mu_2) + (\\mu_1 -\\mu_2)^2 \\big]dx} \\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi\\sigma_2^2 \\\\ \u0026amp; - \\frac{1}{2\\sigma_2^2}\\underbrace{\\int_x{p(x)(x-\\mu_1)^2}}_{\\text{var}(x)}\\\\ \u0026amp; - \\frac{2(\\mu_1 -\\mu_2)}{2\\sigma_2^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)dx}}_0 \\\\ \u0026amp; - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} \\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi -\\log\\sigma_2 - \\frac{\\sigma_1^2}{2\\sigma_2^2} - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} \\end{aligned} $$\nFinally, we obtained the KL divergence for univariate case\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= A - B \\\\ \u0026amp;= (-\\frac{1}{2}\\log2\\pi - \\log\\sigma_1 - \\frac{1}{2}) - ( -\\frac{1}{2}\\log2\\pi -\\log\\sigma_2 - \\frac{\\sigma_1^2}{2\\sigma_2^2} - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}) \\\\ \u0026amp;= \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2}{2\\sigma_2^2} + \\frac{(\\mu_1 -\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2} \\end{aligned} $$\nMultivariate case tbd\nReference https://gregorygundersen.com/blog/ ","permalink":"https://iamtu.dev/posts/closed-form-kl-gaussian/","summary":"The closed form of KL divergence used in Variational Auto Encoder.\nUnivariate case Let\n\\(p(x) = \\mathcal{N}(\\mu_1, \\sigma_1) = (2\\pi\\sigma_1^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2]\\) \\(q(x) = \\mathcal{N}(\\mu_1, \\sigma_2) = (2\\pi\\sigma_2^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2]\\) KL divergence between \\(p\\) and \\(q\\) is defined as:\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= -\\int_{x}{p(x)\\log{\\frac{q(x)}{p(x)}}dx} \\\\ \u0026amp;= -\\int_x p(x) [\\log{q(x)} - \\log{p(x)}]dx \\\\ \u0026amp;= \\underbrace{ \\int_x{p(x)\\log p(x) dx}}_A - \\underbrace{ \\int_x{p(x)\\log q(x) dx}}_B \\end{aligned} $$\nFirst quantity \\(A\\):\n$$ \\begin{aligned} A \u0026amp;= \\int_x{p(x)\\log p(x) dx} \\\\ \u0026amp;= \\int_x{p(x)\\big[ -\\frac{1}{2}\\log{2\\pi\\sigma_1^2 - \\frac{1}{2\\sigma_1^2}(x - \\mu_1)^2} \\big]dx}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi\\sigma_1^2}\\int_x{p(x)dx} - \\frac{1}{2\\sigma_1^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)^2dx}}_{\\text{var(x)}}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi} - \\log\\sigma_1-\\frac{1}{2} \\end{aligned} $$","title":"Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution"},{"content":"Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper. In the analogy above, the black box represents the simulator, and the configurations are the simulatorâ€™s parameters.\nThe applicability of SBI has great potential since we can almost reduce any process with defined input and output to a black-box machine 1.\nThis post documents my notes while studying Likelihood-free MCMC with Amortized Ratio Estimator (Hermans et al, 2020); a method developed to address SBI.\nLikelihood-free MCMC with Amortized Ratio Estimator Likelihood ratio is defined as the ratio between the likelihood of the observation between two different hypothesis:\n$$ r(\\mathbf{x} | \\theta_0, \\theta_1) = \\frac{p(\\mathbf{x} | \\theta_0)}{p(\\mathbf{x}|\\theta_1)} $$\nThis quantity then can be used in various methods to draw sample from a distribution. In the paper, the author mention three sampling methods, namely Markov Chain Monte Carlo, Metropolis-Hasting, and HMC. In the following section, I am briefly summarizing those methods.\nBackground Markov Chain Monte Carlo (MCMC) In statistics, the MCMC method is a class of algorithms for sampling from a probability distribution. By constructing a Markov chain with the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the state chain 2.\nAdapting MCMC for SBI task We want to sample from \\(p(\\theta | \\mathbf{x})\\) using MCMC, we need this quantity\n$$ \\begin{equation} \\begin{aligned} \\frac{p(\\theta | \\mathbf{x} )}{p(\\theta_t| \\mathbf{x})} = \\frac{ p(\\theta)p(\\mathbf{x} | \\theta)/p(\\mathbf{x}) }{ p(\\theta_t)p(\\mathbf{x} | \\theta_t)/p(\\mathbf{x}) } = \\frac{p(\\theta)}{p(\\theta_t)}\\times \\frac{p(\\mathbf{x} | \\theta)}{p(\\mathbf{x} | \\theta_t)} = \\frac{p(\\theta)}{p(\\theta_t)} \\times r(\\mathbf{x} | \\theta, \\theta_t) \\end{aligned} \\end{equation} $$\nWe can compute the first term of the equation since we have access to prior \\(p(\\theta)\\). But we can not compute the second term because we do not have access to the likelihood function \\(p(\\mathbf{x} | \\theta)\\). However, we can reframe the problem in the supervised-learning paradigm, so we can use a parameterized discriminator \\(d_\\theta(\\mathbf{x})\\) to estimate the likelihood. The details are described in the Likelihood Ratio Estimator section.\nMetropolis-Hasting (MH) tbd\nHalmitonian Markov Chain(MH) tbd\nLikelihood Ratio Estimator The remaining question is how to estimate the likelihood ratio \\( r(\\mathbf{x} | \\theta_0, \\theta_1)\\). To estimate the ratio, the author employed the Likelihood Ratio Trick, training a discriminator \\(d_\\phi(\\mathbf{x})\\) to classify samples \\( x \\sim p(\\mathbf{x} | \\theta_0)\\) with class label \\(y = 1\\) from \\(\\mathbf{x} \\sim p(\\mathbf{x} | \\theta_1)\\) with class label \\(y = 0\\). The decision function obtained by the trained discrimininator:\n$$ d^*(\\mathbf{x}) = p(y = 1 | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\theta_0)}{p(\\mathbf{x} | \\theta_0) + p(\\mathbf{x} | \\theta_1)} $$\nThen the estimation of likelihood ratio can be computed by:\n$$ \\hat{r}(\\mathbf{x} | \\theta_0, \\theta_1) = \\frac {d^{*}(\\mathbf{x})} {1 - d^{*}(\\mathbf{x})} $$\nHowever, this method required the discriminator to be trained at every pair of \\((\\theta_0, \\theta_1)\\), which is impractical in the context. To overcome this issue, the paper proposed to train the discriminator to classify dependent sample-parameter pairs \\((\\mathbf{x}, \\mathbf{\\theta}) \\sim p(\\mathbf{x}, \\mathbf{\\theta})\\) with label \\(y=1\\) from the independent sample-parameter pairs \\((\\mathbf{x}, \\mathbf{\\theta}) \\sim p(\\mathbf{x})p(\\mathbf{\\theta})\\) with label \\(y=0\\).\n$$ \\begin{equation} \\begin{aligned} d^*(\\mathbf{x}, \\mathbf{\\theta}) \u0026amp;= \\frac {p(\\mathbf{x}, \\mathbf{\\theta})} { p(\\mathbf{x}, \\mathbf{\\theta}) + p(\\mathbf{x}) p(\\mathbf{\\theta}) } \\ \\end{aligned} \\end{equation} $$\nThe likelihood-to-evidence ratio is computed by\n$$ r(\\mathbf{x} | \\theta) = \\frac {p(\\mathbf{x} | \\theta)} {p(x)} = \\frac{p(x, \\theta)}{p(x)p(\\theta)} = \\frac {d^{*}(x, \\theta)} {1 - d^{*}(x, \\theta)} $$\nThen the likelihood ratio for any two hypotheses can be estimated at any point by\n$$ r(x | \\theta_0, \\theta_1) = \\frac{d^{*}(x,\\theta_0)}{d^{*}(x, \\theta_1)} $$\nToy example Setup:\nThe simulator: a function takes 1 parameter \\(\\mu\\), and return a random variable drawn from \\(\\mathcal{N}(\\mu, 1)\\) The observations \\(\\mathbf{x}\\): Observation drawn from the simulator with \\(\\mu = 2.5\\), which is unknown to the algorithm. The discriminator: A fully connected neural network. The prior of the parameters: \\(\\mathcal{N}(0, 1)\\) We want to draw samples from the posterior distribution \\(p(\\theta | \\mathbf{x})\\), where \\(x \\sim \\mathcal{N}(2.5, 1)\\).\nImplementation import click import numpy as np import torch from matplotlib import pyplot as plt from torch import nn from torch.nn import functional as F from typing import NamedTuple np.random.seed(1) torch.manual_seed(1) def stochastic(func): def __wrapper__(*args, **kwargs): np.random.seed() rs = func(*args, **kwargs) np.random.seed(1) return rs return __wrapper__ class Layer(NamedTuple): h: int # hidden dim a: str # activation def Dense(h_i: int, h_j: int, a : str): if a == \u0026#34;tanh\u0026#34;: act = nn.Tanh() elif a == \u0026#34;sigmoid\u0026#34;: act = nn.Sigmoid() elif a == \u0026#34;relu\u0026#34;: act = nn.ReLU() else: raise NotImplementedError(a) return nn.Sequential( nn.Linear(h_i, h_j), act) def build_mlp(input_dim: int, seq: list[Layer]) -\u0026gt; nn.Module: h0, a0 = seq[0] _seq = [Dense(input_dim, h0, a0)] for j in range(1, len(seq)): h_j, a_j = seq[j] h_i, _ = seq[j - 1] _seq.append(Dense(h_i, h_j, a_j)) return nn.Sequential(*_seq) def train_step( Xpos: torch.Tensor, Xneg: torch.Tensor, d: nn.Module, opt: torch.optim.Optimizer) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Args: - Xpos: (x, theta) - Xneg: (x, theta\u0026#39;) - d: classifier Where theta/theta\u0026#39; ~ p, x ~ p(x | theta) \u0026#34;\u0026#34;\u0026#34; for i in range(32): opt.zero_grad() zpos = d(Xpos) zneg = d(Xneg) loss = F.binary_cross_entropy(zpos, torch.ones_like(zpos))\\ + F.binary_cross_entropy(zneg, torch.zeros_like(zneg)) loss.backward() opt.step() return loss.item() def train_d( p: callable, sim: callable, d: nn.Module, m: int, e: int, lr: float): \u0026#34;\u0026#34;\u0026#34; Args: - p : prior - sim: simulator (implicit p(x | theta) - d: parameterized classifier - m: batch_size - e: max epochs - lr: learning rate \u0026#34;\u0026#34;\u0026#34; opt = torch.optim.Adam(d.parameters(), lr=lr) sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt) losses = [] for b in range(e): theta = p(m) theta_prime = p(m) x = sim(theta) # expand dims everything theta = np.expand_dims(theta, -1) theta_prime = np.expand_dims(theta_prime, -1) x = np.expand_dims(x, -1) # construct training sample Xpos = np.concatenate([x, theta], -1) Xneg = np.concatenate([x, theta_prime], -1) Xpos, Xneg = torch.tensor(Xpos, dtype=torch.float),\\ torch.tensor(Xneg, dtype=torch.float) loss = train_step(Xpos, Xneg, d, opt) losses.append(loss) if b%50 == 49: sch.step(loss) return d, losses @stochastic def mcmc(lp: callable, obs: np.ndarray, d: nn.Module, n_samples: int, step_size: float): \u0026#34;\u0026#34;\u0026#34; Amortized MCMC likelihood free \u0026#34;\u0026#34;\u0026#34; # proposal distribution: q = lambda theta: np.random.normal(theta, step_size) # initialize theta theta = 0. samples = [] obs = np.expand_dims(obs, -1) for i in range(n_samples): theta_prime = q(theta) mu_theta = np.ones_like(obs) * theta mu_theta_prime = np.ones_like(obs) * theta_prime # construct input vector X = np.concatenate([obs, mu_theta], -1) Xp = np.concatenate([obs, mu_theta_prime], -1) X, Xp= torch.tensor(X, dtype=torch.float),\\ torch.tensor(Xp, dtype=torch.float) # Compute the decision function d_theta = d(X).detach().mean().numpy() d_theta_prime = d(Xp).detach().mean().numpy() r_theta = d_theta / (1 - d_theta) r_theta_prime = d_theta_prime / (1- d_theta_prime) H = r_theta_prime / r_theta H = lp(theta_prime) / lp(theta) * H H = 1 if H \u0026gt; 1 else H u = np.random.uniform() if u \u0026lt; H: # accept theta_prime samples.append(theta_prime) theta = theta_prime return samples def main( batch_size: int, max_iter: int, lr: float, n_obs: int, n_samples: int, step_size: float): # PROBLEM SETUP # -------------------------------------------------- # prior theta p = lambda m: np.random.normal(0, 1, size=m) lp = lambda x: np.exp(-0.5 * x**2)#likelihood function # simulator: unknown sim = lambda mu: np.random.normal(mu, np.ones_like(mu) * .25) # parmeterized classifier d = build_mlp( 2, [Layer(4, \u0026#39;relu\u0026#39;), Layer(2, \u0026#39;relu\u0026#39;), Layer(1, \u0026#39;sigmoid\u0026#39;)]) # TRAINING the classifier # -------------------------------------------------- d, losses = train_d(p, sim, d, m=batch_size, e=max_iter, lr=lr) # inference # -------------------------------------------------- MU = 2.5 #unknown obs = sim(np.ones(n_obs) * MU) # Posterior sample: sample p(theta | obs) samples = mcmc(lp, obs, d, n_samples, step_size) Result References The frontier of simulation-based inference Likelihood-free MCMC with Amortized Ratio Estimator I am a black-box machine, you are a black-box machine, everyone is a black-box machine as long as we don\u0026rsquo;t care enough about the person.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShamelessly copied from Wikipedia.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://iamtu.dev/posts/sbi/","summary":"Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper.","title":"Likelihood-free MCMC with Amortized Ratio Estimator"},{"content":"TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \\(T\\) observations \\((x_1, \u0026hellip; x_T)\\) drawn from a true distribution \\(p_d(.)\\). We then try to approximate \\(p_d\\) by a parameterized function \\(p_m(.;\\theta)\\). The estimator \\(\\hat{\\theta}_T\\) is defined to be the \\(\\theta\\) that maximize function\n$$ J_T(\\theta) = \\frac{1}{2T}\\sum_t{\\log[h(x_t; 0)]} + \\log[1-h(y_t; \\theta)] $$\nIn which:\n\\(y=(y_1, \u0026hellip;, y_T)\\) be a dataset of \\(T\\) observations draw from a noise density function $p_n(.)$. \\(h(u; \\theta) = 1/(1 + e^{-G(u;\\theta)})\\) \\(G(u; \\theta) = \\log p_m(u; \\theta) - \\log p_n(u)\\) For \\(p_m(.; \\theta)\\) to be a valid p.d.f, we also need to include unit integral constraint into the optimization problem, that is \\(\\int_x{p_m(x; \\theta)dx} = 1\\). However, this integral is often intractable in most cases, for example, when we use a neural network to parameterize \\(p_m(.;\\theta)\\).\nInterestingly, the paper claims that maximizing the objective function gives a valid p.d.f without placing the unit integral constraint on the optimization (Theorem 1). In this post, I\u0026rsquo;ll attempt to prove the theorem as an exercise. Note that, I made an assumption that support of $x$ and $y$ are equal (in eq.7); which mean $p_n(.)$ is nonzero whenever $p_d(.)$ is nonzero and $p_n(.)$ is zero everywhere else.\nProof of threorem I When the sample size $T$ becomes arbitrarily large, the objective function $J_T(\\theta)$ converges in probability (this is a new word for me) to $\\tilde{J}$\n\\begin{equation} \\begin{aligned} \\tilde{J}(\\theta) = \\frac{1}{2}\\mathbb{E}_{x, y} { \\log{r\\big(f(x) - \\log{p_n(x)}\\big)} + \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} } \\end{aligned} \\end{equation}\nIn which \\(f(x) = \\log p_m(x; \\theta)\\) is the function approximating log-likelihood the true distribution $p_d(.)$.\nNotation \\(p_d(x)\\) true probability density function (p.d.f) of data. \\(p_n(x)\\) p.d.f of noise generating distribution. \\(r(x) = \\frac{1}{1+\\exp(-x)}\\) sigmoid function. \\(X = (x_1, \u0026hellip; x_T); x \\sim p_d(x)\\) be the dataset of T observations. \\(Y = (y_1, \u0026hellip; y_T); y \\sim p_n(y)\\) be the dataset of T artificially generated noise. \\(p_m(.; \\theta)\\) is estimation of \\(p_d(.)\\) parameterized by \\(\\theta\\). Theorem \\(\\tilde{J}\\) attains a maximum at \\(f(.) = \\log p_d(.)\\). There are no other extrema if the noise density \\(p_n(.)\\) is chosen such it is nonzero whenever \\(p_d(.)\\) is nonzero.\nProof Let \\(\\hat{f}(x)\\) be the optimal function that maximizes \\(\\tilde{J}\\), and \\(f(x)=\\hat{f}(x) + \\epsilon\\eta(x)\\).\n\\begin{equation} \\begin{aligned} \\tilde{J}(\\theta) \u0026amp;= K(\\epsilon) \\\\ \u0026amp;= \\frac{1}{2}\\mathbb{E}_{x, y} { \\log{ r\\big(f(x) - \\log{p_n(x)}\\big) } + \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} } \\\\ \u0026amp;= \\frac{1}{2}\\underbrace{ \\mathbb{E}_x { \\log r\\big( f(x) - \\log p_n(x) \\big) } }_A + \\frac{1}{2} \\underbrace{\\mathbb{E}_y { \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} }}_B \\\\ \\implies \\frac{dK}{d\\epsilon} \u0026amp;= \\frac{dA}{d\\epsilon} + \\frac{dB}{d\\epsilon} \\end{aligned} \\end{equation}\nExpand the first term of $K(\\epsilon)$\n\\begin{equation} \\begin{aligned} A(\\epsilon) \u0026amp;= \\mathbb{E}_x { \\log r\\big( f(x) - \\log p_n(x) \\big) } \\\\ \u0026amp; = \\int_x { p_d(x) \\log{ r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) \\big) } dx } \\end{aligned} \\end{equation}\nTaking derivative of $A(\\epsilon)$\n\\begin{equation} \\begin{aligned} \\frac{dA}{d\\epsilon} \u0026amp;= \\frac{1}{d\\epsilon} \\int_x { p_d(x) \\log{ r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) \\big) } dx } \\\\ \u0026amp; = \\int_x { p_d(x) \\big[ \\frac{1}{d\\epsilon}\\log{ r \\big( \\underbrace{ \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) }_{g(\\epsilon)} \\big) } \\big]dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\frac{d\\log{r}}{dr} \\frac{dr}{dg} \\frac{dg}{d\\epsilon} dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\frac{1}{r} r(1-r) \\eta(x) dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\big[1 - r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x)\\big) \\big] \\eta(x) dx } \\end{aligned} \\end{equation}\nNow let\u0026rsquo;s turn our attention to the second term of $K(\\epsilon)$\n\\begin{equation} \\begin{aligned} B(\\epsilon) \u0026amp;= \\mathbb{E}_y { \\log\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big] } \\\\ \u0026amp; = \\int_y { p_n(y) \\log \\big[ 1 - r \\big( \\underbrace{ \\hat{f}(y) + \\epsilon \\eta(y) - \\log p_n(y) }_h \\big) \\big]dy } \\end{aligned} \\end{equation}\nTaking derivative of $B$ w.r.t $\\epsilon$\n\\begin{equation} \\begin{aligned} \\frac{dB}{d\\epsilon} \u0026amp;= \\frac{1}{d\\epsilon} \\int_y{ p_n(y)\\log{ \\big[ 1 - r\\big( h(\\epsilon)\\big) \\big] }dy } \\\\ \u0026amp;= \\int_y { p_n(y) \\frac{d\\log(1-r)}{d(1-r)} \\frac{d(1-r)}{dr} \\frac{dr}{dh} \\frac{dh}{d\\epsilon} dy } \\\\ \u0026amp; = \\int_y { p_n(y) \\frac{1}{1-r} (-1) r(1-r) \\eta(y) } \\\\ \u0026amp; = -\\int_y{ p_n(y) r\\big( \\hat{f}(y) + \\epsilon \\eta(y) - \\log p_n(y) \\big) \\eta(y) dy } \\end{aligned} \\end{equation}\nSubstitute result from eq(4) and eq(6) to eq(2), $\\frac{dK}{d\\epsilon}$ is evaluated to $0$ at $\\epsilon = 0$.\n\\begin{equation} \\begin{aligned} \\frac{dK}{d\\epsilon}\\big\\vert_{\\epsilon=0} \u0026amp;= \\frac{dA}{d\\epsilon}\\big\\vert_{\\epsilon=0} + \\frac{dB}{d\\epsilon}\\big\\vert_{\\epsilon=0} \\\\ \u0026amp;= \\int_x { p_d(x) \\big[1 - r\\big( \\hat{f}(x) - \\log p_n(x)\\big) \\big] \\eta(x) dx } \\\\ \u0026amp; - \\int_y{ p_n(y) r \\big( \\hat{f}(y) - \\log p_n(y) \\big) \\eta(y) dy } \\\\ \u0026amp; = 0 \\end{aligned} \\end{equation}\nConsider eq. (7), if the support for $x$ and $y$ are equal, which mean we integrate $x$ and $y$ over a same region, we can change $y$ to $x$ and rewrite eq.(7) as\n\\begin{equation} \\begin{aligned} \\frac{dK}{d\\epsilon} \\big\\vert_{\\epsilon = 0} \u0026amp;= \\int_x { \\underbrace{ p_d(x) \\big[1 - r\\big( \\hat{f}(x) - \\log p_n(x)\\big) \\big] }_C \\eta(x) dx } \\\\ \u0026amp; - \\int_x{ \\underbrace{ p_n(x) r \\big( \\hat{f}(x) - \\log p_n(x) \\big) }_D \\eta(x) dx } \\\\ \u0026amp; = \\int_x{(C-D)\\eta(x)dx} = 0 \\quad \\forall \\eta(x) \\end{aligned} \\end{equation}\nThe equality in eq.(8) happend if and only if \\(C=D\\). This result easily leads to \\(\\hat{f}(x) = \\log p_d(x)\\).\nReferences Noise-contrastive estimation: A new estimation principle for unnormalized statistical models ","permalink":"https://iamtu.dev/posts/noise-contrastive-estimation/","summary":"TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \\(T\\) observations \\((x_1, \u0026hellip; x_T)\\) drawn from a true distribution \\(p_d(.)\\). We then try to approximate \\(p_d\\) by a parameterized function \\(p_m(.;\\theta)\\). The estimator \\(\\hat{\\theta}_T\\) is defined to be the \\(\\theta\\) that maximize function\n$$ J_T(\\theta) = \\frac{1}{2T}\\sum_t{\\log[h(x_t; 0)]} + \\log[1-h(y_t; \\theta)] $$","title":"Noise constrastive estimation"},{"content":"I am 29 (as of 2022). My undergrad was Economics at a local university, and I am going back to school so I can pursue higher education in the field of Machine Learning \u0026amp; Artificial Intelligence.\n","permalink":"https://iamtu.dev/about/","summary":"I am 29 (as of 2022). My undergrad was Economics at a local university, and I am going back to school so I can pursue higher education in the field of Machine Learning \u0026amp; Artificial Intelligence.","title":"About"}]