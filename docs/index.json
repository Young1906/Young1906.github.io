[{"content":" (note) Editting note TODO:\nMotivation section Introduction to heat equation BTCS scheme PINN Theory? Coding Citation? Compare results with close-form solution Proof read? TLDR Surveying numerical methods (finite difference methods) and physics-informed neural networks to solve a 1D heat equation. This post was heavily inspired by:\n(Book) Partial Differential Equations for Scientists and Engineers - Standley J. Farlow for deriving closed-form solution. (Article) Finite-Difference Approximations to the Heat Equation (Course) ETH Zurich | Deep Learning for Scientific Computing 2023 for Theory and Implementation of Physics-Informed Neural Network. Introduction Physics-Informed Machine Learning (PIML) is an exciting subfield of Machine Learning that aims to incorporate physical laws and/or constraints into statistical machine learning. The representations of the laws and constraints can be categorized into three groups (with decreasing strength of inductive bias):\nPartial differential equations (PDE) Symmetry: translatation, rotation invariant. And intuitive physical constraints. The PINN method incorporates PDE into the learning problem by adding PDE as a regularization term into the machine learning loss term.\nHeat equations This instance of the 1D heat equation describes how the temperature of an insulated rod changes over time at any point on the rod, where the two ends of the rod are kept at a constant temperature of \\(0^o C\\) and the initial temperature of the rod was given by a function of location \\(x\\).\n$$ \\begin{equation} \\begin{aligned} PDE: \u0026amp; \u0026amp; u_t = \\alpha^2 u_{xx} \u0026amp; \u0026amp; 0 \u0026lt; x\u0026lt; 1 \u0026amp; \u0026amp; 0 \u0026lt; t \u0026lt; \\infty \\\\ BCs: \u0026amp; \u0026amp; \\begin{cases} u(0, t) = 0\\\\ u(1, t) = 0 \\end{cases} \u0026amp; \u0026amp; 0 \u0026lt; t \u0026lt; \\infty \\\\ IC: \u0026amp; \u0026amp; u(x, 0) = \\sin(2\\pi x) \u0026amp; \u0026amp; 0 \\leq x \\leq 1 \\end{aligned} \\end{equation} $$\nSolving heat equation with variables seperation Suppose that we can factorize \\(u(x, t) = X(x)T(t)\\), from the PDE we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; X(x)T^\\prime(t) = \\alpha^2 X^{\\prime\\prime}(x)T(t)\\\\ \\implies \u0026amp; \\frac{T^\\prime(t)}{\\alpha^2 T(t)} = \\frac{X^{\\prime\\prime}(x)}{X(x)} = \\mu \\\\ \\implies \u0026amp; \\begin{cases} T^\\prime(t) - \\mu\\alpha^2 T(t) = 0 \u0026amp; \u0026amp; (2a) \\\\ X^{\\prime\\prime}(x) - \\mu X(x) = 0 \u0026amp; \u0026amp; (2b) \\end{cases} \\end{aligned} \\end{equation} $$\nFrom equation (2a), \\(T(t) = Ae^{\\mu\\alpha^2t}\\). This implies \\(\\mu\\) must be negative so that \\(T\\) doesn\u0026rsquo;t go to \\(\\infty\\). Let \\(\\mu = -\\lambda^2\\), so \\(T(t) = Ae^{-\\lambda^2\\alpha^2t}\\). Replacing into (2), we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; X^{\\prime\\prime}(x) + \\lambda^2 X(x) = 0 \\\\ \\implies \u0026amp; X(x) = B \\sin\\lambda x + C\\cos\\lambda x \\end{aligned} \\end{equation} $$\nSubstitute \\(T(t), X(x)\\) into \\(u(x, t)\\):\n$$ \\begin{equation} u(x, t) = e^{-\\lambda^2\\alpha^2 t}(A\\sin\\lambda x + B\\cos\\lambda x) \\end{equation} $$\nSubsititute this into boundary conditions:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\begin{cases} u(0, t) = 0 \\\\ u(1, t) = 0 \\end{cases} \\\\ \\implies \u0026amp; \\begin{cases} e^{-\\lambda^2\\alpha^2 t}(A \\sin 0 + B \\cos 0)= 0 \\\\ e^{-\\lambda^2\\alpha^2 t}(A \\sin \\lambda + B \\cos \\lambda) = 0 \\\\ \\end{cases}\\\\ \\implies \u0026amp; \\begin{cases} B = 0 \\\\ \\lambda = n\\pi \u0026amp; n = 1, 2, \\cdots \\end{cases} \\end{aligned} \\end{equation} $$\nSo for a given \\(n\\), we have a particular solution for \\(u(x, t)\\):\n$$ \\begin{equation} u_n(x, t) = A_n e^{-n^2\\pi^2\\alpha^2 t} \\sin n\\pi x \\end{equation} $$\nAnd the general solution for \\(u(x, t)\\):\n$$ \\begin{equation} u(x, t) = \\sum_{n=1}^\\infty{A}_n e^{-n^2\\pi^2\\alpha^2t} \\sin n\\pi x \\end{equation} $$\nWhere \\(A_n\\) is given by:\n$$ \\begin{equation} A_n = 2\\int_0^1 \\sin 2\\pi x\\sin n\\pi x dx = \\begin{cases} 0 \\quad \\text{if }n \\neq 2\\\\ 1 \\quad \\text{if }n = 2 \\end{cases} \\end{equation} $$\nFinally, we have the solution to the PDE:\n$$ \\begin{equation} \\blue{u(x, t) = e^{-4\\pi^2\\alpha^2t}\\sin 2\\pi x} \\end{equation} $$\nFinite Difference Method Numerical approximation of first and second order derivative First Order Forward Difference Consider a Taylor series expansion of \\(\\phi(x)\\) about point \\(x_i\\)\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\phi(x_i + \\delta x) = \\phi(x_i) + \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} \\delta x + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\delta x^2}{2!} + \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\delta x^3}{3!} + \\cdots \\\\ \\end{aligned} \\end{equation} $$\nReplace \\(\\delta x = \\Delta x \\ll 1\\) in equation (10):\n$$ \\begin{aligned} \u0026amp; \\phi(x_i + \\delta x) = \\phi(x_i) + \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} \\Delta x + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{2!} + \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^3}{3!} + \\cdots \\\\ \\implies \u0026amp; \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} = \\frac{\\phi(x_i +\\Delta x) - \\phi(x_i)}{\\Delta x} \\red{\\underbrace{ - \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x}{2!} - \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{3!} - \\cdots }_{\\text{Truncation error: }\\mathcal{O}(\\Delta x)}}\\\\ \u0026amp; \\blue{\\approx \\frac{\\phi(x_i +\\Delta x) - \\phi(x_i)}{\\Delta x}} \\end{aligned} $$\nNote that in this tutorial, the truncation error is \\(\\mathcal{O}(\\Delta x^2)\\). I haven\u0026rsquo;t been able to understand why yet!!!.\nFirst Order Backward Difference Replace \\(\\delta x = -\\Delta x, \\Delta x \\ll 1\\) in equation (10):\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\phi(x_i + \\delta x) = \\phi(x_i) - \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} \\Delta x + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{2!} - \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^3}{3!} + \\cdots \\\\ \\implies \u0026amp; \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} = \\frac{\\phi(x_i) - \\phi(x_i - \\Delta x)}{\\Delta x} \\red{\\underbrace{ + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x}{2!} - \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{3!} + \\cdots }_{\\text{Truncation error: }\\mathcal{O}(\\Delta x)}}\\\\ \u0026amp; \\blue{\\approx \\frac{\\phi(x_i) - \\phi(x_i - \\Delta x)}{\\Delta x}} \\end{aligned} \\end{equation} $$\nSecond Order Central Difference Replace in equation (10):\n\\(\\delta x = \\Delta x\\) $$ \\begin{equation} \\begin{aligned} \u0026amp; \\phi(x_i + \\Delta x) = \\phi(x_i) + \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} \\Delta x + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{2!} + \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^3}{3!} + \\cdots \\end{aligned} \\end{equation} $$\n\\(\\delta x = -\\Delta x\\) $$ \\begin{equation} \\begin{aligned} \u0026amp; \\phi(x_i - \\Delta x) = \\phi(x_i) - \\frac{\\partial \\phi}{\\partial x}\\bigg\\vert_{x_i} \\Delta x + \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{2!} - \\frac{\\partial^3 \\phi}{\\partial x^3}\\bigg\\vert_{x_i} \\frac{\\Delta x^3}{3!} + \\cdots \\end{aligned} \\end{equation} $$\nAdding equation (12) and (13) we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\phi(x_i + \\Delta x) + \\phi(x_i - \\Delta x) = 2 \\phi(x_i) + 2 \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{2!} + 2 \\frac{\\partial^4 \\phi}{\\partial x^4}\\bigg\\vert_{x_i} \\frac{\\Delta x^4}{4!} + \\cdots\\\\ \\implies \u0026amp; \\frac{\\partial^2 \\phi}{\\partial x^2}\\bigg\\vert_{x_i} = \\frac{\\phi(x_i + \\Delta x) - 2\\phi(x_i) + \\phi(x - \\Delta x)}{\\Delta x^2} \\red{\\underbrace{ - 2 \\frac{\\partial^4 \\phi}{\\partial x^4}\\bigg\\vert_{x_i} \\frac{\\Delta x^2}{4!} - \\cdots}_{\\mathcal{O}(\\Delta x^2)}}\\\\ \u0026amp; \\blue{ \\approx \\frac{\\phi(x_i + \\Delta x) - 2\\phi(x_i) + \\phi(x - \\Delta x)}{\\Delta x^2} } \\end{aligned} \\end{equation} $$\nFinite Difference Method for the Heat Equation Discretize the domain \\(\\mathcal{D} = (0, 1) \\times (0, T)\\) by constructing a grid \\(\\{x_i\\}_{i=1\\cdots N} \\times \\{t_m\\}_{m=1\\cdots M}\\). Where:\n\\(x_i = (i - 1) \\Delta x,\\quad \\Delta x = \\frac{1}{N - 1}\\) \\(t_m = (m - 1) \\Delta t,\\quad \\Delta t = \\frac{T}{M - 1}\\) Let \\(u(x, t)\\) be the true solution to the PDE\nForward Time, Centered Space (FTCS) Using First Order Forward Difference (equation 10) to approximate parital derivative of \\(u\\) at a grid point \\((x_i, t_m)\\):\n$$ \\begin{equation} \\begin{aligned} \\frac{\\partial u}{\\partial t} \\bigg\\vert_{x=x_i, t=t_m} \u0026amp; = \\frac{u(x_i, t_m + \\Delta t) - u(x_i, t_m)}{\\Delta t} + \\mathcal{O}(\\Delta t)\\\\ \u0026amp; \\approx \\frac{u_i^{m+1}-u_i^m}{\\Delta t} \\end{aligned} \\end{equation} $$\nUsing Second Order Central Difference (equation 14) to approximate the second order partial derivative of \\(u\\) with respect to \\(x\\) at the grid point: $$ \\begin{equation} \\begin{aligned} \\frac{\\partial^2 u}{\\partial x^2} \\bigg\\vert_{x=x_i, t=t_m} \u0026amp; = \\frac{u(x_i + \\Delta x, t_m) - 2 u(x_i, t_m) + u(x_i - \\Delta x, t_m)}{\\Delta x^2} + \\mathcal{O}(\\Delta x^2)\\\\ \u0026amp; \\approx \\frac{u_{i+1}^m - 2 u_i^m + u_{i-1}^m}{\\Delta x^2} \\end{aligned} \\end{equation} $$\nWhere \\(u_i^m\\) is the numerical approximation of true function evaluated at the grid point \\((x_i, t_m)\\). Replacing equation (15) and (16) into the LHS and RHS of the PDE in (1):\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\frac{u_i^{m+1}-u_i^m}{\\Delta t} = \\alpha^2 \\frac{u_{i+1}^m - 2 u_i^m + u_{i-1}^m}{\\Delta x^2}\\\\ \\implies \u0026amp; u_i^{m+1} = u_i^m + \\frac{\\alpha^2 \\Delta t}{\\Delta x^2} (u_{i+1}^m - 2 u_i^m + u_{i-1}^m) \\\\ \u0026amp; = \\blue{u_i^m(1 - 2r) + r(u_{i+1}^m + u_{i-1}^m)} \\end{aligned} \\end{equation} $$\nWhere \\(r = \\frac{\\alpha^2\\Delta t}{\\Delta x^2}\\). In order for \\(u(x, t)\\) reach steady state, \\(r\\) must be smaller than \\(\\frac{1}{2}\\). The proof was provided in Von Neumann Stability Analysis.\nTODO: Haven\u0026rsquo;t understood yet !!!\nEquation (17) allows us to sequentially compute the approximation \\(u_i^m\\) at any point \\((x_i, t_m)\\), where \\(u_i^1 = u(x_i, 0), i = 1\\cdots N\\) were given by the initial and boundary conditions. In matrix notation, the series of equation can be written as:\n$$ \\begin{equation} \\begin{aligned} \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ r \u0026amp; 1 - 2r \u0026amp; r \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp;\\vdots \u0026amp;\\ddots \u0026amp;\\vdots \u0026amp;\\vdots \u0026amp;\\vdots \\\\ 0 \u0026amp;0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; r \u0026amp; 1 - 2r \u0026amp; r \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix} \\underbrace{\\begin{bmatrix} u_1^m \\\\ u_2^m \\\\ \\vdots \\\\ u_{N-1}^m \\\\ u_N^m \\\\ \\end{bmatrix}}_{u^m} = \\underbrace{\\begin{bmatrix} u_1^{m+1} \\\\ u_2^{m+1} \\\\ \\vdots \\\\ u_{N-1}^{m+1} \\\\ u_N^{m+1} \\\\ \\end{bmatrix}}_{u^{m+1}} \\end{aligned} \\end{equation} $$\nNote that \\(u_1^m\\), \\(u_N^m\\) are always equal to its value in the next time step. This is due to the boundary condition, the temperature at the boundary is always \\(0\\).\n(code) Implementation of FTCS scheme solve_fdm() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import numpy as np def solve_fdm(N: int, M: int, T: float): \u0026#34;\u0026#34;\u0026#34; solving 1D heat equation: PDE: u_t = u_xx (\\alpha^2 = 1) BCs: u(0, t) = u(1, t) = 0 ICs: u(x, 0) = x - x**2 args: - N, M : number of collocation points in spacial and temporal dimension - T : solving from t = 0 to T \u0026#34;\u0026#34;\u0026#34; # constructing the grid dx = 1 / (N - 1) # 0 \u0026lt;= x \u0026lt;= 1 dt = T / (M - 1) # 0 \u0026lt; t \u0026lt;= T r = dt/dx**2 # (alpha = 1) # Condition for numerical stability assert r \u0026lt; .5, ValueError(f\u0026#34;Choose smaller r, r={r:.4f}\u0026#34;) x_grid = np.linspace(0, 1, N) # approximate the result U = np.zeros((N, M)) # already satisfied the BCs # IC impose initial condition ic = lambda x: np.sin(2 * np.pi * x) U[:, 0] = np.vectorize(ic)(x_grid) # kernel to approximate 2nd derivative of u wrt x ker = np.array([1., -2., 1.], dtype=np.float64) for i in range(1, M): ut = np.convolve(U[:, i - 1], ker, mode=\u0026#34;same\u0026#34;) U[:,i] = U[:, i-1] + r * ut return U 1 2 # solving the PDE with 100x4000 grid from t=0 to t=0.2 U = solve_fdm(100, 4000, .2) Backward Time, Centered Space (BTCS) Using First Order Backward Difference (equation 11) to approximate parital derivative of \\(u\\) at a grid point \\((x_i, t_m)\\):\n$$ \\begin{equation} \\begin{aligned} \\frac{\\partial u}{\\partial t} \\bigg\\vert_{x=x_i, t=t_m} \u0026amp; = \\frac{u(x_i, t_m) - u(x_i, t_m - \\Delta t)}{\\Delta t} + \\mathcal{O}(\\Delta t)\\\\ \u0026amp; \\approx \\frac{u_i^{m}-u_i^{m-1}}{\\Delta t} \\end{aligned} \\end{equation} $$\nUsing Second Order Central Difference (equation 14) to approximate the second order partial derivative of \\(u\\) with respect to \\(x\\) at the grid point: $$ \\begin{equation} \\begin{aligned} \\frac{\\partial^2 u}{\\partial x^2} \\bigg\\vert_{x=x_i, t=t_m} \u0026amp; = \\frac{u(x_i + \\Delta x, t_m) - 2 u(x_i, t_m) + u(x_i - \\Delta x, t_m)}{\\Delta x^2} + \\mathcal{O}(\\Delta x^2)\\\\ \u0026amp; \\approx \\frac{u_{i+1}^m - 2 u_i^m + u_{i-1}^m}{\\Delta x^2} \\end{aligned} \\end{equation} $$\nReplacing equation (19), and (20) into LHS and RHS of the PDE in (1) respectively we have:\n$$ \\begin{equation} \\begin{aligned} \u0026amp; \\frac{u_i^{m}-u_i^{m-1}}{\\Delta t} = \\alpha^2 \\frac{u_{i+1}^m - 2 u_i^m + u_{i-1}^m}{\\Delta x^2}\\\\ \\implies \u0026amp; u_i^{m-1} = u_i^m - \\frac{\\alpha^2 \\Delta t}{\\Delta x^2} (u_{i+1}^m - 2 u_i^m + u_{i-1}^m) \\\\ \u0026amp; = \\blue{u_i^m(1 + 2r) - r(u_{i+1}^m + u_{i-1}^m)} \\end{aligned} \\end{equation} $$\nWhere \\(r = \\frac{\\alpha^2\\Delta t}{\\Delta x^2}\\). Rewriting equation(21) in matrix notation:\n$$ \\begin{equation} \\begin{aligned} \\underbrace{\\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ -r \u0026amp; 1 + 2r \u0026amp; -r \u0026amp; \\cdots \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \\\\ \\vdots \u0026amp; \\vdots \u0026amp;\\vdots \u0026amp;\\ddots \u0026amp;\\vdots \u0026amp;\\vdots \u0026amp;\\vdots \\\\ 0 \u0026amp;0 \u0026amp; 0 \u0026amp; \\cdots \u0026amp; -r \u0026amp; 1 + 2r \u0026amp; -r \\\\ 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 0 \u0026amp; 1 \\end{bmatrix}}_A \\underbrace{\\begin{bmatrix} u_1^m \\\\ u_2^m \\\\ \\vdots \\\\ u_{N-1}^m \\\\ u_N^m \\\\ \\end{bmatrix}}_{\\mathbf{u}^m} = \\underbrace{\\begin{bmatrix} u_1^{m-1} \\\\ u_2^{m-1} \\\\ \\vdots \\\\ u_{N-1}^{m-1} \\\\ u_N^{m-1} \\\\ \\end{bmatrix}}_{\\mathbf{u}^{m-1}} \\end{aligned} \\end{equation} $$\nSo that we can sequentially compute the next state by solving the system of linear equations in (22):\n$$ \\blue{ \\mathbf{u}^{m} = A^{-1} \\mathbf{u}^{m-1}; \\quad m = 2,\\cdots M } $$\nWhere \\(\\mathbf{u}_i^1\\) are given by the initial and boundary conditions. Unlike FTCS, BTCS are unconditionally stable with respect to the choice of \\(r\\). Therefore we can choose much fewer steps along temporal dimension.\n(code) Implementation of BTCS scheme solve_bdm() 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 import numpy as np from scipy.sparse import diags def solve_bdm(N: int, M: int, T: float): \u0026#34;\u0026#34;\u0026#34; solving 1D heat equation using BTCS scheme PDE: u_t = u_xx (\\alpha^2 = 1) BCs: u(0, t) = u(1, t) = 0 ICs: u(x, 0) = x - x**2 args: - N, M : number of collocation points in spacial and temporal dimension - T : solving from t = 0 to T \u0026#34;\u0026#34;\u0026#34; # constructing the grid dx = 1 / (N - 1) # 0 \u0026lt;= x \u0026lt;= 1 dt = T / (M - 1) # 0 \u0026lt; t \u0026lt;= T r = dt/dx**2 # (alpha = 1) # construct A: A = diags([-r, 1 + 2 * r, -r], [-1, 0, 1], shape=(N, N)).toarray() A[0, :] = 0 A[-1, :] = 0 A[0, 0] = 1 A[-1,-1] = 1 A_inv = np.linalg.inv(A) # approximate the result U = np.zeros((N, M)) # already satisfied the BCs # IC impose initial condition x_grid = np.linspace(0, 1, N) ic = lambda x: np.sin(2 * np.pi * x) U[:, 0] = np.vectorize(ic)(x_grid) for m in range(1, M): U[:, m] = A_inv @ U[:, m-1] return U 1 2 # solving the PDE with 100x100 grid from t=0 to t=0.2 U = solve_bdm(100, 100, .2) We can see that results of FTCS and BTCS agree with each other, however, BTCS only using a \\(100 \\times 100\\) grid while FTCS using \\(100 \\times 4000\\) grid.\nThere is one more scheme for finite different methods which is Crank-Nicolson methods, which use central diffence method to estimate first order derivative !!\nPhysics Informed Neural Network Let\u0026rsquo;s rewrite the heat equation in a more general form:\n$$ \\begin{equation} \\begin{aligned} PDE: \u0026amp; \u0026amp; u_t = \\alpha^2 u_{xx} \u0026amp; \u0026amp; 0 \u0026lt; x\u0026lt; 1 \u0026amp; \u0026amp; 0 \u0026lt; t \u0026lt; \\infty \\\\ BCs: \u0026amp; \u0026amp; \\begin{cases} u(0, t) = f_0(t)\\\\ u(1, t) = f_1(t) \\end{cases} \u0026amp; \u0026amp; 0 \u0026lt; t \u0026lt; \\infty \\\\ IC: \u0026amp; \u0026amp; u(x, 0) = \\phi(t) \u0026amp; \u0026amp; 0 \\leq x \\leq 1 \\end{aligned} \\end{equation} $$\nIn our case, \\(f_0(t) = f_1(t) = 0\\) and \\(\\phi(x) = \\sin 2\\pi x\\). PINN approximate the function \\(u(x, t)\\) by a neural network \\(U_\\theta(x, t)\\), and then learn the networks parameters \\(\\theta\\) by minimize the loss function:\n$$ \\begin{equation} \\begin{aligned} \\mathcal{L}(\\theta) \u0026amp; = \\frac{1}{N}\\sum_{i=1}^N{[U_\\theta(x_i, t_i) - u_i]^2} \u0026amp; \\text{(Supervised loss)}\\\\ \u0026amp; + \\frac{\\lambda_j}{M_j} \\sum_{j=1}^{M_j}{\\bigg[ \\frac{\\partial U_\\theta}{\\partial t}- \\alpha^2 \\frac{\\partial^2 U_\\theta}{\\partial x^2} \\bigg](x_j, t_j)} \u0026amp; \\text{(PDE residual)}\\\\ \u0026amp; + \\frac{\\lambda_k}{M_k} \\sum_{k=1}^{M_k}{[(U_\\theta(0, t_k) - f_0(t_k))^2 + (U_\\theta(1, t_k) - f_1(t_k))^2]} \u0026amp; \\text{(Boundary conditions)} \\\\ \u0026amp; + \\frac{\\lambda_h}{M_h} \\sum_{h=1}^{M_h}{[U_\\theta(x_h, 0) - \\phi(x_h)]^2} \u0026amp; \\text{(Initial condition)} \\end{aligned} \\end{equation} $$\nThe first term is the supervised loss, coinciding with statistical machine learning. Where \\({(x_i, t_i, u_i)}_{i=1\\cdots N}\\) is the set of collocation points \\((x_i, t_i)\\), and value of \\(u_i = u(x_i, t_i)\\).\nThe second term is the PDE residual, where:\n\\(\\frac{\\partial U_\\theta}{\\partial t}\\) is the partial derivative of the network \\(U_\\theta\\) with respect to the time input \\(t\\). Similarly, \\(\\frac{\\partial^2 U_\\theta}{\\partial x^2}\\) is the second derivative of the network with repsect to location \\(x\\). Second and third terms are the initial and boundary conditions, given by equation (23).\nWe don\u0026rsquo;t necessarily have access to the first loss term. In the implementation bellow, I ignored the first loss term. For the remaining three loss terms:\nPDE residual: \\(x_j \\sim \\text{Uniform}(0, 1); t_j \\sim\\text{Uniform}(0, 0.2)\\) Boundary condition: \\(t_k \\sim \\text{Uniform}(0, 0.2)\\) Initial condition: \\(x_h \\sim \\text{Uniform}(0, 1)\\) Note: The code need some refactoring but it still works.\n(code) JAX Implementation of PINN 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 \u0026#34;\u0026#34;\u0026#34; Solving PDE using PINN PDE: u_t = \\alpha^2 u_{xx} 0 \u0026lt; x \u0026lt; 1; 0 \u0026lt; t \u0026lt; \\infty BCs: u(0, t) = u(1, t) = 0 0 \u0026lt; t \u0026lt; \\infty ICs: u(x, 0) = \\sin(2\\pi x) 0 \u0026lt; x \u0026lt; 1 Params: alpha^2 = 1 \u0026#34;\u0026#34;\u0026#34; import jax import jax.numpy as jnp import equinox as eqx import optax from tqdm import trange from matplotlib import pyplot as plt class U(eqx.Module): \u0026#34;\u0026#34;\u0026#34; Simple MLP taking (x, t) as input, return mlp(x, t) \u0026#34;\u0026#34;\u0026#34; layers: list # def __init__(self, layers: list[int], key): self.layers = [] for _in, _out in zip(layers[:-1], layers[1:]): key, subkey = jax.random.split(key, 2) self.layers.append(eqx.nn.Linear(_in, _out, key=subkey)) def __call__(self, x, t): \u0026#34;\u0026#34;\u0026#34; assuming x \\in R^{n x (d - 1)}, t \\in R \u0026#34;\u0026#34;\u0026#34; out = jnp.concatenate([x, t], axis=-1) for layer in self.layers[:-1]: out = layer(out) out = jax.nn.tanh(out) out = self.layers[-1](out) return jax.nn.tanh(out) def interior_loss(u, x, t): \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; # First and second derivative of u wrt x u_x = jax.grad(lambda x, t: jnp.squeeze(u(x, t)), argnums=0) u_xx = jax.grad(lambda x, t: jnp.squeeze(u_x(x, t)), argnums=0) u_t = jax.grad(lambda x, t: jnp.squeeze(u(x, t)), argnums=1) pde_resid = jax.vmap(u_t)(x, t) - jax.vmap(u_xx)(x, t) return jnp.mean(pde_resid**2) def boundary_loss(u, x, t, f_bc: callable): \u0026#34;\u0026#34;\u0026#34; \u0026#34;\u0026#34;\u0026#34; # compute boundary value at each collocation point y = jax.vmap(f_bc)(x, t) y_hat = jax.vmap(u)(x, t) return jnp.mean((y - y_hat) ** 2) def initial_condition_loss(u, x, t, f_ic): y = jax.vmap(f_ic)(x, t) y_hat = jax.vmap(u)(x, t) return jnp.mean((y - y_hat) ** 2) def generate_interior_batch(key, n): \u0026#34;\u0026#34;\u0026#34; interior collocation points \u0026#34;\u0026#34;\u0026#34; # sample, discretizing interior point key, subkey = jax.random.split(key, 2) X = jax.random.uniform(subkey, shape=(n, 1), minval=1e-5, maxval=1-1e-5) key, subkey = jax.random.split(key, 2) T = jax.random.uniform(subkey, shape=(n, 1), minval=1e-5, maxval=.2) return X, T def generate_ic_batch(key, n): \u0026#34;\u0026#34;\u0026#34; initial collocation points {(x_i, 0)} \u0026#34;\u0026#34;\u0026#34; # sample, discretizing interior point key, subkey = jax.random.split(key, 2) X = jax.random.uniform(subkey, shape=(n, 1), minval=1e-5, maxval=1-1e-5) key, subkey = jax.random.split(key, 2) T = jnp.zeros(shape=(n, 1)) return X, T def generate_bc_batch(key, n): \u0026#34;\u0026#34;\u0026#34; initial collocation points {(0/1, t_m)} \u0026#34;\u0026#34;\u0026#34; # sample, discretizing interior point key, subkey = jax.random.split(key, 2) X = jax.random.randint(subkey, shape=(n, 1), minval=0., maxval=2.) key, subkey = jax.random.split(key, 2) T = jax.random.uniform(subkey, shape=(n, 1), minval=1e-5, maxval=.2) return X, T def main(): key = jax.random.PRNGKey(0) key, subkey = jax.random.split(key, 2) u = U([2, 128, 64, 32, 1], subkey) def loss_fn(u, x_i, t_i, x_ic, t_ic, x_bc, t_bc, f_ic, f_bc): \u0026#34;\u0026#34;\u0026#34; u: model x_i, t_i: interior collocation point x_ic, t_ic: initial points x_bc, t_bc: boundar points f_ic: initial condition f_bc: boundary condition \u0026#34;\u0026#34;\u0026#34; return interior_loss(u, x_i, t_i) +\\ initial_condition_loss(u, x_ic, t_ic, f_ic) +\\ boundary_loss(u, x_bc, t_bc, f_bc) # definite initial condition def f_ic(x, t): return jnp.sin(2 * jnp.pi * x) def f_bc(x, t): return 0. grad_loss_fn = jax.value_and_grad(loss_fn) optim = optax.adam(1e-3) optim_state = optim.init(u) @jax.jit def train_step(model, key, optim_state): # Generate data point ic_key, bc_key, i_key = jax.random.split(key, 3) x_ic, t_ic = generate_ic_batch(ic_key, 16) x_bc, t_bc = generate_bc_batch(bc_key, 16) x_i, t_i = generate_interior_batch(i_key, 128) loss_val, grads = grad_loss_fn(model, x_i, t_i, x_ic, t_ic, x_bc, t_bc, f_ic, f_bc) updates, optim_state = optim.update(grads, optim_state) new_model = eqx.apply_updates(model, updates) return loss_val, new_model, key, optim_state losses = [] pbar = trange(10000) for i in pbar: loss, u, key, optim_state = train_step(u, key, optim_state) pbar.set_description(f\u0026#34;Loss = {loss:.4f}\u0026#34;) losses.append(loss) plt.plot(losses) plt.show() References ","permalink":"https://iamtu.dev/posts/heat/","summary":"(note) Editting note TODO:\nMotivation section Introduction to heat equation BTCS scheme PINN Theory? Coding Citation? Compare results with close-form solution Proof read? TLDR Surveying numerical methods (finite difference methods) and physics-informed neural networks to solve a 1D heat equation. This post was heavily inspired by:\n(Book) Partial Differential Equations for Scientists and Engineers - Standley J. Farlow for deriving closed-form solution. (Article) Finite-Difference Approximations to the Heat Equation (Course) ETH Zurich | Deep Learning for Scientific Computing 2023 for Theory and Implementation of Physics-Informed Neural Network.","title":"Learning to solve heat equation"},{"content":" PAGE UNDER CONSTRUCTION\nResumes / Portfolio Academic Resume Research portfolio Professional Resume Researchs / Publications Accepted (BME 2020) Tu, Do Thanh, Thuong Nguyen, Anh Tho Le, Sinh Nguyen, Huong Ha. \u0026ldquo;Automated EOG removal from EEG signal using Independent Component Analysis and Machine Learning Algorithms\u0026rdquo; at The 8th International Conference in Vietnam on the Development of Biomedical Engineering. (link)\n(ICHST 2023) Tu, Do Thanh, Luan Van Tran, Tho Anh Le, Thao Mai Thi Le, Lan‑Anh Hoang Duong, Thuong Hoai Nguyen, Anh Minh Hoang An, Duy The Phan, Khiet Thu Thi Dang, Quyen Hoang Quoc Vo, Nam Phuong Nguyen, Huong Thanh Thi Ha. \u0026ldquo;Stress prediction using machine‑learning technique on physiological signal\u0026rdquo; (link)\n(IJCNN 2024) Tu T. Do, Mai Anh Vu, Hoang Thien Ly, Thu Nguyen, Steven A. Hicks, Michael A. Riegler, Pål Halvorsen Halvorsen and Binh T. Nguyen. \u0026ldquo;Blockwise Principal Component Analysis for monotone missing data imputation and dimensionalityreduction\u0026rdquo;\nSubmitted Mai Anh Vu , Thu Nguyen , Tu T. Do , Nhan Phan, Nitesh V. Chawla, Pål Halvorsen, Michael A. Riegler and Binh T. Nguyen. \u0026ldquo;Conditional expectation with regularization for missing data imputation\u0026rdquo;\nTu T. Do, Mai Anh Vu, Hoang Thien Ly, Thu Nguyen, Steven A. Hicks, Michael A. Riegler, Pål Halvorsen Halvorsen and Binh T. Nguyen. \u0026ldquo;Estimating lower‑dimensional space representation in Principal Component Analysis under missing data condition\u0026rdquo;\nContact Email tu.dothanh1906@gmail.com Phone (+84) 343 610 772 Github young1906 Facebook tu.dothanh Update 2024 Apr Currently looking for PhD Opportunity in the field of AI/ML. My research interest lies in Reinforcement Learning and Physics-Informed Machine Learning. 2022 I am 29. My undergrad was Economics at a local university, and I am going back to school so I can pursue higher education in the field of Machine Learning \u0026amp; Artificial Intelligence. ","permalink":"https://iamtu.dev/about/","summary":"PAGE UNDER CONSTRUCTION\nResumes / Portfolio Academic Resume Research portfolio Professional Resume Researchs / Publications Accepted (BME 2020) Tu, Do Thanh, Thuong Nguyen, Anh Tho Le, Sinh Nguyen, Huong Ha. \u0026ldquo;Automated EOG removal from EEG signal using Independent Component Analysis and Machine Learning Algorithms\u0026rdquo; at The 8th International Conference in Vietnam on the Development of Biomedical Engineering. (link)\n(ICHST 2023) Tu, Do Thanh, Luan Van Tran, Tho Anh Le, Thao Mai Thi Le, Lan‑Anh Hoang Duong, Thuong Hoai Nguyen, Anh Minh Hoang An, Duy The Phan, Khiet Thu Thi Dang, Quyen Hoang Quoc Vo, Nam Phuong Nguyen, Huong Thanh Thi Ha.","title":"About"},{"content":" Problem Given a statistical model \\(P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})\\), which generate set of observations \\(\\boldsymbol{X}\\), where \\(\\boldsymbol{Z}\\) is a latent variable and unknow parameter vector \\(\\boldsymbol{\\theta}\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that maximize the marginal likelihood:\n$$ \\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{X}) = P(\\boldsymbol{X} | \\boldsymbol{\\theta}) = \\int_{\\boldsymbol{Z}}P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})d\\boldsymbol{Z} $$\nAs an example for this type of problem, there are two (unfair) coin A and B with probability of head for each coin is \\(p_A(H) = p \\text{ and } p_B(H) = q\\). For each trial, we select coin A with probability \\(p(A) = \\tau\\) and coin B with probability \\(p(B) = 1 -\\tau\\), toss the coin and record the observation. The set of observations \\(\\boldsymbol{X}\\) is the record of head or tail \\(\\{H, T, H, H, \\cdots\\}\\), the latent variable which is unobserved is which coint is selected for each trail \\(\\{A, B, B, A, \\cdots\\}\\), and the unknown parameter vector \\(\\boldsymbol{\\theta} = [p, q, \\tau]\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that best fit observations; EM is an instance of Maximum Likelihood Estimation (MLE).\nThe EM algorithm The algorithm The EM algorithm seeks for \\(\\boldsymbol{\\theta}\\) by first initiates a random parameter vector \\(\\boldsymbol{\\theta}^{(0)}\\) and then iteratively performs two steps, namely the expectation step (E step) and the maximization step (M step):\n(The E step) the expected loglikelihood of \\(\\boldsymbol{\\theta}\\), with respect to the current conditional distribution of \\(\\boldsymbol{Z}\\) given observations \\(\\boldsymbol{X}\\) and current estimation of \\(\\boldsymbol{\\theta}^{(t)}\\) $$ Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) = \\mathbb{E}_{\\boldsymbol{Z} \\sim P(. | \\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)})} {[ \\log P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) ]} $$\n(The M step) update parameter vector \\(\\boldsymbol{\\theta}\\) $$ \\boldsymbol{\\theta}^{(t+1)} = \\arg\\max_{\\boldsymbol{\\theta}} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) $$\nProof of correctness Setup We need to proof that updating parameter vector \\(\\boldsymbol{\\theta}\\) by EM algorithm will monotonically increase the marginal likelihood of \\(P(X|\\theta)\\)\n$$ \\log P(X|\\theta^*) - \\log P(X|\\theta^{(t)}) \\geq Q(\\theta^* | \\theta^{(t)}) - Q(\\theta^{(t)} | \\theta^{(t)}) \\geq 0 $$ Where the second inequality come from \\(\\theta^* = \\arg\\max_\\theta Q(\\theta | \\theta^{(t)}) \\)\nProof $$ \\begin{aligned} \u0026amp; P(X, Z | \\theta) = P(Z | X, \\theta) P(X | \\theta) \u0026amp; \\text{\\tiny(Bayes theorem)} \\\\ \\iff \u0026amp; \\log P(X, Z | \\theta) = \\log P(Z |X, \\theta) + \\log P(X |\\theta) \u0026amp; \\\\ \\iff \u0026amp; \\log P(X | \\theta) = \\log P(X, Z | \\theta) - \\log P (Z | X,\\theta) \u0026amp; \\\\ \\implies \u0026amp; \\log P(X | \\theta) = \\underbrace{ \\mathbb{E}_{Z|X,\\theta^{(t)}}{[\\log P(X, Z | \\theta)]} }_{Q(\\theta | \\theta^{(t)})} + \\underbrace{ - \\mathbb{E}_{Z|X,\\theta^{(t)}}{[\\log P(Z|X,\\theta)]} }_{H(\\theta | \\theta^{(t)})} \u0026amp; \\text{\\tiny(taking expectation for both side)} \\end{aligned} $$\nSo consider \\(\\log P(X|\\theta) - \\log P(X|\\theta^{(t)})\\) is the change in loglikelihood of observed data when we update the parameter vector \\(\\theta\\)\n$$ \\begin{aligned} \\log P(X|\\theta) - \\log P(X|\\theta^{(t)}) \u0026amp; = Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)}) \\\\ \u0026amp; + \\underbrace{H(\\theta|\\theta^{(t)}) - H(\\theta^{(t)}|\\theta^{(t)})}_{A} \\\\ \\end{aligned} $$\nQuantity \\(A\\)\n$$ \\begin{aligned} A \u0026amp; = - \\mathbb{E}_{Z|X,\\theta^{(t)}}{[\\log P(Z|X,\\theta)]} - \\big( -\\mathbb{E}_{Z|X,\\theta^{(t)}}{[\\log P(Z|X,\\theta^{(t)})]} \\big) \\\\ \u0026amp; = \\mathbb{E}_{Z|X,\\theta^{(t)}}{\\bigg[ \\log P(Z|X,\\theta^{(t)}) - \\log P(Z|X,\\theta) \\bigg]} \\\\ \u0026amp; = \\int_{Z} P(Z|X,\\theta^{(t)}) \\log{\\frac{P(Z|X,\\theta^{(t)})}{P(Z|X,\\theta)} dZ}\\\\ \u0026amp; \\geq 0 \u0026amp;\\text{\\tiny(Gibb\u0026rsquo;s inequality)} \\end{aligned} $$\nSo that\n$$ \\begin{aligned} \\log P(X|\\theta) - \\log P(X|\\theta^{(t)}) \u0026amp; = Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)}) + A \\\\ \u0026amp; \\geq Q(\\theta|\\theta^{(t)}) - Q(\\theta^{(t)}|\\theta^{(t)}) \u0026amp; \\square \\end{aligned} $$\nExamples EM for the coin example Setup\nParameter vector \\(\\boldsymbol{\\theta} = [p, q, \\tau]\\), and its estimation at step (t) is \\(\\boldsymbol{\\theta}^{(t)} = [p_t, q_t, \\tau_t]\\)\nThe \\(i^{th}\\) observation \\(x^{(i)}\\) is either head (H) or tail (T).\nThe coin selected for the \\(i^{th}\\) trail \\(z^{(i)}\\) is either A or B:\n\\(p(z^{(i)} = A) = \\tau\\) \\(p(z^{(i)} = B) = 1 -\\tau\\). For both cases, $$ \\begin{equation} p(z^{(i)}) = \\tau^{\\mathbb{I}(z^{(i)}=A)}(1-\\tau)^{\\mathbb{I}(z^{(i)}=B)} \\end{equation} $$\nWhen selected the coin A,\nProbability that we get a head (H): \\(p(x^{(i)}=H | z^{(i)} = A) = p\\) Probability that we get a head (T): \\(p(x^{(i)}=T | z^{(i)} = A) = 1 - p\\) For both cases, $$ \\begin{equation} p(x^{(i)} | z^{(i)}=A) = p^{\\mathbb{I}(x^{(i)}=H)}(1 - p)^{\\mathbb{I}(x^{(i)}=T)} \\end{equation} $$\nSimilarly, when B is selected $$ \\begin{equation} p(x^{(i)} | z^{(i)}=B) = q^{\\mathbb{I}(x^{(i)}=H)}(1 - q)^{\\mathbb{I}(x^{(i)}=T)} \\end{equation} $$\nWhere \\(\\mathbb{I}(\\cdot)\\) is an indicator function on a predicate $$ \\mathbb{I}(p) = \\begin{cases} 1 \\quad \\text{if } p \\text{ is True}\\\\ 0 \\quad \\text{otherwise} \\end{cases} $$\nOnce again, we generalize for both cases of \\(z^{(i)}\\)\n$$ \\begin{equation} \\begin{aligned} p(x^{(i)} | z^{(i)}) = [p^{\\mathbb{I}(x^{(i)}=H)}(1 - p)^{\\mathbb{I}(x^{(i)}=T)}]^{\\mathbb{I}(z^{(i)}=A)}\\\\ \\times [q^{\\mathbb{I}(x^{(i)}=H)}(1 - q)^{\\mathbb{I}(x^{(i)}=T)}]^{\\mathbb{I}(z^{(i)}=B)} \\end{aligned} \\end{equation} $$\nThe equation looks rather ugly, we can simplify this by encoding head as 1 and tail as 0; coin A as 1 and coin B as 0. The equation above can be written as\n$$ \\begin{equation} p(x^{(i)} | z^{(i)}) = [p^{x^{(i)}}(1-p)^{1 - x^{(i)}}]^{z^{(i)}} [q^{x^{(i)}}(1-q)^{1 - x^{(i)}}]^{1-z^{(i)}} \\end{equation} $$\nSimilarly for \\(p(z^{(i)})\\) $$ \\begin{equation} p(z^{(i)}) = \\tau^{z^{(i)}}(1-\\tau)^{1-z^{(i)}} \\end{equation} $$\nApplying EM algorithm\nThe (E step):\nConstruct the joint likelihood of a single pair of observation and latent variable \\(p(x^{(i)}, z^{(i)})\\ | \\boldsymbol{\\theta})\\). For the conciseness, we drop the \\((i)\\) superscript from the equation. $$ \\begin{equation} \\begin{aligned} p(x, z | \\boldsymbol{\\theta}) = \u0026amp; p(x | z, \\boldsymbol{\\theta})p(z | \\boldsymbol{\\theta})\\\\ = \u0026amp; [p^{x}(1-p)^{1 - x}]^{z} [q^{x}(1-q)^{1 - x}]^{1-z} \\tau^{z}(1-\\tau)^{1-z} \u0026amp; \\text{\\tiny(from eq. 5 and 6)} \\end{aligned} \\end{equation} $$\nLikelihood over entire observations \\(\\boldsymbol{X}\\) and latent \\(\\boldsymbol{Z}\\):\n$$\\boldsymbol{X}\\odot\\boldsymbol{Z} := \\{(x^{(i)}, z^{(i)})\\}_{i=1\\cdots N}$$\nA side note is that I am not entirely sure that \\(\\odot\\) operator is appropriate in this situation.\n$$ \\begin{equation} \\begin{aligned} P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) =\u0026amp; \\prod_{(x, z) \\in \\boldsymbol{X}\\odot\\boldsymbol{Z}} { p(x, z | \\boldsymbol{\\theta}) } \\end{aligned} \\end{equation} $$\nLog likelihood of the joint probability\n$$ \\begin{equation} \\begin{aligned} \\log P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) \u0026amp; = \\sum_{(x, z)} \\log p(x, z | \\boldsymbol{\\theta}) \\end{aligned} \\end{equation} $$\nTaking a log always seem to make thing to be better.\nFinally, we need to take the expectation of the log likelihood w.r.t conditional probability of \\(\\boldsymbol{Z}|\\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)}\\)\nPosterior for a single latent \\(z\\)\n$$ \\begin{equation} \\begin{aligned} p(z | x, \\boldsymbol{\\theta}^{(t)}) \u0026amp; = \\frac{p(x, z | \\boldsymbol{\\theta}^{(t)})} {p(x | \\boldsymbol{\\theta}^{(t)})} \u0026amp; \\text{\\tiny(Bayes Theorem)}\\\\ \u0026amp; = \\frac{p(x, z | \\boldsymbol{\\theta}^{(t)})} { p(x, z = 0| \\boldsymbol{\\theta}^{(t)}) + p(x, z = 1| \\boldsymbol{\\theta}^{(t)}) } \u0026amp; \\text{\\tiny(Marginal likelihood over z in denominator)}\\\\ \u0026amp; = \\frac{ [p_t^{x}(1-p_t)^{1 - x}]^{z} [q_t^{x}(1-q_t)^{1 - x}]^{1-z} \\tau_t^{z}(1-\\tau_t)^{1-z} }{ q_t^{x}(1-q_t)^{1 - x} (1-\\tau_t) + p_t^{x}(1-p_t)^{1 - x}\\tau_t } \u0026amp; \\text{\\tiny(from eq. 7)} \\end{aligned} \\end{equation} $$\nTaking the expectation\n$$ \\begin{equation} \\begin{aligned} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) \u0026amp;= \\mathbb{E}_{\\boldsymbol{Z} | \\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)}}{\\bigg[ \\sum_{(x, z)}{\\log p(x, z | \\boldsymbol{\\theta})} \\bigg]} \\\\ \u0026amp;= \\sum_{(x, z)} { \\mathbb{E}_{\\boldsymbol{Z} | \\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)}}{[ \\log p(x, z | \\boldsymbol{\\theta}) ]} } \\\\ \u0026amp;= \\sum_{(x, z)} { \\mathbb{E}_{z | x, \\boldsymbol{\\theta}^{(t)}}{[ \\log p(x, z | \\boldsymbol{\\theta}) ]} } \\\\ \\end{aligned} \\end{equation} $$\nIt is always bothering for me that in literature, the posterior, of which to be taken expectation over, for the entire set latent variables \\(\\boldsymbol{Z} = \\{ z^{(1)}, \\cdots z^{(n)}\\}\\) can be replaced by the posterior for a single latent \\(z\\) in (eq. 11) without explanation. So in order to understand this, consider the equation.\n$$ \\begin{aligned} \\mathbb{E}_{\\boldsymbol{Z}}{\\bigg[\\sum_{z\\in \\boldsymbol{Z}}{f(z)}\\bigg]} \u0026amp;= \\int_{\\boldsymbol{Z}}{ \\bigg[\\sum_{z\\in\\boldsymbol{Z}} f(z)\\bigg] p(\\boldsymbol{Z}) d\\boldsymbol{Z} } \\\\ \u0026amp; = \\sum_{z\\in\\boldsymbol{Z}}{\\int_{\\boldsymbol{Z}}{f(z)}} p(\\boldsymbol{Z})d\\boldsymbol{Z} \\\\ \u0026amp; = \\sum_{z\\in\\boldsymbol{Z}}{ \\int_{\\boldsymbol{Z}\\text{/}z} \\underbrace{\\bigg[\\int_{z}f(z)p(z)dz\\bigg]}_{A=\\mathbb{E}_z[f(z)]} p(\\boldsymbol{Z}\\text{/}z)d(\\boldsymbol{Z}/z) } \\\\ \u0026amp; = \\sum_{z\\in\\boldsymbol{Z}} A \\int_{\\boldsymbol{Z}\\text{/}z} p(\\boldsymbol{Z}\\text{/}z)d(\\boldsymbol{Z}/z) \u0026amp; \\text{\\tiny(A is constant w.r.t variable being integrated over)} \\\\ \u0026amp; = \\sum_{z\\in\\boldsymbol{Z}} \\mathbb{E}_z[f(z)] \u0026amp; \\text{\\tiny(Integerating over a p.d.f evalulated to 1)} \\end{aligned} $$\nWhere \\(\\boldsymbol{Z} = \\{z^i\\}_{i=1\\cdots N}; z \\sim p(Z)\\); \\(\\boldsymbol{Z}/z\\) denotes set all variables within \\(\\boldsymbol{Z}\\) except \\(z\\).\nHaving clear that up, we are able to resume from (eq. 11) $$ \\begin{equation} \\begin{aligned} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) \u0026amp;= \\sum_{(x, z)} { \\mathbb{E}_{z | x, \\boldsymbol{\\theta}^{(t)}}{[ \\log p(x, z | \\boldsymbol{\\theta}) ]} } \\\\ \u0026amp;= \\sum_{(x, z)} {\\bigg[ p(z = 0 | x, \\boldsymbol{\\theta}^{(t)}) \\log p(x, z = 0 | \\boldsymbol{\\theta}) \\ + p(z = 1 | x, \\boldsymbol{\\theta}^{(t)}) \\log p(x, z = 1 | \\boldsymbol{\\theta}) \\bigg]} \\end{aligned} \\end{equation} $$\nFrom (eq. 7)\n$$ \\begin{aligned} p(x, z = 0 |\\boldsymbol{\\theta}) = q^x(1-q)^{1-x}(1-\\tau) \\end{aligned} $$ $$ \\begin{aligned} p(x, z = 1 |\\boldsymbol{\\theta}) = p^x(1-p)^{1-x}\\tau \\end{aligned} $$ From (eq. 10)\n$$ \\begin{aligned} p(z = 0 | x, \\boldsymbol{\\theta}^{(t)}) \u0026amp; = \\frac{ q_t^{x}(1-q_t)^{1-x}(1-\\tau_t) }{ q_t^{x}(1-q_t)^{1 - x} (1-\\tau_t) + p_t^{x}(1-p_t)^{1 - x}\\tau_t } \\end{aligned} $$\n$$ \\begin{aligned} p(z = 1 | x, \\boldsymbol{\\theta}^{(t)}) \u0026amp; = \\frac{ p_t^{x}(1-p_t)^{1-x}\\tau_t }{ q_t^{x}(1-q_t)^{1 - x} (1-\\tau_t) + p_t^{x}(1-p_t)^{1 - x}\\tau_t } \\end{aligned} $$\nThis probability is often referred as membership probability, denote the membership probability of the \\(i^{th}\\) observation \\(p(z^{(i)} = 0 | x^{(i)}, \\boldsymbol{\\theta}^{(t)}) = a_i\\) and \\(p(z^{(i)} = 1|x^{(i)},\\boldsymbol{\\theta}^{(t)}) = 1 - a_i\\). With this\nSubstitute these quantities into eq. 12 we have the expectation of the log-likelihood w.r.t conditional probability of \\(\\boldsymbol{Z}\\) given observations and current state of the parameters.\n$$ \\begin{aligned} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) = \\sum_{i=1}^{N}{ a_i [x^{(i)}\\log q + (1-x^{(i)})\\log(1-q) + \\log(1-\\tau)] } \\\\ + (1 - a_i)[x^{(i)} \\log p + (1-x^{(i)})\\log (1-p) + log\\tau] \\end{aligned} $$\nThe M step $$ \\boldsymbol{\\theta}^{(t+1)} = \\arg\\max_{\\boldsymbol{\\theta}}{Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)})} $$\n\\(\\frac{\\partial Q}{\\partial p} = 0 \\)\n$$ \\begin{aligned} \u0026amp; \\frac{\\partial Q}{\\partial p} = 0 \\\\ \\iff \u0026amp; \\sum_{i=1}^N{(1-a_i)[\\frac{ x^{(i)}}{p}} - \\frac{1-x^{(i)}}{1-p}] = 0 \\\\ \\iff \u0026amp; \\frac{1}{p} \\underbrace{\\sum_{i=1}^N{(1-a_i)x^{(i)}}}_{A} = \\frac{1}{1-p} \\underbrace{\\sum_{i=1}^N{(1-a_i)(1-x^{(i)})}}_{B} \\\\ \\implies \u0026amp; p = A/(A+B) \\\\ \u0026amp; = \\color{red}{\\frac{\\sum_{i=1}^N{(1-a_i)x^{(i)}}}{\\sum_{i=1}^N{(1-a_i)x^{(i)}}+\\sum_{i=1}^N{(1-a_i)(1-x^{(i)})}}} \\end{aligned} $$\n\\(\\frac{\\partial Q}{\\partial q} = 0 \\), same with \\(p\\)\n$$ \\begin{aligned} \\color{red}{ q = \\frac{\\sum_{i=1}^N{a_i x^{(i)}}}{\\sum_{i=1}^N{a_i x^{(i)}}+\\sum_{i=1}^N{a_i(1-x^{(i)})}} } \\end{aligned} $$\n\\(\\frac{\\partial Q}{\\partial \\tau} = 0 \\) $$ \\begin{aligned} \u0026amp; \\frac{\\partial Q}{\\partial \\tau} = 0 \\\\ \\iff \u0026amp; \\sum_{1}^N{\\frac{-a_i}{1-\\tau} + \\frac{1-a_i}{\\tau}} = 0\\\\ \\iff \u0026amp; \\frac{1}{1-\\tau}\\sum_{1}^N{a_i} = \\frac{1}{\\tau}\\sum_{i=1}^N{(1-a_i)}\\\\ \\implies \u0026amp; \\color{red}{\\tau = \\frac{\\sum_{i=1}^N{1-a_i}}{N}} \\end{aligned} $$\nEM for Gaussian Mixture Model EM for GMM\u0026rsquo;s python implementation ","permalink":"https://iamtu.dev/posts/em/","summary":"Problem Given a statistical model \\(P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})\\), which generate set of observations \\(\\boldsymbol{X}\\), where \\(\\boldsymbol{Z}\\) is a latent variable and unknow parameter vector \\(\\boldsymbol{\\theta}\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that maximize the marginal likelihood:\n$$ \\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{X}) = P(\\boldsymbol{X} | \\boldsymbol{\\theta}) = \\int_{\\boldsymbol{Z}}P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})d\\boldsymbol{Z} $$\nAs an example for this type of problem, there are two (unfair) coin A and B with probability of head for each coin is \\(p_A(H) = p \\text{ and } p_B(H) = q\\).","title":"Expectation Maximization - EM"},{"content":"This post is a note I take from while reading Blei et al 2018.\nGoal:\nMotivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \\(q(z)\\) to approximate \\(p(z|x)\\)\nThe KL-divergence\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026amp;= \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\end{aligned} \\end{equation} $$\nHowever, this quantity is intractable to compute hence, we\u0026rsquo;re unable to optimize this quantity directly.\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026amp;= - \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\\\ \u0026amp;= -\\int_z{ q(z) \\log { \\frac{\\log p(z, x)}{q(z) p(x)} } }\\\\ \u0026amp;= -\\int_z{q(z)[\\log{\\frac{p(z,x)}{q(z)}} - \\log p(x)]dz} \\\\ \u0026amp;= -\\int_z{ q(z) \\log \\frac{p(z, x)}{q(z)}dz } + \\int_z{q(z)\\log p(x) dz} \\\\ \u0026amp; =: -\\texttt{ELBO}[q] + \\log p(x) \\\\ \\iff \\texttt{ELBO}[q] \u0026amp;= -KL(q||p) + \\log p(x) \\end{aligned} \\end{equation} $$\nBecause \\(\\log p(x)\\) is a constant, by maximizing \\(\\text{ELBO}[q]\\), we minimize \\(KL(q||p)\\) by proxy. Rewrite ELBO:\n$$ \\begin{equation} \\begin{aligned} \\texttt{ELBO}(q) \u0026amp;= \\int_z{q(z)\\log \\frac{p(z, x)}{q(z)}} \\\\ \u0026amp;= \\mathbb{E}_{z\\sim q}[\\log p(z, x)] - \\mathbb{E}_{z\\sim q}[\\log q(z)] \\end{aligned} \\end{equation} $$\nMean field Variational Family Mean-field variational family made a strong assumption of independence between it\u0026rsquo;s latent variable\n$$ q(\\mathbf{z}) = \\prod_{j} {q_j(z_j)} $$\nCoordinate ascent variational inference is a common method to solve mean-field variational inference problem. Holding other latent variable fixed, the \\(j^{th}\\) latent variable is given by:\n$$ q^*_{j}(z_j) = \\text{exp}{\\mathbb{E}_{-j}[\\log p(z_j | z_{-j}, \\mathbf{x})]} \\propto \\exp{\\mathbb{E}_{-j} [\\log p(z_j, z_{-j}, \\mathbf{x})]} $$\nProof $$ \\begin{equation} \\begin{aligned} q^*_j(z_j) \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad{\\texttt{ELBO}(q)} \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_q[\\log p(z_j, z_{-j}, x)] - \\mathbb{E}_q[\\log q(z_j, z_{-j})] \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log q_j(z_j) + \\log q_{-j}(z_{-j})]] \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] + const \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] \\end{aligned} \\end{equation} $$\nWe need to find function \\(q_j(z_j)\\) that maximize \\(\\text{ELBO}(q)\\)\nAssuming \\(q_j(z_j)= \\epsilon \\eta(z_j) + q^*_j(z_j)\\)\n$$ \\begin{aligned} K(\\epsilon) \u0026amp;= \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] \\\\ \u0026amp;= \\int_{z_j} q_j(z_j) A d_{z_j} - \\int_{z_j}q_j(z_j)\\log q_z(z_j) d_{z_j} \\\\ \u0026amp;= \\int_{z_j} [\\epsilon \\eta(z_j) + q^*_j(z_j)] A d_{z_j} - \\int_{z_j}[\\epsilon \\eta(z_j) + q^*_j(z_j)] \\log [\\epsilon \\eta(z_j) + q^*_j(z_j)] d_{z_j} \\end{aligned} $$\nEvaluate the partial derivative of \\(K\\) wrt \\(\\epsilon\\) we have:\n$$ \\begin{aligned} \u0026amp; \\frac{\\partial}{\\partial \\epsilon}K \\bigg\\vert_{\\epsilon=0} = 0 \\\\ \\iff \u0026amp; \\int_{z_j} {\\eta(z_j) A d_{z_j}} - \\int_{z_j} { {\\eta(z_j) \\log [\\epsilon \\eta(z_j) + q^*_j(z_j)]} + [\\epsilon \\eta(z_j) + q^*_j(z_j)] \\frac{\\eta(z_j)}{\\epsilon \\eta(z_j) + q^*_j(z_j)}d_{z_j} } = 0\\\\ \\iff \u0026amp; \\int_{z_j} {\\eta(z_j) A d_{z_j}} - \\int_{z_j}{[\\eta(z_j)\\log q^*_j(z_j) +\\eta(z_j)]d_{z_j}} = 0; \\quad \\forall \\eta(z_j) \\\\ \\iff \u0026amp; \\log q^*_j(z_j) = A-1 = \\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)] - 1 \\\\ \\iff \u0026amp; q^*_j(z_j) \\propto \\exp{\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]} \\end{aligned} $$\nComplete example of Bayesian Gaussian Mixture TDB\n","permalink":"https://iamtu.dev/posts/variational_inference/","summary":"This post is a note I take from while reading Blei et al 2018.\nGoal:\nMotivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \\(q(z)\\) to approximate \\(p(z|x)\\)\nThe KL-divergence\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026amp;= \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\end{aligned} \\end{equation} $$\nHowever, this quantity is intractable to compute hence, we\u0026rsquo;re unable to optimize this quantity directly.","title":"Understanding Variational Inference"},{"content":"Motivating example Evaluating following integral\n$$ I = \\int_0^1{\\frac{1 - x^2}{\\ln{x}}dx} $$\nClosed-form results $$ \\begin{equation} \\begin{aligned} F(t) \u0026amp;= \\int_0^1{\\frac{1-x^t}{\\ln(x)}dx} \\\\ \\implies \\frac{d}{dt}F \u0026amp;= \\frac{d}{dt}\\int_0^1{\\frac{1-x^t}{\\ln(x)}dx}\\\\ \u0026amp;= \\int_0^1{ \\frac{\\partial}{\\partial t} \\frac{1-x^t}{\\ln(x)}dx }\\\\ \u0026amp;= \\int_0^1{ \\frac{-\\ln(x)x^t}{ln(x)} dx} \\\\ \u0026amp;= \\bigg[-\\frac{x^{t+1}}{t+1}\\bigg]_0^1\\\\ \u0026amp;= -\\frac{1}{t+1}\\\\ \\implies F(t) \u0026amp;= -\\ln({t+1}) \\\\ \\implies I \u0026amp;= f(2) = -\\ln3 \\end{aligned} \\end{equation} $$\nNumerical approximation Code to produce the figure 1 2 3 4 5 6 7 8 import numpy as np from matplotlib import pyplot as plt def I(): g = lambda x: (1 - x**2)/np.log(x) vG = np.vectorize(g) x = np.random.uniform(0, 1, 10000) return vG(x).mean() ","permalink":"https://iamtu.dev/posts/diff-under-integral-sign/","summary":"Motivating example Evaluating following integral\n$$ I = \\int_0^1{\\frac{1 - x^2}{\\ln{x}}dx} $$\nClosed-form results $$ \\begin{equation} \\begin{aligned} F(t) \u0026amp;= \\int_0^1{\\frac{1-x^t}{\\ln(x)}dx} \\\\ \\implies \\frac{d}{dt}F \u0026amp;= \\frac{d}{dt}\\int_0^1{\\frac{1-x^t}{\\ln(x)}dx}\\\\ \u0026amp;= \\int_0^1{ \\frac{\\partial}{\\partial t} \\frac{1-x^t}{\\ln(x)}dx }\\\\ \u0026amp;= \\int_0^1{ \\frac{-\\ln(x)x^t}{ln(x)} dx} \\\\ \u0026amp;= \\bigg[-\\frac{x^{t+1}}{t+1}\\bigg]_0^1\\\\ \u0026amp;= -\\frac{1}{t+1}\\\\ \\implies F(t) \u0026amp;= -\\ln({t+1}) \\\\ \\implies I \u0026amp;= f(2) = -\\ln3 \\end{aligned} \\end{equation} $$\nNumerical approximation Code to produce the figure 1 2 3 4 5 6 7 8 import numpy as np from matplotlib import pyplot as plt def I(): g = lambda x: (1 - x**2)/np.","title":"Differentiation under integral sign"},{"content":"The closed form of KL divergence used in Variational Auto Encoder.\nUnivariate case Let\n\\(p(x) = \\mathcal{N}(\\mu_1, \\sigma_1) = (2\\pi\\sigma_1^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2]\\) \\(q(x) = \\mathcal{N}(\\mu_1, \\sigma_2) = (2\\pi\\sigma_2^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2]\\) KL divergence between \\(p\\) and \\(q\\) is defined as:\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= -\\int_{x}{p(x)\\log{\\frac{q(x)}{p(x)}}dx} \\\\ \u0026amp;= -\\int_x p(x) [\\log{q(x)} - \\log{p(x)}]dx \\\\ \u0026amp;= \\underbrace{ \\int_x{p(x)\\log p(x) dx}}_A - \\underbrace{ \\int_x{p(x)\\log q(x) dx}}_B \\end{aligned} $$\nFirst quantity \\(A\\):\n$$ \\begin{aligned} A \u0026amp;= \\int_x{p(x)\\log p(x) dx} \\\\ \u0026amp;= \\int_x{p(x)\\big[ -\\frac{1}{2}\\log{2\\pi\\sigma_1^2 - \\frac{1}{2\\sigma_1^2}(x - \\mu_1)^2} \\big]dx}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi\\sigma_1^2}\\int_x{p(x)dx} - \\frac{1}{2\\sigma_1^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)^2dx}}_{\\text{var(x)}}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi} - \\log\\sigma_1-\\frac{1}{2} \\end{aligned} $$\nThe second quantity \\(B\\):\n$$ \\begin{aligned} B =\u0026amp; \\int_x{p(x)\\big[ -\\frac{1}{2}\\log2\\pi\\sigma_2^2 - \\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2 \\big]dx}\\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi\\sigma_2^2 - \\frac{1}{2\\sigma_2^2}\\int_x{ p(x)\\big[ (x - \\mu_1)^2 + 2(x-\\mu_1)(\\mu_1 - \\mu_2) + (\\mu_1 -\\mu_2)^2 \\big]dx} \\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi\\sigma_2^2 \\\\ \u0026amp; - \\frac{1}{2\\sigma_2^2}\\underbrace{\\int_x{p(x)(x-\\mu_1)^2}}_{\\text{var}(x)}\\\\ \u0026amp; - \\frac{2(\\mu_1 -\\mu_2)}{2\\sigma_2^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)dx}}_0 \\\\ \u0026amp; - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} \\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi -\\log\\sigma_2 - \\frac{\\sigma_1^2}{2\\sigma_2^2} - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} \\end{aligned} $$\nFinally, we obtained the KL divergence for univariate case\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= A - B \\\\ \u0026amp;= (-\\frac{1}{2}\\log2\\pi - \\log\\sigma_1 - \\frac{1}{2}) - ( -\\frac{1}{2}\\log2\\pi -\\log\\sigma_2 - \\frac{\\sigma_1^2}{2\\sigma_2^2} - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}) \\\\ \u0026amp;= \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2}{2\\sigma_2^2} + \\frac{(\\mu_1 -\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2} \\end{aligned} $$\nMultivariate case tbd\nReference https://gregorygundersen.com/blog/ ","permalink":"https://iamtu.dev/posts/closed-form-kl-gaussian/","summary":"The closed form of KL divergence used in Variational Auto Encoder.\nUnivariate case Let\n\\(p(x) = \\mathcal{N}(\\mu_1, \\sigma_1) = (2\\pi\\sigma_1^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2]\\) \\(q(x) = \\mathcal{N}(\\mu_1, \\sigma_2) = (2\\pi\\sigma_2^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2]\\) KL divergence between \\(p\\) and \\(q\\) is defined as:\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= -\\int_{x}{p(x)\\log{\\frac{q(x)}{p(x)}}dx} \\\\ \u0026amp;= -\\int_x p(x) [\\log{q(x)} - \\log{p(x)}]dx \\\\ \u0026amp;= \\underbrace{ \\int_x{p(x)\\log p(x) dx}}_A - \\underbrace{ \\int_x{p(x)\\log q(x) dx}}_B \\end{aligned} $$\nFirst quantity \\(A\\):\n$$ \\begin{aligned} A \u0026amp;= \\int_x{p(x)\\log p(x) dx} \\\\ \u0026amp;= \\int_x{p(x)\\big[ -\\frac{1}{2}\\log{2\\pi\\sigma_1^2 - \\frac{1}{2\\sigma_1^2}(x - \\mu_1)^2} \\big]dx}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi\\sigma_1^2}\\int_x{p(x)dx} - \\frac{1}{2\\sigma_1^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)^2dx}}_{\\text{var(x)}}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi} - \\log\\sigma_1-\\frac{1}{2} \\end{aligned} $$","title":"Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution"},{"content":"Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper. In the analogy above, the black box represents the simulator, and the configurations are the simulator’s parameters.\nThe applicability of SBI has great potential since we can almost reduce any process with defined input and output to a black-box machine 1.\nThis post documents my notes while studying Likelihood-free MCMC with Amortized Ratio Estimator (Hermans et al, 2020); a method developed to address SBI.\nLikelihood-free MCMC with Amortized Ratio Estimator Likelihood ratio is defined as the ratio between the likelihood of the observation between two different hypothesis:\n$$ r(\\mathbf{x} | \\theta_0, \\theta_1) = \\frac{p(\\mathbf{x} | \\theta_0)}{p(\\mathbf{x}|\\theta_1)} $$\nThis quantity then can be used in various methods to draw sample from a distribution. In the paper, the author mention three sampling methods, namely Markov Chain Monte Carlo, Metropolis-Hasting, and HMC. In the following section, I am briefly summarizing those methods.\nBackground Markov Chain Monte Carlo (MCMC) In statistics, the MCMC method is a class of algorithms for sampling from a probability distribution. By constructing a Markov chain with the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the state chain 2.\nAdapting MCMC for SBI task We want to sample from \\(p(\\theta | \\mathbf{x})\\) using MCMC, we need this quantity\n$$ \\begin{equation} \\begin{aligned} \\frac{p(\\theta | \\mathbf{x} )}{p(\\theta_t| \\mathbf{x})} = \\frac{ p(\\theta)p(\\mathbf{x} | \\theta)/p(\\mathbf{x}) }{ p(\\theta_t)p(\\mathbf{x} | \\theta_t)/p(\\mathbf{x}) } = \\frac{p(\\theta)}{p(\\theta_t)}\\times \\frac{p(\\mathbf{x} | \\theta)}{p(\\mathbf{x} | \\theta_t)} = \\frac{p(\\theta)}{p(\\theta_t)} \\times r(\\mathbf{x} | \\theta, \\theta_t) \\end{aligned} \\end{equation} $$\nWe can compute the first term of the equation since we have access to prior \\(p(\\theta)\\). But we can not compute the second term because we do not have access to the likelihood function \\(p(\\mathbf{x} | \\theta)\\). However, we can reframe the problem in the supervised-learning paradigm, so we can use a parameterized discriminator \\(d_\\theta(\\mathbf{x})\\) to estimate the likelihood. The details are described in the Likelihood Ratio Estimator section.\nMetropolis-Hasting (MH) tbd\nHalmitonian Markov Chain(MH) tbd\nLikelihood Ratio Estimator The remaining question is how to estimate the likelihood ratio \\( r(\\mathbf{x} | \\theta_0, \\theta_1)\\). To estimate the ratio, the author employed the Likelihood Ratio Trick, training a discriminator \\(d_\\phi(\\mathbf{x})\\) to classify samples \\( x \\sim p(\\mathbf{x} | \\theta_0)\\) with class label \\(y = 1\\) from \\(\\mathbf{x} \\sim p(\\mathbf{x} | \\theta_1)\\) with class label \\(y = 0\\). The decision function obtained by the trained discrimininator:\n$$ d^*(\\mathbf{x}) = p(y = 1 | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\theta_0)}{p(\\mathbf{x} | \\theta_0) + p(\\mathbf{x} | \\theta_1)} $$\nThen the estimation of likelihood ratio can be computed by:\n$$ \\hat{r}(\\mathbf{x} | \\theta_0, \\theta_1) = \\frac {d^{*}(\\mathbf{x})} {1 - d^{*}(\\mathbf{x})} $$\nHowever, this method required the discriminator to be trained at every pair of \\((\\theta_0, \\theta_1)\\), which is impractical in the context. To overcome this issue, the paper proposed to train the discriminator to classify dependent sample-parameter pairs \\((\\mathbf{x}, \\mathbf{\\theta}) \\sim p(\\mathbf{x}, \\mathbf{\\theta})\\) with label \\(y=1\\) from the independent sample-parameter pairs \\((\\mathbf{x}, \\mathbf{\\theta}) \\sim p(\\mathbf{x})p(\\mathbf{\\theta})\\) with label \\(y=0\\).\n$$ \\begin{equation} \\begin{aligned} d^*(\\mathbf{x}, \\mathbf{\\theta}) \u0026amp;= \\frac {p(\\mathbf{x}, \\mathbf{\\theta})} { p(\\mathbf{x}, \\mathbf{\\theta}) + p(\\mathbf{x}) p(\\mathbf{\\theta}) } \\ \\end{aligned} \\end{equation} $$\nThe likelihood-to-evidence ratio is computed by\n$$ r(\\mathbf{x} | \\theta) = \\frac {p(\\mathbf{x} | \\theta)} {p(x)} = \\frac{p(x, \\theta)}{p(x)p(\\theta)} = \\frac {d^{*}(x, \\theta)} {1 - d^{*}(x, \\theta)} $$\nThen the likelihood ratio for any two hypotheses can be estimated at any point by\n$$ r(x | \\theta_0, \\theta_1) = \\frac{d^{*}(x,\\theta_0)}{d^{*}(x, \\theta_1)} $$\nToy example Setup:\nThe simulator: a function takes 1 parameter \\(\\mu\\), and return a random variable drawn from \\(\\mathcal{N}(\\mu, 1)\\) The observations \\(\\mathbf{x}\\): Observation drawn from the simulator with \\(\\mu = 2.5\\), which is unknown to the algorithm. The discriminator: A fully connected neural network. The prior of the parameters: \\(\\mathcal{N}(0, 1)\\) We want to draw samples from the posterior distribution \\(p(\\theta | \\mathbf{x})\\), where \\(x \\sim \\mathcal{N}(2.5, 1)\\).\nImplementation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 import click import numpy as np import torch from matplotlib import pyplot as plt from torch import nn from torch.nn import functional as F from typing import NamedTuple np.random.seed(1) torch.manual_seed(1) def stochastic(func): def __wrapper__(*args, **kwargs): np.random.seed() rs = func(*args, **kwargs) np.random.seed(1) return rs return __wrapper__ class Layer(NamedTuple): h: int # hidden dim a: str # activation def Dense(h_i: int, h_j: int, a : str): if a == \u0026#34;tanh\u0026#34;: act = nn.Tanh() elif a == \u0026#34;sigmoid\u0026#34;: act = nn.Sigmoid() elif a == \u0026#34;relu\u0026#34;: act = nn.ReLU() else: raise NotImplementedError(a) return nn.Sequential( nn.Linear(h_i, h_j), act) def build_mlp(input_dim: int, seq: list[Layer]) -\u0026gt; nn.Module: h0, a0 = seq[0] _seq = [Dense(input_dim, h0, a0)] for j in range(1, len(seq)): h_j, a_j = seq[j] h_i, _ = seq[j - 1] _seq.append(Dense(h_i, h_j, a_j)) return nn.Sequential(*_seq) def train_step( Xpos: torch.Tensor, Xneg: torch.Tensor, d: nn.Module, opt: torch.optim.Optimizer) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Args: - Xpos: (x, theta) - Xneg: (x, theta\u0026#39;) - d: classifier Where theta/theta\u0026#39; ~ p, x ~ p(x | theta) \u0026#34;\u0026#34;\u0026#34; for i in range(32): opt.zero_grad() zpos = d(Xpos) zneg = d(Xneg) loss = F.binary_cross_entropy(zpos, torch.ones_like(zpos))\\ + F.binary_cross_entropy(zneg, torch.zeros_like(zneg)) loss.backward() opt.step() return loss.item() def train_d( p: callable, sim: callable, d: nn.Module, m: int, e: int, lr: float): \u0026#34;\u0026#34;\u0026#34; Args: - p : prior - sim: simulator (implicit p(x | theta) - d: parameterized classifier - m: batch_size - e: max epochs - lr: learning rate \u0026#34;\u0026#34;\u0026#34; opt = torch.optim.Adam(d.parameters(), lr=lr) sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt) losses = [] for b in range(e): theta = p(m) theta_prime = p(m) x = sim(theta) # expand dims everything theta = np.expand_dims(theta, -1) theta_prime = np.expand_dims(theta_prime, -1) x = np.expand_dims(x, -1) # construct training sample Xpos = np.concatenate([x, theta], -1) Xneg = np.concatenate([x, theta_prime], -1) Xpos, Xneg = torch.tensor(Xpos, dtype=torch.float),\\ torch.tensor(Xneg, dtype=torch.float) loss = train_step(Xpos, Xneg, d, opt) losses.append(loss) if b%50 == 49: sch.step(loss) return d, losses @stochastic def mcmc(lp: callable, obs: np.ndarray, d: nn.Module, n_samples: int, step_size: float): \u0026#34;\u0026#34;\u0026#34; Amortized MCMC likelihood free \u0026#34;\u0026#34;\u0026#34; # proposal distribution: q = lambda theta: np.random.normal(theta, step_size) # initialize theta theta = 0. samples = [] obs = np.expand_dims(obs, -1) for i in range(n_samples): theta_prime = q(theta) mu_theta = np.ones_like(obs) * theta mu_theta_prime = np.ones_like(obs) * theta_prime # construct input vector X = np.concatenate([obs, mu_theta], -1) Xp = np.concatenate([obs, mu_theta_prime], -1) X, Xp= torch.tensor(X, dtype=torch.float),\\ torch.tensor(Xp, dtype=torch.float) # Compute the decision function d_theta = d(X).detach().mean().numpy() d_theta_prime = d(Xp).detach().mean().numpy() r_theta = d_theta / (1 - d_theta) r_theta_prime = d_theta_prime / (1- d_theta_prime) H = r_theta_prime / r_theta H = lp(theta_prime) / lp(theta) * H H = 1 if H \u0026gt; 1 else H u = np.random.uniform() if u \u0026lt; H: # accept theta_prime samples.append(theta_prime) theta = theta_prime return samples def main( batch_size: int, max_iter: int, lr: float, n_obs: int, n_samples: int, step_size: float): # PROBLEM SETUP # -------------------------------------------------- # prior theta p = lambda m: np.random.normal(0, 1, size=m) lp = lambda x: np.exp(-0.5 * x**2)#likelihood function # simulator: unknown sim = lambda mu: np.random.normal(mu, np.ones_like(mu) * .25) # parmeterized classifier d = build_mlp( 2, [Layer(4, \u0026#39;relu\u0026#39;), Layer(2, \u0026#39;relu\u0026#39;), Layer(1, \u0026#39;sigmoid\u0026#39;)]) # TRAINING the classifier # -------------------------------------------------- d, losses = train_d(p, sim, d, m=batch_size, e=max_iter, lr=lr) # inference # -------------------------------------------------- MU = 2.5 #unknown obs = sim(np.ones(n_obs) * MU) # Posterior sample: sample p(theta | obs) samples = mcmc(lp, obs, d, n_samples, step_size) Result References The frontier of simulation-based inference Likelihood-free MCMC with Amortized Ratio Estimator I am a black-box machine, you are a black-box machine, everyone is a black-box machine as long as we don\u0026rsquo;t care enough about the person.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nShamelessly copied from Wikipedia.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://iamtu.dev/posts/sbi/","summary":"Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper.","title":"Likelihood-free MCMC with Amortized Ratio Estimator"},{"content":"TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \\(T\\) observations \\((x_1, \u0026hellip; x_T)\\) drawn from a true distribution \\(p_d(.)\\). We then try to approximate \\(p_d\\) by a parameterized function \\(p_m(.;\\theta)\\). The estimator \\(\\hat{\\theta}_T\\) is defined to be the \\(\\theta\\) that maximize function\n$$ J_T(\\theta) = \\frac{1}{2T}\\sum_t{\\log[h(x_t; 0)]} + \\log[1-h(y_t; \\theta)] $$\nIn which:\n\\(y=(y_1, \u0026hellip;, y_T)\\) be a dataset of \\(T\\) observations draw from a noise density function $p_n(.)$. \\(h(u; \\theta) = 1/(1 + e^{-G(u;\\theta)})\\) \\(G(u; \\theta) = \\log p_m(u; \\theta) - \\log p_n(u)\\) For \\(p_m(.; \\theta)\\) to be a valid p.d.f, we also need to include unit integral constraint into the optimization problem, that is \\(\\int_x{p_m(x; \\theta)dx} = 1\\). However, this integral is often intractable in most cases, for example, when we use a neural network to parameterize \\(p_m(.;\\theta)\\).\nInterestingly, the paper claims that maximizing the objective function gives a valid p.d.f without placing the unit integral constraint on the optimization (Theorem 1). In this post, I\u0026rsquo;ll attempt to prove the theorem as an exercise. Note that, I made an assumption that support of $x$ and $y$ are equal (in eq.7); which mean $p_n(.)$ is nonzero whenever $p_d(.)$ is nonzero and $p_n(.)$ is zero everywhere else.\nProof of threorem I When the sample size $T$ becomes arbitrarily large, the objective function $J_T(\\theta)$ converges in probability (this is a new word for me) to $\\tilde{J}$\n\\begin{equation} \\begin{aligned} \\tilde{J}(\\theta) = \\frac{1}{2}\\mathbb{E}_{x, y} { \\log{r\\big(f(x) - \\log{p_n(x)}\\big)} + \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} } \\end{aligned} \\end{equation}\nIn which \\(f(x) = \\log p_m(x; \\theta)\\) is the function approximating log-likelihood the true distribution $p_d(.)$.\nNotation \\(p_d(x)\\) true probability density function (p.d.f) of data. \\(p_n(x)\\) p.d.f of noise generating distribution. \\(r(x) = \\frac{1}{1+\\exp(-x)}\\) sigmoid function. \\(X = (x_1, \u0026hellip; x_T); x \\sim p_d(x)\\) be the dataset of T observations. \\(Y = (y_1, \u0026hellip; y_T); y \\sim p_n(y)\\) be the dataset of T artificially generated noise. \\(p_m(.; \\theta)\\) is estimation of \\(p_d(.)\\) parameterized by \\(\\theta\\). Theorem \\(\\tilde{J}\\) attains a maximum at \\(f(.) = \\log p_d(.)\\). There are no other extrema if the noise density \\(p_n(.)\\) is chosen such it is nonzero whenever \\(p_d(.)\\) is nonzero.\nProof Let \\(\\hat{f}(x)\\) be the optimal function that maximizes \\(\\tilde{J}\\), and \\(f(x)=\\hat{f}(x) + \\epsilon\\eta(x)\\).\n\\begin{equation} \\begin{aligned} \\tilde{J}(\\theta) \u0026amp;= K(\\epsilon) \\\\ \u0026amp;= \\frac{1}{2}\\mathbb{E}_{x, y} { \\log{ r\\big(f(x) - \\log{p_n(x)}\\big) } + \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} } \\\\ \u0026amp;= \\frac{1}{2}\\underbrace{ \\mathbb{E}_x { \\log r\\big( f(x) - \\log p_n(x) \\big) } }_A + \\frac{1}{2} \\underbrace{\\mathbb{E}_y { \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} }}_B \\\\ \\implies \\frac{dK}{d\\epsilon} \u0026amp;= \\frac{dA}{d\\epsilon} + \\frac{dB}{d\\epsilon} \\end{aligned} \\end{equation}\nExpand the first term of $K(\\epsilon)$\n\\begin{equation} \\begin{aligned} A(\\epsilon) \u0026amp;= \\mathbb{E}_x { \\log r\\big( f(x) - \\log p_n(x) \\big) } \\\\ \u0026amp; = \\int_x { p_d(x) \\log{ r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) \\big) } dx } \\end{aligned} \\end{equation}\nTaking derivative of $A(\\epsilon)$\n\\begin{equation} \\begin{aligned} \\frac{dA}{d\\epsilon} \u0026amp;= \\frac{1}{d\\epsilon} \\int_x { p_d(x) \\log{ r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) \\big) } dx } \\\\ \u0026amp; = \\int_x { p_d(x) \\big[ \\frac{1}{d\\epsilon}\\log{ r \\big( \\underbrace{ \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) }_{g(\\epsilon)} \\big) } \\big]dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\frac{d\\log{r}}{dr} \\frac{dr}{dg} \\frac{dg}{d\\epsilon} dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\frac{1}{r} r(1-r) \\eta(x) dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\big[1 - r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x)\\big) \\big] \\eta(x) dx } \\end{aligned} \\end{equation}\nNow let\u0026rsquo;s turn our attention to the second term of $K(\\epsilon)$\n\\begin{equation} \\begin{aligned} B(\\epsilon) \u0026amp;= \\mathbb{E}_y { \\log\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big] } \\\\ \u0026amp; = \\int_y { p_n(y) \\log \\big[ 1 - r \\big( \\underbrace{ \\hat{f}(y) + \\epsilon \\eta(y) - \\log p_n(y) }_h \\big) \\big]dy } \\end{aligned} \\end{equation}\nTaking derivative of $B$ w.r.t $\\epsilon$\n\\begin{equation} \\begin{aligned} \\frac{dB}{d\\epsilon} \u0026amp;= \\frac{1}{d\\epsilon} \\int_y{ p_n(y)\\log{ \\big[ 1 - r\\big( h(\\epsilon)\\big) \\big] }dy } \\\\ \u0026amp;= \\int_y { p_n(y) \\frac{d\\log(1-r)}{d(1-r)} \\frac{d(1-r)}{dr} \\frac{dr}{dh} \\frac{dh}{d\\epsilon} dy } \\\\ \u0026amp; = \\int_y { p_n(y) \\frac{1}{1-r} (-1) r(1-r) \\eta(y) } \\\\ \u0026amp; = -\\int_y{ p_n(y) r\\big( \\hat{f}(y) + \\epsilon \\eta(y) - \\log p_n(y) \\big) \\eta(y) dy } \\end{aligned} \\end{equation}\nSubstitute result from eq(4) and eq(6) to eq(2), $\\frac{dK}{d\\epsilon}$ is evaluated to $0$ at $\\epsilon = 0$.\n\\begin{equation} \\begin{aligned} \\frac{dK}{d\\epsilon}\\big\\vert_{\\epsilon=0} \u0026amp;= \\frac{dA}{d\\epsilon}\\big\\vert_{\\epsilon=0} + \\frac{dB}{d\\epsilon}\\big\\vert_{\\epsilon=0} \\\\ \u0026amp;= \\int_x { p_d(x) \\big[1 - r\\big( \\hat{f}(x) - \\log p_n(x)\\big) \\big] \\eta(x) dx } \\\\ \u0026amp; - \\int_y{ p_n(y) r \\big( \\hat{f}(y) - \\log p_n(y) \\big) \\eta(y) dy } \\\\ \u0026amp; = 0 \\end{aligned} \\end{equation}\nConsider eq. (7), if the support for $x$ and $y$ are equal, which mean we integrate $x$ and $y$ over a same region, we can change $y$ to $x$ and rewrite eq.(7) as\n\\begin{equation} \\begin{aligned} \\frac{dK}{d\\epsilon} \\big\\vert_{\\epsilon = 0} \u0026amp;= \\int_x { \\underbrace{ p_d(x) \\big[1 - r\\big( \\hat{f}(x) - \\log p_n(x)\\big) \\big] }_C \\eta(x) dx } \\\\ \u0026amp; - \\int_x{ \\underbrace{ p_n(x) r \\big( \\hat{f}(x) - \\log p_n(x) \\big) }_D \\eta(x) dx } \\\\ \u0026amp; = \\int_x{(C-D)\\eta(x)dx} = 0 \\quad \\forall \\eta(x) \\end{aligned} \\end{equation}\nThe equality in eq.(8) happend if and only if \\(C=D\\). This result easily leads to \\(\\hat{f}(x) = \\log p_d(x)\\).\nReferences Noise-contrastive estimation: A new estimation principle for unnormalized statistical models ","permalink":"https://iamtu.dev/posts/noise-contrastive-estimation/","summary":"TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \\(T\\) observations \\((x_1, \u0026hellip; x_T)\\) drawn from a true distribution \\(p_d(.)\\). We then try to approximate \\(p_d\\) by a parameterized function \\(p_m(.;\\theta)\\). The estimator \\(\\hat{\\theta}_T\\) is defined to be the \\(\\theta\\) that maximize function\n$$ J_T(\\theta) = \\frac{1}{2T}\\sum_t{\\log[h(x_t; 0)]} + \\log[1-h(y_t; \\theta)] $$","title":"Noise constrastive estimation"},{"content":"Notes I took during studying MIT OCW Real Analysis. The class taught by Professor Casey Rodriguez, he also taught Functional analysis.\nResources (Useful link) Video lecture Course\u0026rsquo;s homepage Lecture notes Goal of the course - Gain experience with proofs - Prove statements about the real numbers, function and limits\nLecture 1: Sets, Set operations, and Mathematical Induction Definition (Sets) A sets is a collection of objects called elements/members.\nDefinition (Empty set) A set with no elements, denoted as \\(\\emptyset\\)\nNotation\n\\(a \\in S\\): \\(a\\) is a element of \\(S\\) \\(a \\notin S\\): \\(a\\) is not a element of \\(S\\) \\(\\forall\\): for all \\(\\exists\\): there exists \\(\\implies\\): implies \\(\\iff\\): if and only if Definition\n(subset) A set \\(A\\) is a subset of \\(B\\), denoted as \\(A \\subset B\\) if: \\(a \\in A \\implies a \\in B\\) (equal) Two sets are equal if \\(A \\subset B \\land B \\subset A\\) (proper subset) \\(A \\subsetneqq B \\iff A \\subset B \\land A \\neq B\\) Set building notation\n$$ \\{ x \\in A : P(x) \\} $$\nExamples\n\\(\\mathbb{N} = \\{1, 2, 3 \\cdots\\}\\) \\(\\mathbb{Z} = \\{\\cdots,-2, -1, 0, 1, 2, \\cdots\\}\\) \\(\\mathbb{Q} = \\{\\frac{m}{n}: m, n \\in \\mathbb{Z}\\}\\) Real number set \\(\\mathbb{R}\\) Remark: \\(\\mathbb{N} \\subset \\mathbb{Z} \\subset \\mathbb{Q} \\subset \\mathbb{R}\\)\nGoal: describe the real number set $\\mathbb{R}$\nDefinition (union) The union of A and B is the set $$ A \\cup B := {x: x\\in A \\lor x\\in B} $$ Definition (intersection) The intersection of A and B is the set $$ A \\cap B := {x: x\\in A \\land x \\in B} $$ Definition (different) The set different between A w.r.t B is the set $$ A\\backslash B = {x\\in A: x\\notin B} $$ Definition (complement) A complement of set A is the set $$ A^c = {x: x\\notin A} $$ Definition (disjoint) Two sets A and B are disjoint if \\(A \\cap B = \\emptyset\\).\nTheorem (De-Morgan) If A, B, C are sets then\n\\((B \\cup C)^c = B^c \\cap C^c\\) \\((B \\cap C)^c = B^c \\cup C^c\\) \\(A\\backslash (B\\cup C) = (A\\backslash B) \\cap (A\\backslash C)\\) \\(A\\backslash (B\\cap C) = (A\\backslash B) \\cup(A\\backslash C)\\) Induction A way to prove theorem about natural number.\n\\(\\mathbb{N} = \\{1, 2, 3, \\cdots \\}\\) has an ordering \\(1 \u0026lt; 2 \u0026lt; 3\u0026lt; 4 \u0026lt; \\cdots\\)\nAxiom (Well ordering of natural numbers) if \\(S\\subset \\mathbb{N}\\) and \\(S\\neq \\emptyset\\) has a least element \\(\\exists x\\in S\\) st \\(\\forall y \\in S: x\\leq y\\).\nTheorem (Induction) Let \\(P(n)\\) be a statement depending on \\(n\\in \\mathbb{N}\\). Assume:\n(Base case) \\(P(1)\\) is true (Inductive step) If \\(P(m)\\) is true, then \\(P(m+1)\\) is true. Then \\(P(n)\\) is true for all \\(n\\in \\mathbb{N}\\). Proof: Let \\(S = {n\\in\\mathbb{N}: P(n) \\text{ is not true}}\\). Want to show \\(S=\\emptyset\\)\nSuppose \\(S\\neq \\emptyset\\). By WOP.\\(\\mathbb{N}\\), \\(S\\) has a least element \\(x\\in S\\). Since \\(P(1)\\) is true, \\(1\\notin S\\), so \\(x\u0026gt;1\\).\nSince \\(x\\) is the least element in \\(S\\) \\(\\implies x-1 \\notin S\\).\nBy the definition of \\(S\\), \\(P(x-1)\\) is true, by 2) \\(\\implies P(x)\\) is true \\(\\implies x \\notin S\\).\n\\(\\therefore S = \\emptyset\\)\nUsing induction We want to prove some statement \\(\\forall n\\in\\mathbb{N}:P(n)\\) is true, we have to do two things:\nProve \\(P(1)\\). Prove \\(P(m) \\implies P(m+1)\\) Example For all \\(c\\neq 1, \\forall n\\in\\mathbb{N}\\):\n$$ 1 + c + c^2 + \\cdots +c^n=\\frac{1-c^{n+1}}{1-c} $$\nProof\n(Base case): $$ 1 + c^1 = \\frac{1-c^{1+1}}{1-c}=\\frac{(1-c)(1+c)}{1-c} = {1+c}; \\forall c\\neq1 $$\n(Inductive step) Assume: $$ 1+c+c^2+\\cdots+c^m=\\frac{1-c^{m+1}}{1-c}\\quad(*) $$ We want to show $$ 1+c+c^2+\\cdots+c^n=\\frac{1-c^{n+1}}{1-c}\\quad(**) $$ for \\(n = m+1\\). We have:\n$$ \\begin{aligned} 1+c+c^2+\\cdots+c^m+c^{m+1} \u0026amp;= \\frac{1-c^{m+1}}{1-c}+c^{m+1} \\\\ \u0026amp; = \\frac{1-c^{m+1}+c^{m+1}-c^{m+2}}{1-c}\\\\ \u0026amp; = \\frac{1-c^{(m+1)+1}}{1-c} \\end{aligned} $$ So (*) hold for \\(n=m+1\\). By induction, \\(P(n)\\) is true \\(\\forall n\\in\\mathbb{N}\\).\n","permalink":"https://iamtu.dev/posts/real-analysis/","summary":"Notes I took during studying MIT OCW Real Analysis. The class taught by Professor Casey Rodriguez, he also taught Functional analysis.\nResources (Useful link) Video lecture Course\u0026rsquo;s homepage Lecture notes Goal of the course - Gain experience with proofs - Prove statements about the real numbers, function and limits\nLecture 1: Sets, Set operations, and Mathematical Induction Definition (Sets) A sets is a collection of objects called elements/members.\nDefinition (Empty set) A set with no elements, denoted as \\(\\emptyset\\)","title":"Real Analysis - Lecture notes"}]