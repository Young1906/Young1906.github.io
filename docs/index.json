[{"content":"Note\n I am preparing for my graduate school application, this post is written in preparation for the application and interview. I suppose to be preparing a slide for my supervisor, but sometime you can\u0026rsquo;t help when the mood strikes. So sorry in advance to my supervisor.  Problem Given a statistical model \\(P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) = P(\\boldsymbol{X} | \\boldsymbol{Z}, \\boldsymbol{\\theta})\\), which generate set of observations \\(\\boldsymbol{X}\\), where \\(\\boldsymbol{Z}\\) is a latent variable and unknow parameter vector \\(\\boldsymbol{\\theta}\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that maximize the marginal likelihood:\n$$ \\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{X}) = P(\\boldsymbol{X} | \\boldsymbol{\\theta}) = \\int_{\\boldsymbol{Z}}P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})d\\boldsymbol{Z} $$\nAs an example for this type of problem, there are two (unfair) coin A and B with probability of head for each coin is \\(p_A(H) = p \\text{ and } p_B(H) = q\\). For each trial, we select coin A with probability \\(p(A) = \\tau\\) and coin B with probability \\(p(B) = 1 -\\tau\\), toss the coin and record the observation. The set of observations \\(\\boldsymbol{X}\\) is the record of head or tail \\(\\{H, T, H, H, \\cdots\\}\\), the latent variable which is unobserved is which coint is selected for each trail \\(\\{A, B, B, A, \\cdots\\}\\), and the unknown parameter vector \\(\\boldsymbol{\\theta} = [p, q, \\tau]\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that best fit observations; EM is an instance of Maximum Likelihood Estimation (MLE).\nThe EM algorithm The EM algorithm seeks for \\(\\boldsymbol{\\theta}\\) by first initiates a random parameter vector \\(\\boldsymbol{\\theta}^{(0)}\\) and then iteratively performs two steps, namely the expectation step (E step) and the maximization step (M step):\n (The E step) the expected loglikelihood of \\(\\boldsymbol{\\theta}\\), with respect to the current conditional distribution of \\(\\boldsymbol{Z}\\) given observations \\(\\boldsymbol{X}\\) and current estimation of \\(\\boldsymbol{\\theta}^{(t)}\\)  $$ Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) = \\mathbb{E}_{\\boldsymbol{Z} \\sim P(. | \\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)})} {[ \\log P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) ]} $$\n (The M step) update parameter vector \\(\\boldsymbol{\\theta}\\)  $$ \\boldsymbol{\\theta}^{(t+1)} = \\arg\\max_{\\boldsymbol{\\theta}} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) $$\nEM for the coin example Setup\n  Parameter vector \\(\\boldsymbol{\\theta} = [p, q, \\tau]\\), and its estimation at step (t) is \\(\\boldsymbol{\\theta}^{(t)} = [p^{(t)}, q^{(t)}, \\tau^{(t)}]\\)\n  The \\(i^{th}\\) observation \\(x^{(i)}\\) is either head (H) or tail (T).\n  The coin selected for the \\(i^{th}\\) trail \\(z^{(i)}\\) is either A or B:\n \\(p(z^{(i)} = A) = \\tau\\) \\(p(z^{(i)} = B) = 1 -\\tau\\).  For both cases, $$ \\begin{equation} p(z^{(i)}) = \\tau^{\\mathbb{I}(z^{(i)}=A)}(1-\\tau)^{\\mathbb{I}(z^{(i)}=B)} \\end{equation} $$\n  When selected the coin A,\n Probability that we get a head (H): \\(p(x^{(i)}=H | z^{(i)} = A) = p\\) Probability that we get a head (T): \\(p(x^{(i)}=T | z^{(i)} = A) = 1 - p\\)  For both cases, $$ \\begin{equation} p(x^{(i)} | z^{(i)}=A) = p^{\\mathbb{I}(x^{(i)}=H)}(1 - p)^{\\mathbb{I}(x^{(i)}=T)} \\end{equation} $$\n  Similarly, when B is selected $$ \\begin{equation} p(x^{(i)} | z^{(i)}=B) = q^{\\mathbb{I}(x^{(i)}=H)}(1 - q)^{\\mathbb{I}(x^{(i)}=T)} \\end{equation} $$\n  Where \\(\\mathbb{I}(\\cdot)\\) is an indicator function on a predicate $$ \\mathbb{I}(p) = \\begin{cases} 1 \\quad \\text{if } p \\text{ is True}\\\\ 0 \\quad \\text{otherwise} \\end{cases} $$\nOnce again, we generalize for both cases of \\(z^{(i)}\\)\n$$ \\begin{equation} \\begin{aligned} p(x^{(i)} | z^{(i)}) = [p^{\\mathbb{I}(x^{(i)}=H)}(1 - p)^{\\mathbb{I}(x^{(i)}=T)}]^{\\mathbb{I}(z^{(i)}=A)}\\\\ \\times [q^{\\mathbb{I}(x^{(i)}=H)}(1 - q)^{\\mathbb{I}(x^{(i)}=T)}]^{\\mathbb{I}(z^{(i)}=B)} \\end{aligned} \\end{equation} $$\nThe equation looks rather ugly, we can simplify this by encoding head as 1 and tail as 0; coin A as 1 and coin B as 0. The equation above can be written as\n$$ \\begin{equation} p(x^{(i)} | z^{(i)}) = [p^{x^{(i)}}(1-p)^{1 - x^{(i)}}]^{z^{(i)}} [q^{x^{(i)}}(1-q)^{1 - x^{(i)}}]^{1-z^{(i)}} \\end{equation} $$\nSimilarly for \\(p(z^{(i)})\\) $$ \\begin{equation} p(z^{(i)}) = \\tau^{z^{(i)}}(1-\\tau)^{1-z^{(i)}} \\end{equation} $$\nApplying EM algorithm\n  The E step:\n Construct the joint likelihood of a single pair of observation and latent variable \\(p(x^{(i)}, z^{(i)})\\ | \\boldsymbol{\\theta})\\). For the conciseness, we drop the \\((i)\\) superscript from the equation.  $$ \\begin{equation} \\begin{aligned} p(x, z | \\boldsymbol{\\theta}) = \u0026amp; p(x | z, \\boldsymbol{\\theta})p(z | \\boldsymbol{\\theta})\\\\ = \u0026amp; [p^{x}(1-p)^{1 - x}]^{z} [q^{x}(1-q)^{1 - x}]^{1-z} \\tau^{z}(1-\\tau)^{1-z} \u0026amp; \\text{\\tiny(from eq. 5 and 6)} \\end{aligned} \\end{equation} $$\n  Likelihood over entire observations \\(\\boldsymbol{X}\\) and latent \\(\\boldsymbol{Z}\\):\n$$\\boldsymbol{X}\\odot\\boldsymbol{Z} := \\{(x^{(i)}, z^{(i)})\\}_{i=1\\cdots N}$$\n A side note is that I am not entirely sure that \\(\\odot\\) operator is appropriate in this situation.\n $$ \\begin{equation} \\begin{aligned} P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) =\u0026amp; \\prod_{(x, z) \\in \\boldsymbol{X}\\odot\\boldsymbol{Z}} { p(x, z | \\boldsymbol{\\theta}) } \\end{aligned} \\end{equation} $$\n  Log likelihood of the joint probability\n$$ \\begin{equation} \\begin{aligned} \\log P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) \u0026amp; = \\sum_{(x, z)} \\log p(x, z | \\boldsymbol{\\theta}) \\end{aligned} \\end{equation} $$\n Taking a log always seem to make thing to be better.\n   Finally, we need to take the expectation of the log likelihood w.r.t conditional probability of \\(\\boldsymbol{Z}|\\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)}\\)\n    Proof of correctness EM for Gaussian Mixture Model ","permalink":"https://iamtu.dev/posts/em/","summary":"Note\n I am preparing for my graduate school application, this post is written in preparation for the application and interview. I suppose to be preparing a slide for my supervisor, but sometime you can\u0026rsquo;t help when the mood strikes. So sorry in advance to my supervisor.  Problem Given a statistical model \\(P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) = P(\\boldsymbol{X} | \\boldsymbol{Z}, \\boldsymbol{\\theta})\\), which generate set of observations \\(\\boldsymbol{X}\\), where \\(\\boldsymbol{Z}\\) is a latent variable and unknow parameter vector \\(\\boldsymbol{\\theta}\\).","title":"Expectation Maximization - EM"},{"content":"This post is a note I take from while reading Blei et al 2018.\nGoal:\n Motivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI  ELBO Goal is to find \\(q(z)\\) to approximate \\(p(z|x)\\)\nThe KL-divergence\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026amp;= \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\end{aligned} \\end{equation} $$\nHowever, this quantity is intractable to compute hence, we\u0026rsquo;re unable to optimize this quantity directly.\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026amp;= - \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\\\ \u0026amp;= -\\int_z{ q(z) \\log { \\frac{\\log p(z, x)}{q(z) p(x)} } }\\\\ \u0026amp;= -\\int_z{q(z)[\\log{\\frac{p(z,x)}{q(z)}} - \\log p(x)]dz} \\\\ \u0026amp;= -\\int_z{ q(z) \\log \\frac{p(z, x)}{q(z)}dz } + \\int_z{q(z)\\log p(x) dz} \\\\ \u0026amp; =: -\\texttt{ELBO}[q] + \\log p(x) \\\\ \\iff \\texttt{ELBO}[q] \u0026amp;= -KL(q||p) + \\log p(x) \\end{aligned} \\end{equation} $$\nBecause \\(\\log p(x)\\) is a constant, by maximizing \\(\\text{ELBO}[q]\\), we minimize \\(KL(q||p)\\) by proxy. Rewrite ELBO:\n$$ \\begin{equation} \\begin{aligned} \\texttt{ELBO}(q) \u0026amp;= \\int_z{q(z)\\log \\frac{p(z, x)}{q(z)}} \\\\ \u0026amp;= \\mathbb{E}_{z\\sim q}[\\log p(z, x)] - \\mathbb{E}_{z\\sim q}[\\log q(z)] \\end{aligned} \\end{equation} $$\nMean field Variational Family Mean-field variational family made a strong assumption of independence between it\u0026rsquo;s latent variable\n$$ q(\\mathbf{z}) = \\prod_{j} {q_j(z_j)} $$\nCoordinate ascent variational inference is a common method to solve mean-field variational inference problem. Holding other latent variable fixed, the \\(j^{th}\\) latent variable is given by:\n$$ q^*_{j}(z_j) = \\text{exp}{\\mathbb{E}_{-j}[\\log p(z_j | z_{-j}, \\mathbf{x})]} \\propto \\exp{\\mathbb{E}_{-j} [\\log p(z_j, z_{-j}, \\mathbf{x})]} $$\nProof $$ \\begin{equation} \\begin{aligned} q^*_j(z_j) \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad{\\texttt{ELBO}(q)} \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_q[\\log p(z_j, z_{-j}, x)] - \\mathbb{E}_q[\\log q(z_j, z_{-j})] \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log q_j(z_j) + \\log q_{-j}(z_{-j})]] \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] + const \\\\ \u0026amp;= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] \\end{aligned} \\end{equation} $$\nWe need to find function \\(q_j(z_j)\\) that maximize \\(\\text{ELBO}(q)\\)\nAssuming \\(q_j(z_j)= \\epsilon \\eta(z_j) + q^*_j(z_j)\\)\n$$ \\begin{aligned} K(\\epsilon) \u0026amp;= \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] \\\\ \u0026amp;= \\int_{z_j} q_j(z_j) A d_{z_j} - \\int_{z_j}q_j(z_j)\\log q_z(z_j) d_{z_j} \\\\ \u0026amp;= \\int_{z_j} [\\epsilon \\eta(z_j) + q^*_j(z_j)] A d_{z_j} - \\int_{z_j}[\\epsilon \\eta(z_j) + q^*_j(z_j)] \\log [\\epsilon \\eta(z_j) + q^*_j(z_j)] d_{z_j} \\end{aligned} $$\nEvaluate the partial derivative of \\(K\\) wrt \\(\\epsilon\\) we have:\n$$ \\begin{aligned} \u0026amp; \\frac{\\partial}{\\partial \\epsilon}K \\bigg\\vert_{\\epsilon=0} = 0 \\\\ \\iff \u0026amp; \\int_{z_j} {\\eta(z_j) A d_{z_j}} - \\int_{z_j} { {\\eta(z_j) \\log [\\epsilon \\eta(z_j) + q^*_j(z_j)]} + [\\epsilon \\eta(z_j) + q^*_j(z_j)] \\frac{\\eta(z_j)}{\\epsilon \\eta(z_j) + q^*_j(z_j)}d_{z_j} } = 0\\\\ \\iff \u0026amp; \\int_{z_j} {\\eta(z_j) A d_{z_j}} - \\int_{z_j}{[\\eta(z_j)\\log q^*_j(z_j) +\\eta(z_j)]d_{z_j}} = 0; \\quad \\forall \\eta(z_j) \\\\ \\iff \u0026amp; \\log q^*_j(z_j) = A-1 = \\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)] - 1 \\\\ \\iff \u0026amp; q^*_j(z_j) \\propto \\exp{\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]} \\end{aligned} $$\nComplete example of Bayesian Gaussian Mixture TDB\n","permalink":"https://iamtu.dev/posts/variational_inference/","summary":"This post is a note I take from while reading Blei et al 2018.\nGoal:\n Motivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI  ELBO Goal is to find \\(q(z)\\) to approximate \\(p(z|x)\\)\nThe KL-divergence\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026amp;= \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\end{aligned} \\end{equation} $$\nHowever, this quantity is intractable to compute hence, we\u0026rsquo;re unable to optimize this quantity directly.","title":"Understanding Variational Inference"},{"content":"Motivating example Evaluating following integral\n$$ I = \\int_0^1{\\frac{1 - x^2}{\\ln{x}}dx} $$\nClosed-form results $$ \\begin{equation} \\begin{aligned} F(t) \u0026amp;= \\int_0^1{\\frac{1-x^t}{\\ln(x)}dx} \\\\ \\implies \\frac{d}{dt}F \u0026amp;= \\frac{d}{dt}\\int_0^1{\\frac{1-x^t}{\\ln(x)}dx}\\\\ \u0026amp;= \\int_0^1{ \\frac{\\partial}{\\partial t} \\frac{1-x^t}{\\ln(x)}dx }\\\\ \u0026amp;= \\int_0^1{ \\frac{-\\ln(x)x^t}{ln(x)} dx} \\\\ \u0026amp;= \\bigg[-\\frac{x^{t+1}}{t+1}\\bigg]_0^1\\\\ \u0026amp;= -\\frac{1}{t+1}\\\\ \\implies F(t) \u0026amp;= -\\ln({t+1}) \\\\ \\implies I \u0026amp;= f(2) = -\\ln3 \\end{aligned} \\end{equation} $$\nNumerical approximation Code to produce the figure import numpy as np from matplotlib import pyplot as plt def I(): g = lambda x: (1 - x**2)/np.log(x) vG = np.vectorize(g) x = np.random.uniform(0, 1, 10000) return vG(x).mean()  \n","permalink":"https://iamtu.dev/posts/diff-under-integral-sign/","summary":"Motivating example Evaluating following integral\n$$ I = \\int_0^1{\\frac{1 - x^2}{\\ln{x}}dx} $$\nClosed-form results $$ \\begin{equation} \\begin{aligned} F(t) \u0026amp;= \\int_0^1{\\frac{1-x^t}{\\ln(x)}dx} \\\\ \\implies \\frac{d}{dt}F \u0026amp;= \\frac{d}{dt}\\int_0^1{\\frac{1-x^t}{\\ln(x)}dx}\\\\ \u0026amp;= \\int_0^1{ \\frac{\\partial}{\\partial t} \\frac{1-x^t}{\\ln(x)}dx }\\\\ \u0026amp;= \\int_0^1{ \\frac{-\\ln(x)x^t}{ln(x)} dx} \\\\ \u0026amp;= \\bigg[-\\frac{x^{t+1}}{t+1}\\bigg]_0^1\\\\ \u0026amp;= -\\frac{1}{t+1}\\\\ \\implies F(t) \u0026amp;= -\\ln({t+1}) \\\\ \\implies I \u0026amp;= f(2) = -\\ln3 \\end{aligned} \\end{equation} $$\nNumerical approximation Code to produce the figure import numpy as np from matplotlib import pyplot as plt def I(): g = lambda x: (1 - x**2)/np.","title":"Differentiation under integral sign"},{"content":"The closed form of KL divergence used in Variational Auto Encoder.\nUnivariate case Let\n \\(p(x) = \\mathcal{N}(\\mu_1, \\sigma_1) = (2\\pi\\sigma_1^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2]\\) \\(q(x) = \\mathcal{N}(\\mu_1, \\sigma_2) = (2\\pi\\sigma_2^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2]\\)  KL divergence between \\(p\\) and \\(q\\) is defined as:\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= -\\int_{x}{p(x)\\log{\\frac{q(x)}{p(x)}}dx} \\\\ \u0026amp;= -\\int_x p(x) [\\log{q(x)} - \\log{p(x)}]dx \\\\ \u0026amp;= \\underbrace{ \\int_x{p(x)\\log p(x) dx}}_A - \\underbrace{ \\int_x{p(x)\\log q(x) dx}}_B \\end{aligned} $$\nFirst quantity \\(A\\):\n$$ \\begin{aligned} A \u0026amp;= \\int_x{p(x)\\log p(x) dx} \\\\ \u0026amp;= \\int_x{p(x)\\big[ -\\frac{1}{2}\\log{2\\pi\\sigma_1^2 - \\frac{1}{2\\sigma_1^2}(x - \\mu_1)^2} \\big]dx}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi\\sigma_1^2}\\int_x{p(x)dx} - \\frac{1}{2\\sigma_1^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)^2dx}}_{\\text{var(x)}}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi} - \\log\\sigma_1-\\frac{1}{2} \\end{aligned} $$\nThe second quantity \\(B\\):\n$$ \\begin{aligned} B =\u0026amp; \\int_x{p(x)\\big[ -\\frac{1}{2}\\log2\\pi\\sigma_2^2 - \\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2 \\big]dx}\\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi\\sigma_2^2 - \\frac{1}{2\\sigma_2^2}\\int_x{ p(x)\\big[ (x - \\mu_1)^2 + 2(x-\\mu_1)(\\mu_1 - \\mu_2) + (\\mu_1 -\\mu_2)^2 \\big]dx} \\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi\\sigma_2^2 \\\\ \u0026amp; - \\frac{1}{2\\sigma_2^2}\\underbrace{\\int_x{p(x)(x-\\mu_1)^2}}_{\\text{var}(x)}\\\\ \u0026amp; - \\frac{2(\\mu_1 -\\mu_2)}{2\\sigma_2^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)dx}}_0 \\\\ \u0026amp; - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} \\\\ =\u0026amp; -\\frac{1}{2}\\log2\\pi -\\log\\sigma_2 - \\frac{\\sigma_1^2}{2\\sigma_2^2} - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2} \\end{aligned} $$\nFinally, we obtained the KL divergence for univariate case\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= A - B \\\\ \u0026amp;= (-\\frac{1}{2}\\log2\\pi - \\log\\sigma_1 - \\frac{1}{2}) - ( -\\frac{1}{2}\\log2\\pi -\\log\\sigma_2 - \\frac{\\sigma_1^2}{2\\sigma_2^2} - \\frac{(\\mu_1-\\mu_2)^2}{2\\sigma_2^2}) \\\\ \u0026amp;= \\log\\frac{\\sigma_2}{\\sigma_1} + \\frac{\\sigma_1^2}{2\\sigma_2^2} + \\frac{(\\mu_1 -\\mu_2)^2}{2\\sigma_2^2} - \\frac{1}{2} \\end{aligned} $$\nMultivariate case tbd\nReference  https://gregorygundersen.com/blog/  ","permalink":"https://iamtu.dev/posts/closed-form-kl-gaussian/","summary":"The closed form of KL divergence used in Variational Auto Encoder.\nUnivariate case Let\n \\(p(x) = \\mathcal{N}(\\mu_1, \\sigma_1) = (2\\pi\\sigma_1^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_1^2}(x-\\mu_1)^2]\\) \\(q(x) = \\mathcal{N}(\\mu_1, \\sigma_2) = (2\\pi\\sigma_2^2)^{-\\frac{1}{2}}\\exp[-\\frac{1}{2\\sigma_2^2}(x-\\mu_2)^2]\\)  KL divergence between \\(p\\) and \\(q\\) is defined as:\n$$ \\begin{aligned} \\text{KL}(p\\parallel q) \u0026amp;= -\\int_{x}{p(x)\\log{\\frac{q(x)}{p(x)}}dx} \\\\ \u0026amp;= -\\int_x p(x) [\\log{q(x)} - \\log{p(x)}]dx \\\\ \u0026amp;= \\underbrace{ \\int_x{p(x)\\log p(x) dx}}_A - \\underbrace{ \\int_x{p(x)\\log q(x) dx}}_B \\end{aligned} $$\nFirst quantity \\(A\\):\n$$ \\begin{aligned} A \u0026amp;= \\int_x{p(x)\\log p(x) dx} \\\\ \u0026amp;= \\int_x{p(x)\\big[ -\\frac{1}{2}\\log{2\\pi\\sigma_1^2 - \\frac{1}{2\\sigma_1^2}(x - \\mu_1)^2} \\big]dx}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi\\sigma_1^2}\\int_x{p(x)dx} - \\frac{1}{2\\sigma_1^2} \\underbrace{\\int_x{p(x)(x-\\mu_1)^2dx}}_{\\text{var(x)}}\\\\ \u0026amp;= -\\frac{1}{2}\\log{2\\pi} - \\log\\sigma_1-\\frac{1}{2} \\end{aligned} $$","title":"Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution"},{"content":"Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper. In the analogy above, the black box represents the simulator, and the configurations are the simulator’s parameters.\nThe applicability of SBI has great potential since we can almost reduce any process with defined input and output to a black-box machine 1.\nThis post documents my notes while studying Likelihood-free MCMC with Amortized Ratio Estimator (Hermans et al, 2020); a method developed to address SBI.\nLikelihood-free MCMC with Amortized Ratio Estimator Likelihood ratio is defined as the ratio between the likelihood of the observation between two different hypothesis:\n$$ r(\\mathbf{x} | \\theta_0, \\theta_1) = \\frac{p(\\mathbf{x} | \\theta_0)}{p(\\mathbf{x}|\\theta_1)} $$\nThis quantity then can be used in various methods to draw sample from a distribution. In the paper, the author mention three sampling methods, namely Markov Chain Monte Carlo, Metropolis-Hasting, and HMC. In the following section, I am briefly summarizing those methods.\nBackground Markov Chain Monte Carlo (MCMC) In statistics, the MCMC method is a class of algorithms for sampling from a probability distribution. By constructing a Markov chain with the desired distribution as its equilibrium distribution, one can obtain a sample of the desired distribution by recording states from the state chain 2.\nAdapting MCMC for SBI task We want to sample from \\(p(\\theta | \\mathbf{x})\\) using MCMC, we need this quantity\n$$ \\begin{equation} \\begin{aligned} \\frac{p(\\theta | \\mathbf{x} )}{p(\\theta_t| \\mathbf{x})} = \\frac{ p(\\theta)p(\\mathbf{x} | \\theta)/p(\\mathbf{x}) }{ p(\\theta_t)p(\\mathbf{x} | \\theta_t)/p(\\mathbf{x}) } = \\frac{p(\\theta)}{p(\\theta_t)}\\times \\frac{p(\\mathbf{x} | \\theta)}{p(\\mathbf{x} | \\theta_t)} = \\frac{p(\\theta)}{p(\\theta_t)} \\times r(\\mathbf{x} | \\theta, \\theta_t) \\end{aligned} \\end{equation} $$\nWe can compute the first term of the equation since we have access to prior \\(p(\\theta)\\). But we can not compute the second term because we do not have access to the likelihood function \\(p(\\mathbf{x} | \\theta)\\). However, we can reframe the problem in the supervised-learning paradigm, so we can use a parameterized discriminator \\(d_\\theta(\\mathbf{x})\\) to estimate the likelihood. The details are described in the Likelihood Ratio Estimator section.\nMetropolis-Hasting (MH) tbd\nHalmitonian Markov Chain(MH) tbd\nLikelihood Ratio Estimator The remaining question is how to estimate the likelihood ratio \\( r(\\mathbf{x} | \\theta_0, \\theta_1)\\). To estimate the ratio, the author employed the Likelihood Ratio Trick, training a discriminator \\(d_\\phi(\\mathbf{x})\\) to classify samples \\( x \\sim p(\\mathbf{x} | \\theta_0)\\) with class label \\(y = 1\\) from \\(\\mathbf{x} \\sim p(\\mathbf{x} | \\theta_1)\\) with class label \\(y = 0\\). The decision function obtained by the trained discrimininator:\n$$ d^*(\\mathbf{x}) = p(y = 1 | \\mathbf{x}) = \\frac{p(\\mathbf{x} | \\theta_0)}{p(\\mathbf{x} | \\theta_0) + p(\\mathbf{x} | \\theta_1)} $$\nThen the estimation of likelihood ratio can be computed by:\n$$ \\hat{r}(\\mathbf{x} | \\theta_0, \\theta_1) = \\frac {d^{*}(\\mathbf{x})} {1 - d^{*}(\\mathbf{x})} $$\nHowever, this method required the discriminator to be trained at every pair of \\((\\theta_0, \\theta_1)\\), which is impractical in the context. To overcome this issue, the paper proposed to train the discriminator to classify dependent sample-parameter pairs \\((\\mathbf{x}, \\mathbf{\\theta}) \\sim p(\\mathbf{x}, \\mathbf{\\theta})\\) with label \\(y=1\\) from the independent sample-parameter pairs \\((\\mathbf{x}, \\mathbf{\\theta}) \\sim p(\\mathbf{x})p(\\mathbf{\\theta})\\) with label \\(y=0\\).\n$$ \\begin{equation} \\begin{aligned} d^*(\\mathbf{x}, \\mathbf{\\theta}) \u0026amp;= \\frac {p(\\mathbf{x}, \\mathbf{\\theta})} { p(\\mathbf{x}, \\mathbf{\\theta}) + p(\\mathbf{x}) p(\\mathbf{\\theta}) } \\ \\end{aligned} \\end{equation} $$\nThe likelihood-to-evidence ratio is computed by\n$$ r(\\mathbf{x} | \\theta) = \\frac {p(\\mathbf{x} | \\theta)} {p(x)} = \\frac{p(x, \\theta)}{p(x)p(\\theta)} = \\frac {d^{*}(x, \\theta)} {1 - d^{*}(x, \\theta)} $$\nThen the likelihood ratio for any two hypotheses can be estimated at any point by\n$$ r(x | \\theta_0, \\theta_1) = \\frac{d^{*}(x,\\theta_0)}{d^{*}(x, \\theta_1)} $$\nToy example Setup:\n The simulator: a function takes 1 parameter \\(\\mu\\), and return a random variable drawn from \\(\\mathcal{N}(\\mu, 1)\\) The observations \\(\\mathbf{x}\\): Observation drawn from the simulator with \\(\\mu = 2.5\\), which is unknown to the algorithm. The discriminator: A fully connected neural network. The prior of the parameters: \\(\\mathcal{N}(0, 1)\\)  We want to draw samples from the posterior distribution \\(p(\\theta | \\mathbf{x})\\), where \\(x \\sim \\mathcal{N}(2.5, 1)\\).\nImplementation import click import numpy as np import torch from matplotlib import pyplot as plt from torch import nn from torch.nn import functional as F from typing import NamedTuple np.random.seed(1) torch.manual_seed(1) def stochastic(func): def __wrapper__(*args, **kwargs): np.random.seed() rs = func(*args, **kwargs) np.random.seed(1) return rs return __wrapper__ class Layer(NamedTuple): h: int # hidden dim a: str # activation def Dense(h_i: int, h_j: int, a : str): if a == \u0026#34;tanh\u0026#34;: act = nn.Tanh() elif a == \u0026#34;sigmoid\u0026#34;: act = nn.Sigmoid() elif a == \u0026#34;relu\u0026#34;: act = nn.ReLU() else: raise NotImplementedError(a) return nn.Sequential( nn.Linear(h_i, h_j), act) def build_mlp(input_dim: int, seq: list[Layer]) -\u0026gt; nn.Module: h0, a0 = seq[0] _seq = [Dense(input_dim, h0, a0)] for j in range(1, len(seq)): h_j, a_j = seq[j] h_i, _ = seq[j - 1] _seq.append(Dense(h_i, h_j, a_j)) return nn.Sequential(*_seq) def train_step( Xpos: torch.Tensor, Xneg: torch.Tensor, d: nn.Module, opt: torch.optim.Optimizer) -\u0026gt; torch.Tensor: \u0026#34;\u0026#34;\u0026#34; Args: - Xpos: (x, theta) - Xneg: (x, theta\u0026#39;) - d: classifier Where theta/theta\u0026#39; ~ p, x ~ p(x | theta) \u0026#34;\u0026#34;\u0026#34; for i in range(32): opt.zero_grad() zpos = d(Xpos) zneg = d(Xneg) loss = F.binary_cross_entropy(zpos, torch.ones_like(zpos))\\ + F.binary_cross_entropy(zneg, torch.zeros_like(zneg)) loss.backward() opt.step() return loss.item() def train_d( p: callable, sim: callable, d: nn.Module, m: int, e: int, lr: float): \u0026#34;\u0026#34;\u0026#34; Args: - p : prior - sim: simulator (implicit p(x | theta) - d: parameterized classifier - m: batch_size - e: max epochs - lr: learning rate \u0026#34;\u0026#34;\u0026#34; opt = torch.optim.Adam(d.parameters(), lr=lr) sch = torch.optim.lr_scheduler.ReduceLROnPlateau(opt) losses = [] for b in range(e): theta = p(m) theta_prime = p(m) x = sim(theta) # expand dims everything theta = np.expand_dims(theta, -1) theta_prime = np.expand_dims(theta_prime, -1) x = np.expand_dims(x, -1) # construct training sample Xpos = np.concatenate([x, theta], -1) Xneg = np.concatenate([x, theta_prime], -1) Xpos, Xneg = torch.tensor(Xpos, dtype=torch.float),\\ torch.tensor(Xneg, dtype=torch.float) loss = train_step(Xpos, Xneg, d, opt) losses.append(loss) if b%50 == 49: sch.step(loss) return d, losses @stochastic def mcmc(lp: callable, obs: np.ndarray, d: nn.Module, n_samples: int, step_size: float): \u0026#34;\u0026#34;\u0026#34; Amortized MCMC likelihood free \u0026#34;\u0026#34;\u0026#34; # proposal distribution: q = lambda theta: np.random.normal(theta, step_size) # initialize theta theta = 0. samples = [] obs = np.expand_dims(obs, -1) for i in range(n_samples): theta_prime = q(theta) mu_theta = np.ones_like(obs) * theta mu_theta_prime = np.ones_like(obs) * theta_prime # construct input vector X = np.concatenate([obs, mu_theta], -1) Xp = np.concatenate([obs, mu_theta_prime], -1) X, Xp= torch.tensor(X, dtype=torch.float),\\ torch.tensor(Xp, dtype=torch.float) # Compute the decision function d_theta = d(X).detach().mean().numpy() d_theta_prime = d(Xp).detach().mean().numpy() r_theta = d_theta / (1 - d_theta) r_theta_prime = d_theta_prime / (1- d_theta_prime) H = r_theta_prime / r_theta H = lp(theta_prime) / lp(theta) * H H = 1 if H \u0026gt; 1 else H u = np.random.uniform() if u \u0026lt; H: # accept theta_prime samples.append(theta_prime) theta = theta_prime return samples def main( batch_size: int, max_iter: int, lr: float, n_obs: int, n_samples: int, step_size: float): # PROBLEM SETUP # -------------------------------------------------- # prior theta p = lambda m: np.random.normal(0, 1, size=m) lp = lambda x: np.exp(-0.5 * x**2)#likelihood function # simulator: unknown sim = lambda mu: np.random.normal(mu, np.ones_like(mu) * .25) # parmeterized classifier d = build_mlp( 2, [Layer(4, \u0026#39;relu\u0026#39;), Layer(2, \u0026#39;relu\u0026#39;), Layer(1, \u0026#39;sigmoid\u0026#39;)]) # TRAINING the classifier  # -------------------------------------------------- d, losses = train_d(p, sim, d, m=batch_size, e=max_iter, lr=lr) # inference # -------------------------------------------------- MU = 2.5 #unknown obs = sim(np.ones(n_obs) * MU) # Posterior sample: sample p(theta | obs) samples = mcmc(lp, obs, d, n_samples, step_size)  \nResult References  The frontier of simulation-based inference Likelihood-free MCMC with Amortized Ratio Estimator    I am a black-box machine, you are a black-box machine, everyone is a black-box machine as long as we don\u0026rsquo;t care enough about the person.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n Shamelessly copied from Wikipedia.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n   ","permalink":"https://iamtu.dev/posts/sbi/","summary":"Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper.","title":"Likelihood-free MCMC with Amortized Ratio Estimator"},{"content":"TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \\(T\\) observations \\((x_1, \u0026hellip; x_T)\\) drawn from a true distribution \\(p_d(.)\\). We then try to approximate \\(p_d\\) by a parameterized function \\(p_m(.;\\theta)\\). The estimator \\(\\hat{\\theta}_T\\) is defined to be the \\(\\theta\\) that maximize function\n$$ J_T(\\theta) = \\frac{1}{2T}\\sum_t{\\log[h(x_t; 0)]} + \\log[1-h(y_t; \\theta)] $$\nIn which:\n \\(y=(y_1, \u0026hellip;, y_T)\\) be a dataset of \\(T\\) observations draw from a noise density function $p_n(.)$. \\(h(u; \\theta) = 1/(1 + e^{-G(u;\\theta)})\\) \\(G(u; \\theta) = \\log p_m(u; \\theta) - \\log p_n(u)\\)  For \\(p_m(.; \\theta)\\) to be a valid p.d.f, we also need to include unit integral constraint into the optimization problem, that is \\(\\int_x{p_m(x; \\theta)dx} = 1\\). However, this integral is often intractable in most cases, for example, when we use a neural network to parameterize \\(p_m(.;\\theta)\\).\nInterestingly, the paper claims that maximizing the objective function gives a valid p.d.f without placing the unit integral constraint on the optimization (Theorem 1). In this post, I\u0026rsquo;ll attempt to prove the theorem as an exercise. Note that, I made an assumption that support of $x$ and $y$ are equal (in eq.7); which mean $p_n(.)$ is nonzero whenever $p_d(.)$ is nonzero and $p_n(.)$ is zero everywhere else.\nProof of threorem I When the sample size $T$ becomes arbitrarily large, the objective function $J_T(\\theta)$ converges in probability (this is a new word for me) to $\\tilde{J}$\n\\begin{equation} \\begin{aligned} \\tilde{J}(\\theta) = \\frac{1}{2}\\mathbb{E}_{x, y} { \\log{r\\big(f(x) - \\log{p_n(x)}\\big)} + \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} } \\end{aligned} \\end{equation}\nIn which \\(f(x) = \\log p_m(x; \\theta)\\) is the function approximating log-likelihood the true distribution $p_d(.)$.\nNotation  \\(p_d(x)\\) true probability density function (p.d.f) of data. \\(p_n(x)\\) p.d.f of noise generating distribution. \\(r(x) = \\frac{1}{1+\\exp(-x)}\\) sigmoid function. \\(X = (x_1, \u0026hellip; x_T); x \\sim p_d(x)\\) be the dataset of T observations. \\(Y = (y_1, \u0026hellip; y_T); y \\sim p_n(y)\\) be the dataset of T artificially generated noise. \\(p_m(.; \\theta)\\) is estimation of \\(p_d(.)\\) parameterized by \\(\\theta\\).  Theorem  \\(\\tilde{J}\\) attains a maximum at \\(f(.) = \\log p_d(.)\\). There are no other extrema if the noise density \\(p_n(.)\\) is chosen such it is nonzero whenever \\(p_d(.)\\) is nonzero.\n Proof Let \\(\\hat{f}(x)\\) be the optimal function that maximizes \\(\\tilde{J}\\), and \\(f(x)=\\hat{f}(x) + \\epsilon\\eta(x)\\).\n\\begin{equation} \\begin{aligned} \\tilde{J}(\\theta) \u0026amp;= K(\\epsilon) \\\\ \u0026amp;= \\frac{1}{2}\\mathbb{E}_{x, y} { \\log{ r\\big(f(x) - \\log{p_n(x)}\\big) } + \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} } \\\\ \u0026amp;= \\frac{1}{2}\\underbrace{ \\mathbb{E}_x { \\log r\\big( f(x) - \\log p_n(x) \\big) } }_A + \\frac{1}{2} \\underbrace{\\mathbb{E}_y { \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} }}_B \\\\ \\implies \\frac{dK}{d\\epsilon} \u0026amp;= \\frac{dA}{d\\epsilon} + \\frac{dB}{d\\epsilon} \\end{aligned} \\end{equation}\nExpand the first term of $K(\\epsilon)$\n\\begin{equation} \\begin{aligned} A(\\epsilon) \u0026amp;= \\mathbb{E}_x { \\log r\\big( f(x) - \\log p_n(x) \\big) } \\\\ \u0026amp; = \\int_x { p_d(x) \\log{ r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) \\big) } dx } \\end{aligned} \\end{equation}\nTaking derivative of $A(\\epsilon)$\n\\begin{equation} \\begin{aligned} \\frac{dA}{d\\epsilon} \u0026amp;= \\frac{1}{d\\epsilon} \\int_x { p_d(x) \\log{ r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) \\big) } dx } \\\\ \u0026amp; = \\int_x { p_d(x) \\big[ \\frac{1}{d\\epsilon}\\log{ r \\big( \\underbrace{ \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) }_{g(\\epsilon)} \\big) } \\big]dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\frac{d\\log{r}}{dr} \\frac{dr}{dg} \\frac{dg}{d\\epsilon} dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\frac{1}{r} r(1-r) \\eta(x) dx } \\\\ \u0026amp; = \\int_x{ p_d(x) \\big[1 - r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x)\\big) \\big] \\eta(x) dx } \\end{aligned} \\end{equation}\nNow let\u0026rsquo;s turn our attention to the second term of $K(\\epsilon)$\n\\begin{equation} \\begin{aligned} B(\\epsilon) \u0026amp;= \\mathbb{E}_y { \\log\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big] } \\\\ \u0026amp; = \\int_y { p_n(y) \\log \\big[ 1 - r \\big( \\underbrace{ \\hat{f}(y) + \\epsilon \\eta(y) - \\log p_n(y) }_h \\big) \\big]dy } \\end{aligned} \\end{equation}\nTaking derivative of $B$ w.r.t $\\epsilon$\n\\begin{equation} \\begin{aligned} \\frac{dB}{d\\epsilon} \u0026amp;= \\frac{1}{d\\epsilon} \\int_y{ p_n(y)\\log{ \\big[ 1 - r\\big( h(\\epsilon)\\big) \\big] }dy } \\\\ \u0026amp;= \\int_y { p_n(y) \\frac{d\\log(1-r)}{d(1-r)} \\frac{d(1-r)}{dr} \\frac{dr}{dh} \\frac{dh}{d\\epsilon} dy } \\\\ \u0026amp; = \\int_y { p_n(y) \\frac{1}{1-r} (-1) r(1-r) \\eta(y) } \\\\ \u0026amp; = -\\int_y{ p_n(y) r\\big( \\hat{f}(y) + \\epsilon \\eta(y) - \\log p_n(y) \\big) \\eta(y) dy } \\end{aligned} \\end{equation}\nSubstitute result from eq(4) and eq(6) to eq(2), $\\frac{dK}{d\\epsilon}$ is evaluated to $0$ at $\\epsilon = 0$.\n\\begin{equation} \\begin{aligned} \\frac{dK}{d\\epsilon}\\big\\vert_{\\epsilon=0} \u0026amp;= \\frac{dA}{d\\epsilon}\\big\\vert_{\\epsilon=0} + \\frac{dB}{d\\epsilon}\\big\\vert_{\\epsilon=0} \\\\ \u0026amp;= \\int_x { p_d(x) \\big[1 - r\\big( \\hat{f}(x) - \\log p_n(x)\\big) \\big] \\eta(x) dx } \\\\ \u0026amp; - \\int_y{ p_n(y) r \\big( \\hat{f}(y) - \\log p_n(y) \\big) \\eta(y) dy } \\\\ \u0026amp; = 0 \\end{aligned} \\end{equation}\nConsider eq. (7), if the support for $x$ and $y$ are equal, which mean we integrate $x$ and $y$ over a same region, we can change $y$ to $x$ and rewrite eq.(7) as\n\\begin{equation} \\begin{aligned} \\frac{dK}{d\\epsilon} \\big\\vert_{\\epsilon = 0} \u0026amp;= \\int_x { \\underbrace{ p_d(x) \\big[1 - r\\big( \\hat{f}(x) - \\log p_n(x)\\big) \\big] }_C \\eta(x) dx } \\\\ \u0026amp; - \\int_x{ \\underbrace{ p_n(x) r \\big( \\hat{f}(x) - \\log p_n(x) \\big) }_D \\eta(x) dx } \\\\ \u0026amp; = \\int_x{(C-D)\\eta(x)dx} = 0 \\quad \\forall \\eta(x) \\end{aligned} \\end{equation}\nThe equality in eq.(8) happend if and only if \\(C=D\\). This result easily leads to \\(\\hat{f}(x) = \\log p_d(x)\\).\nReferences  Noise-contrastive estimation: A new estimation principle for unnormalized statistical models  ","permalink":"https://iamtu.dev/posts/noise-contrastive-estimation/","summary":"TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \\(T\\) observations \\((x_1, \u0026hellip; x_T)\\) drawn from a true distribution \\(p_d(.)\\). We then try to approximate \\(p_d\\) by a parameterized function \\(p_m(.;\\theta)\\). The estimator \\(\\hat{\\theta}_T\\) is defined to be the \\(\\theta\\) that maximize function\n$$ J_T(\\theta) = \\frac{1}{2T}\\sum_t{\\log[h(x_t; 0)]} + \\log[1-h(y_t; \\theta)] $$","title":"Noise constrastive estimation"},{"content":"2024: Currently looking for PhD Opportunity in the field of AI/ML. My research interest lies in Reinforcement Learning and Physics-Informed Machine Learning.\n2022: I am 29. My undergrad was Economics at a local university, and I am going back to school so I can pursue higher education in the field of Machine Learning \u0026amp; Artificial Intelligence.\n","permalink":"https://iamtu.dev/about/","summary":"2024: Currently looking for PhD Opportunity in the field of AI/ML. My research interest lies in Reinforcement Learning and Physics-Informed Machine Learning.\n2022: I am 29. My undergrad was Economics at a local university, and I am going back to school so I can pursue higher education in the field of Machine Learning \u0026amp; Artificial Intelligence.","title":"About"},{"content":"Notes I took during studying MIT OCW Real Analysis. The class taught by Professor Casey Rodriguez, he also taught Functional analysis.\nResources (Useful link)  Video lecture Course\u0026rsquo;s homepage  Lecture notes Goal of the course - Gain experience with proofs - Prove statements about the real numbers, function and limits\nLecture 1: Sets, Set operations, and Mathematical Induction Definition (Sets) A sets is a collection of objects called elements/members.\nDefinition (Empty set) A set with no elements, denoted as \\(\\emptyset\\)\nNotation\n \\(a \\in S\\): \\(a\\) is a element of \\(S\\) \\(a \\notin S\\): \\(a\\) is not a element of \\(S\\) \\(\\forall\\): for all \\(\\exists\\): there exists \\(\\implies\\): implies \\(\\iff\\): if and only if  Definition\n (subset) A set \\(A\\) is a subset of \\(B\\), denoted as \\(A \\subset B\\) if: \\(a \\in A \\implies a \\in B\\) (equal) Two sets are equal if \\(A \\subset B \\land B \\subset A\\) (proper subset) \\(A \\subsetneqq B \\iff A \\subset B \\land A \\neq B\\)  Set building notation\n$$ \\{ x \\in A : P(x) \\} $$\nExamples\n \\(\\mathbb{N} = \\{1, 2, 3 \\cdots\\}\\) \\(\\mathbb{Z} = \\{\\cdots,-2, -1, 0, 1, 2, \\cdots\\}\\) \\(\\mathbb{Q} = \\{\\frac{m}{n}: m, n \\in \\mathbb{Z}\\}\\) Real number set \\(\\mathbb{R}\\)  Remark: \\(\\mathbb{N} \\subset \\mathbb{Z} \\subset \\mathbb{Q} \\subset \\mathbb{R}\\)\nGoal: describe the real number set $\\mathbb{R}$\nDefinition (union) The union of A and B is the set $$ A \\cup B := {x: x\\in A \\lor x\\in B} $$ Definition (intersection) The intersection of A and B is the set $$ A \\cap B := {x: x\\in A \\land x \\in B} $$ Definition (different) The set different between A w.r.t B is the set $$ A\\backslash B = {x\\in A: x\\notin B} $$ Definition (complement) A complement of set A is the set $$ A^c = {x: x\\notin A} $$ Definition (disjoint) Two sets A and B are disjoint if \\(A \\cap B = \\emptyset\\).\nTheorem (De-Morgan) If A, B, C are sets then\n \\((B \\cup C)^c = B^c \\cap C^c\\) \\((B \\cap C)^c = B^c \\cup C^c\\) \\(A\\backslash (B\\cup C) = (A\\backslash B) \\cap (A\\backslash C)\\) \\(A\\backslash (B\\cap C) = (A\\backslash B) \\cup(A\\backslash C)\\)  Induction A way to prove theorem about natural number.\n\\(\\mathbb{N} = \\{1, 2, 3, \\cdots \\}\\) has an ordering \\(1 \u0026lt; 2 \u0026lt; 3\u0026lt; 4 \u0026lt; \\cdots\\)\nAxiom (Well ordering of natural numbers) if \\(S\\subset \\mathbb{N}\\) and \\(S\\neq \\emptyset\\) has a least element \\(\\exists x\\in S\\) st \\(\\forall y \\in S: x\\leq y\\).\nTheorem (Induction) Let \\(P(n)\\) be a statement depending on \\(n\\in \\mathbb{N}\\). Assume:\n (Base case) \\(P(1)\\) is true (Inductive step) If \\(P(m)\\) is true, then \\(P(m+1)\\) is true. Then \\(P(n)\\) is true for all \\(n\\in \\mathbb{N}\\).  Proof: Let \\(S = {n\\in\\mathbb{N}: P(n) \\text{ is not true}}\\). Want to show \\(S=\\emptyset\\)\n  Suppose \\(S\\neq \\emptyset\\). By WOP.\\(\\mathbb{N}\\), \\(S\\) has a least element \\(x\\in S\\). Since \\(P(1)\\) is true, \\(1\\notin S\\), so \\(x\u0026gt;1\\).\n  Since \\(x\\) is the least element in \\(S\\) \\(\\implies x-1 \\notin S\\).\n  By the definition of \\(S\\), \\(P(x-1)\\) is true, by 2) \\(\\implies P(x)\\) is true \\(\\implies x \\notin S\\).\n\\(\\therefore S = \\emptyset\\)\nUsing induction We want to prove some statement \\(\\forall n\\in\\mathbb{N}:P(n)\\) is true, we have to do two things:\n Prove \\(P(1)\\). Prove \\(P(m) \\implies P(m+1)\\)  Example For all \\(c\\neq 1, \\forall n\\in\\mathbb{N}\\):\n$$ 1 + c + c^2 + \\cdots +c^n=\\frac{1-c^{n+1}}{1-c} $$\nProof\n (Base case):  $$ 1 + c^1 = \\frac{1-c^{1+1}}{1-c}=\\frac{(1-c)(1+c)}{1-c} = {1+c}; \\forall c\\neq1 $$\n(Inductive step) Assume: $$ 1+c+c^2+\\cdots+c^m=\\frac{1-c^{m+1}}{1-c}\\quad(*) $$ We want to show $$ 1+c+c^2+\\cdots+c^n=\\frac{1-c^{n+1}}{1-c}\\quad(**) $$ for \\(n = m+1\\).  We have:\n$$ \\begin{aligned} 1+c+c^2+\\cdots+c^m+c^{m+1} \u0026amp;= \\frac{1-c^{m+1}}{1-c}+c^{m+1} \\\\ \u0026amp; = \\frac{1-c^{m+1}+c^{m+1}-c^{m+2}}{1-c}\\\\ \u0026amp; = \\frac{1-c^{(m+1)+1}}{1-c} \\end{aligned} $$ So (*) hold for \\(n=m+1\\). By induction, \\(P(n)\\) is true \\(\\forall n\\in\\mathbb{N}\\).\n","permalink":"https://iamtu.dev/posts/real-analysis/","summary":"Notes I took during studying MIT OCW Real Analysis. The class taught by Professor Casey Rodriguez, he also taught Functional analysis.\nResources (Useful link)  Video lecture Course\u0026rsquo;s homepage  Lecture notes Goal of the course - Gain experience with proofs - Prove statements about the real numbers, function and limits\nLecture 1: Sets, Set operations, and Mathematical Induction Definition (Sets) A sets is a collection of objects called elements/members.","title":"Real Analysis - Lecture notes"}]