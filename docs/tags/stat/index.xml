<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Stat on iamtu</title><link>https://iamtu.dev/tags/stat/</link><description>Recent content in Stat on iamtu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 12 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://iamtu.dev/tags/stat/index.xml" rel="self" type="application/rss+xml"/><item><title>Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution</title><link>https://iamtu.dev/posts/closed-form-kl-gaussian/</link><pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/closed-form-kl-gaussian/</guid><description>&lt;p>The closed form of KL divergence used in Variational Auto Encoder.&lt;/p>
&lt;h2 id="univariate-case">Univariate case&lt;/h2>
&lt;p>Let&lt;/p>
&lt;ul>
&lt;li>\(p(x) = \mathcal{N}(\mu_1, \sigma_1) = (2\pi\sigma_1^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_1^2}(x-\mu_1)^2]\)&lt;/li>
&lt;li>\(q(x) = \mathcal{N}(\mu_1, \sigma_2) = (2\pi\sigma_2^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_2^2}(x-\mu_2)^2]\)&lt;/li>
&lt;/ul>
&lt;p>KL divergence between \(p\) and \(q\) is defined as:&lt;/p>
&lt;p>$$
\begin{aligned}
\text{KL}(p\parallel q) &amp;amp;= -\int_{x}{p(x)\log{\frac{q(x)}{p(x)}}dx} \\
&amp;amp;= -\int_x p(x) [\log{q(x)} - \log{p(x)}]dx \\
&amp;amp;=
\underbrace{
\int_x{p(x)\log p(x) dx}}_A
- \underbrace{
\int_x{p(x)\log q(x) dx}}_B
\end{aligned}
$$&lt;/p>
&lt;p>First quantity \(A\):&lt;/p>
&lt;p>$$
\begin{aligned}
A &amp;amp;= \int_x{p(x)\log p(x) dx} \\
&amp;amp;= \int_x{p(x)\big[
-\frac{1}{2}\log{2\pi\sigma_1^2
- \frac{1}{2\sigma_1^2}(x - \mu_1)^2}
\big]dx}\\
&amp;amp;= -\frac{1}{2}\log{2\pi\sigma_1^2}\int_x{p(x)dx}
- \frac{1}{2\sigma_1^2}
\underbrace{\int_x{p(x)(x-\mu_1)^2dx}}_{\text{var(x)}}\\
&amp;amp;= -\frac{1}{2}\log{2\pi} - \log\sigma_1-\frac{1}{2}
\end{aligned}
$$&lt;/p></description></item></channel></rss>