<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Learning on iamtu</title><link>https://iamtu.dev/tags/learning/</link><description>Recent content in Learning on iamtu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Thu, 16 Nov 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://iamtu.dev/tags/learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Differentiation under integral sign</title><link>https://iamtu.dev/posts/diff-under-integral-sign/</link><pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/diff-under-integral-sign/</guid><description>&lt;h2 id="motivating-example">Motivating example&lt;/h2>
&lt;p>Evaluating following integral&lt;/p>
&lt;p>$$
I = \int_0^1{\frac{1 - x^2}{\ln{x}}dx}
$$&lt;/p>
&lt;h3 id="closed-form-results">Closed-form results&lt;/h3>
&lt;p>$$
\begin{equation}
\begin{aligned}
F(t) &amp;amp;= \int_0^1{\frac{1-x^t}{\ln(x)}dx} \\
\implies \frac{d}{dt}F &amp;amp;= \frac{d}{dt}\int_0^1{\frac{1-x^t}{\ln(x)}dx}\\
&amp;amp;= \int_0^1{
\frac{\partial}{\partial t}
\frac{1-x^t}{\ln(x)}dx
}\\
&amp;amp;= \int_0^1{
\frac{-\ln(x)x^t}{ln(x)}
dx} \\
&amp;amp;= \bigg[-\frac{x^{t+1}}{t+1}\bigg]_0^1\\
&amp;amp;= -\frac{1}{t+1}\\
\implies F(t) &amp;amp;= -\ln({t+1}) \\
\implies I &amp;amp;= f(2) = -\ln3
\end{aligned}
\end{equation}
$$&lt;/p>
&lt;h3 id="numerical-approximation">Numerical approximation&lt;/h3>
&lt;p>&lt;img loading="lazy" src="https://iamtu.dev/images/mcmc_integral.png" alt="image" />
&lt;/p>
&lt;p>&lt;details >
&lt;summary markdown="span">Code to produce the figure&lt;/summary>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">matplotlib&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">pyplot&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">I&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">g&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vG&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vectorize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">uniform&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">10000&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">vG&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>
&lt;/details>&lt;/p></description></item><item><title>Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution</title><link>https://iamtu.dev/posts/closed-form-kl-gaussian/</link><pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/closed-form-kl-gaussian/</guid><description>&lt;p>The closed form of KL divergence used in Variational Auto Encoder.&lt;/p>
&lt;h2 id="univariate-case">Univariate case&lt;/h2>
&lt;p>Let&lt;/p>
&lt;ul>
&lt;li>\(p(x) = \mathcal{N}(\mu_1, \sigma_1) = (2\pi\sigma_1^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_1^2}(x-\mu_1)^2]\)&lt;/li>
&lt;li>\(q(x) = \mathcal{N}(\mu_1, \sigma_2) = (2\pi\sigma_2^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_2^2}(x-\mu_2)^2]\)&lt;/li>
&lt;/ul>
&lt;p>KL divergence between \(p\) and \(q\) is defined as:&lt;/p>
&lt;p>$$
\begin{aligned}
\text{KL}(p\parallel q) &amp;amp;= -\int_{x}{p(x)\log{\frac{q(x)}{p(x)}}dx} \\
&amp;amp;= -\int_x p(x) [\log{q(x)} - \log{p(x)}]dx \\
&amp;amp;=
\underbrace{
\int_x{p(x)\log p(x) dx}}_A
- \underbrace{
\int_x{p(x)\log q(x) dx}}_B
\end{aligned}
$$&lt;/p>
&lt;p>First quantity \(A\):&lt;/p>
&lt;p>$$
\begin{aligned}
A &amp;amp;= \int_x{p(x)\log p(x) dx} \\
&amp;amp;= \int_x{p(x)\big[
-\frac{1}{2}\log{2\pi\sigma_1^2
- \frac{1}{2\sigma_1^2}(x - \mu_1)^2}
\big]dx}\\
&amp;amp;= -\frac{1}{2}\log{2\pi\sigma_1^2}\int_x{p(x)dx}
- \frac{1}{2\sigma_1^2}
\underbrace{\int_x{p(x)(x-\mu_1)^2dx}}_{\text{var(x)}}\\
&amp;amp;= -\frac{1}{2}\log{2\pi} - \log\sigma_1-\frac{1}{2}
\end{aligned}
$$&lt;/p></description></item><item><title>Miscellanous</title><link>https://iamtu.dev/posts/miscellanous/</link><pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/miscellanous/</guid><description>&lt;h2 id="convergence-in-probability">Convergence in probability&lt;/h2>
&lt;p>&lt;strong>Definition&lt;/strong> a sequence $\{X_n\}$ of random variable converges in probability towards the random variable $X$ if for all $\epsilon &amp;gt; 0$&lt;/p>
&lt;p>$$
\lim_{n \leftarrow \infty} \mathbb{P}(|X_n - X| &amp;gt; \epsilon) = 0
$$&lt;/p>
&lt;h2 id="consistent-estimator">Consistent estimator&lt;/h2>
&lt;h2 id="a-statistic-singular">A statistic (singular)&lt;/h2>
&lt;p>is any quantity computed from values in a sample which is considered fro a statistical purpose (estimating population parameter, describing sample, evaluating hypothesis).&lt;/p>
&lt;h2 id="sufficient-statistics">Sufficient statistics&lt;/h2>
&lt;p>A statistic is sufficient with respect to a statisticcal model and its associated unknown parameter if no other statistic can be&lt;/p></description></item><item><title>Noise constrastive estimation</title><link>https://iamtu.dev/posts/noise-contrastive-estimation/</link><pubDate>Sat, 23 Sep 2023 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/noise-contrastive-estimation/</guid><description>&lt;h2 id="tldr">TLDR&lt;/h2>
&lt;p>The &lt;a href="https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">paper&lt;/a> proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \(T\) observations \((x_1, &amp;hellip; x_T)\) drawn from a true distribution \(p_d(.)\). We then try to approximate \(p_d\) by a parameterized function \(p_m(.;\theta)\). The estimator \(\hat{\theta}_T\) is defined to be the \(\theta\) that maximize function&lt;/p>
&lt;p>$$
J_T(\theta) = \frac{1}{2T}\sum_t{\log[h(x_t; 0)]} + \log[1-h(y_t; \theta)]
$$&lt;/p></description></item><item><title>Real Analysis - Lecture notes</title><link>https://iamtu.dev/posts/real-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/real-analysis/</guid><description>&lt;p>Notes I took during studying MIT OCW &lt;a href="https://ocw.mit.edu/courses/18-100a-real-analysis-fall-2020/">Real Analysis&lt;/a>. The class taught by Professor Casey Rodriguez, he also taught &lt;a href="https://tarheels.live/crodriguez/168-2/">Functional analysis&lt;/a>.&lt;/p>
&lt;h2 id="resources-useful-link">Resources (Useful link)&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=LY7YmuDbuW0&amp;amp;list=PLUl4u3cNGP61O7HkcF7UImpM0cR_L2gSw">Video lecture&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ocw.mit.edu/courses/18-100a-real-analysis-fall-2020/">Course&amp;rsquo;s homepage&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="lecture-notes">Lecture notes&lt;/h2>
&lt;p>Goal of the course
- Gain experience with proofs
- Prove statements about the real numbers, function and limits&lt;/p>
&lt;h3 id="lecture-1-sets-set-operations-and-mathematical-induction">Lecture 1: Sets, Set operations, and Mathematical Induction&lt;/h3>
&lt;p>&lt;strong>Definition&lt;/strong> (Sets) A sets is a collection of objects called elements/members.&lt;/p>
&lt;p>&lt;strong>Definition&lt;/strong> (Empty set) A set with no elements, denoted as \(\emptyset\)&lt;/p></description></item></channel></rss>