<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Learn on iamtu</title><link>https://iamtu.dev/tags/learn/</link><description>Recent content in Learn on iamtu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 24 Sep 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://iamtu.dev/tags/learn/index.xml" rel="self" type="application/rss+xml"/><item><title>My calculus of variations crash course</title><link>https://iamtu.dev/posts/variational_calculus/</link><pubDate>Tue, 24 Sep 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/variational_calculus/</guid><description>My goal for this post is to have a basic understanding of Calculus of Variations, so that I can be more comfortable with mathematics in NeuralODE paper, where the problem can be formulated as a optimization of a functional with ODE constraint (Adjoint State Method for an ODE).
My first encounter with Calculus of Variation is one of my homework where we try to derive probablity density function of some distribution by the principle of maximum entropy.</description></item><item><title>Learning to solve heat equation</title><link>https://iamtu.dev/posts/heat/</link><pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/heat/</guid><description>Surveying numerical methods (finite difference methods) and physics-informed neural networks to solve a 1D heat equation. This post was heavily inspired by:
(Book) Partial Differential Equations for Scientists and Engineers - Standley J. Farlow for deriving closed-form solution. (Article) Finite-Difference Approximations to the Heat Equation (Course) ETH Zurich | Deep Learning for Scientific Computing 2023 for Theory and Implementation of Physics-Informed Neural Network. Introduction Physics-Informed Machine Learning (PIML) is an exciting subfield of Machine Learning that aims to incorporate physical laws and/or constraints into statistical machine learning.</description></item><item><title>Expectation Maximization - EM</title><link>https://iamtu.dev/posts/em/</link><pubDate>Wed, 08 May 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/em/</guid><description>Problem Given a statistical model \(P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})\), which generate set of observations \(\boldsymbol{X}\), where \(\boldsymbol{Z}\) is a latent variable and unknow parameter vector \(\boldsymbol{\theta}\). The goal is to find \(\boldsymbol{\theta}\) that maximize the marginal likelihood:
$$ \mathcal{L}(\boldsymbol{\theta}; \boldsymbol{X}) = P(\boldsymbol{X} | \boldsymbol{\theta}) = \int_{\boldsymbol{Z}}P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})d\boldsymbol{Z} $$
As an example for this type of problem, there are two (unfair) coin A and B with probability of head for each coin is \(p_A(H) = p \text{ and } p_B(H) = q\).</description></item></channel></rss>