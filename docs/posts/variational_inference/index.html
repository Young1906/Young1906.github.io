<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding Variational Inference | iamtu</title>
<meta name=keywords content="variational-inference"><meta name=description content="This post is a note I take from while reading Blei et al 2018.
Goal:
Motivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \(q(z)\) to approximate \(p(z|x)\)
The KL-divergence
$$ \begin{equation} \begin{aligned} KL[q(z)||p(z | x)] &= \int_z{q(z)\log{\frac{p(z|x)}{q(z)}} dz} \end{aligned} \end{equation} $$
However, this quantity is intractable to compute hence, we&rsquo;re unable to optimize this quantity directly."><meta name=author content="Tu T. Do"><link rel=canonical href=https://iamtu.dev/posts/variational_inference/><meta name=google-site-verification content="G-PWLR4FLELZ"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://iamtu.dev/posts/variational_inference/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PWLR4FLELZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PWLR4FLELZ")}</script><meta property="og:title" content="Understanding Variational Inference"><meta property="og:description" content="This post is a note I take from while reading Blei et al 2018.
Goal:
Motivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \(q(z)\) to approximate \(p(z|x)\)
The KL-divergence
$$ \begin{equation} \begin{aligned} KL[q(z)||p(z | x)] &= \int_z{q(z)\log{\frac{p(z|x)}{q(z)}} dz} \end{aligned} \end{equation} $$
However, this quantity is intractable to compute hence, we&rsquo;re unable to optimize this quantity directly."><meta property="og:type" content="article"><meta property="og:url" content="https://iamtu.dev/posts/variational_inference/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-02-29T00:00:00+00:00"><meta property="article:modified_time" content="2024-02-29T00:00:00+00:00"><meta property="og:site_name" content="iamtu"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding Variational Inference"><meta name=twitter:description content="This post is a note I take from while reading Blei et al 2018.
Goal:
Motivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \(q(z)\) to approximate \(p(z|x)\)
The KL-divergence
$$ \begin{equation} \begin{aligned} KL[q(z)||p(z | x)] &= \int_z{q(z)\log{\frac{p(z|x)}{q(z)}} dz} \end{aligned} \end{equation} $$
However, this quantity is intractable to compute hence, we&rsquo;re unable to optimize this quantity directly."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://iamtu.dev/posts/"},{"@type":"ListItem","position":2,"name":"Understanding Variational Inference","item":"https://iamtu.dev/posts/variational_inference/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding Variational Inference","name":"Understanding Variational Inference","description":"This post is a note I take from while reading Blei et al 2018.\nGoal:\nMotivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \\(q(z)\\) to approximate \\(p(z|x)\\)\nThe KL-divergence\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026amp;= \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\end{aligned} \\end{equation} $$\nHowever, this quantity is intractable to compute hence, we\u0026rsquo;re unable to optimize this quantity directly.","keywords":["variational-inference"],"articleBody":"This post is a note I take from while reading Blei et al 2018.\nGoal:\nMotivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \\(q(z)\\) to approximate \\(p(z|x)\\)\nThe KL-divergence\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026= \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\end{aligned} \\end{equation} $$\nHowever, this quantity is intractable to compute hence, we’re unable to optimize this quantity directly.\n$$ \\begin{equation} \\begin{aligned} KL[q(z)||p(z | x)] \u0026= - \\int_z{q(z)\\log{\\frac{p(z|x)}{q(z)}} dz} \\\\ \u0026= -\\int_z{ q(z) \\log { \\frac{\\log p(z, x)}{q(z) p(x)} } }\\\\ \u0026= -\\int_z{q(z)[\\log{\\frac{p(z,x)}{q(z)}} - \\log p(x)]dz} \\\\ \u0026= -\\int_z{ q(z) \\log \\frac{p(z, x)}{q(z)}dz } + \\int_z{q(z)\\log p(x) dz} \\\\ \u0026 =: -\\texttt{ELBO}[q] + \\log p(x) \\\\ \\iff \\texttt{ELBO}[q] \u0026= -KL(q||p) + \\log p(x) \\end{aligned} \\end{equation} $$\nBecause \\(\\log p(x)\\) is a constant, by maximizing \\(\\text{ELBO}[q]\\), we minimize \\(KL(q||p)\\) by proxy. Rewrite ELBO:\n$$ \\begin{equation} \\begin{aligned} \\texttt{ELBO}(q) \u0026= \\int_z{q(z)\\log \\frac{p(z, x)}{q(z)}} \\\\ \u0026= \\mathbb{E}_{z\\sim q}[\\log p(z, x)] - \\mathbb{E}_{z\\sim q}[\\log q(z)] \\end{aligned} \\end{equation} $$\nMean field Variational Family Mean-field variational family made a strong assumption of independence between it’s latent variable\n$$ q(\\mathbf{z}) = \\prod_{j} {q_j(z_j)} $$\nCoordinate ascent variational inference is a common method to solve mean-field variational inference problem. Holding other latent variable fixed, the \\(j^{th}\\) latent variable is given by:\n$$ q^*_{j}(z_j) = \\text{exp}{\\mathbb{E}_{-j}[\\log p(z_j | z_{-j}, \\mathbf{x})]} \\propto \\exp{\\mathbb{E}_{-j} [\\log p(z_j, z_{-j}, \\mathbf{x})]} $$\nProof $$ \\begin{equation} \\begin{aligned} q^*_j(z_j) \u0026= \\texttt{arg}\\max_{q_j(z_j)} \\quad{\\texttt{ELBO}(q)} \\\\ \u0026= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_q[\\log p(z_j, z_{-j}, x)] - \\mathbb{E}_q[\\log q(z_j, z_{-j})] \\\\ \u0026= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log q_j(z_j) + \\log q_{-j}(z_{-j})]] \\\\ \u0026= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] + const \\\\ \u0026= \\texttt{arg}\\max_{q_j(z_j)} \\quad \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] \\end{aligned} \\end{equation} $$\nWe need to find function \\(q_j(z_j)\\) that maximize \\(\\text{ELBO}(q)\\)\nAssuming \\(q_j(z_j)= \\epsilon \\eta(z_j) + q^*_j(z_j)\\)\n$$ \\begin{aligned} K(\\epsilon) \u0026= \\mathbb{E}_j[\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]] - \\mathbb{E}_j[\\log q_j(z_j)] \\\\ \u0026= \\int_{z_j} q_j(z_j) A d_{z_j} - \\int_{z_j}q_j(z_j)\\log q_z(z_j) d_{z_j} \\\\ \u0026= \\int_{z_j} [\\epsilon \\eta(z_j) + q^*_j(z_j)] A d_{z_j} - \\int_{z_j}[\\epsilon \\eta(z_j) + q^*_j(z_j)] \\log [\\epsilon \\eta(z_j) + q^*_j(z_j)] d_{z_j} \\end{aligned} $$\nEvaluate the partial derivative of \\(K\\) wrt \\(\\epsilon\\) we have:\n$$ \\begin{aligned} \u0026 \\frac{\\partial}{\\partial \\epsilon}K \\bigg\\vert_{\\epsilon=0} = 0 \\\\ \\iff \u0026 \\int_{z_j} {\\eta(z_j) A d_{z_j}} - \\int_{z_j} { {\\eta(z_j) \\log [\\epsilon \\eta(z_j) + q^*_j(z_j)]} + [\\epsilon \\eta(z_j) + q^*_j(z_j)] \\frac{\\eta(z_j)}{\\epsilon \\eta(z_j) + q^*_j(z_j)}d_{z_j} } = 0\\\\ \\iff \u0026 \\int_{z_j} {\\eta(z_j) A d_{z_j}} - \\int_{z_j}{[\\eta(z_j)\\log q^*_j(z_j) +\\eta(z_j)]d_{z_j}} = 0; \\quad \\forall \\eta(z_j) \\\\ \\iff \u0026 \\log q^*_j(z_j) = A-1 = \\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)] - 1 \\\\ \\iff \u0026 q^*_j(z_j) \\propto \\exp{\\mathbb{E}_{-j}[\\log p(z_j, z_{-j}, x)]} \\end{aligned} $$\nComplete example of Bayesian Gaussian Mixture TDB\n","wordCount":"451","inLanguage":"en","datePublished":"2024-02-29T00:00:00Z","dateModified":"2024-02-29T00:00:00Z","author":{"@type":"Person","name":"Tu T. Do"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://iamtu.dev/posts/variational_inference/"},"publisher":{"@type":"Organization","name":"iamtu","logo":{"@type":"ImageObject","url":"https://iamtu.dev/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://iamtu.dev/ accesskey=h title="iamtu (Alt + H)">iamtu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://iamtu.dev/ title=home><span>home</span></a></li><li><a href=https://iamtu.dev/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://iamtu.dev/categories/ title=categories><span>categories</span></a></li><li><a href=https://iamtu.dev/about/ title=about><span>about</span></a></li><li><a href=https://iamtu.dev/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://iamtu.dev/>Home</a>&nbsp;»&nbsp;<a href=https://iamtu.dev/posts/>Posts</a></div><h1 class=post-title>Understanding Variational Inference</h1><div class=post-meta><span title='2024-02-29 00:00:00 +0000 UTC'>February 29, 2024</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;451 words&nbsp;·&nbsp;Tu T. Do</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#elbo aria-label=ELBO>ELBO</a></li><li><a href=#mean-field-variational-family aria-label="Mean field Variational Family">Mean field Variational Family</a><ul><li><a href=#proof aria-label=Proof>Proof</a></li></ul></li><li><a href=#complete-example-of-bayesian-gaussian-mixture aria-label="Complete example of Bayesian Gaussian Mixture">Complete example of Bayesian Gaussian Mixture</a></li></ul></div></details></div><div class=post-content><p>This post is a note I take from while reading <a href>Blei et al 2018</a>.</p><p>Goal:</p><ul><li>Motivation of variational inference</li><li>Understand the derivation of ELBO and its intiution</li><li>Walk through the derivation, some of which was skip the in original paper</li><li>Implementation of CAVI</li></ul><h2 id=elbo>ELBO<a hidden class=anchor aria-hidden=true href=#elbo>#</a></h2><p>Goal is to find \(q(z)\) to approximate \(p(z|x)\)</p><p>The KL-divergence</p><p>$$
\begin{equation}
\begin{aligned}
KL[q(z)||p(z | x)] &=
\int_z{q(z)\log{\frac{p(z|x)}{q(z)}} dz}
\end{aligned}
\end{equation}
$$</p><p>However, this quantity is intractable to compute hence, we&rsquo;re unable to optimize this quantity directly.</p><p>$$
\begin{equation}
\begin{aligned}
KL[q(z)||p(z | x)] &=
- \int_z{q(z)\log{\frac{p(z|x)}{q(z)}} dz} \\
&= -\int_z{
q(z) \log {
\frac{\log p(z, x)}{q(z) p(x)}
}
}\\
&= -\int_z{q(z)[\log{\frac{p(z,x)}{q(z)}} - \log p(x)]dz} \\
&= -\int_z{
q(z) \log \frac{p(z, x)}{q(z)}dz
} + \int_z{q(z)\log p(x) dz} \\
& =: -\texttt{ELBO}[q] + \log p(x) \\
\iff \texttt{ELBO}[q] &= -KL(q||p) + \log p(x)
\end{aligned}
\end{equation}
$$</p><p>Because \(\log p(x)\) is a constant, by maximizing \(\text{ELBO}[q]\), we minimize \(KL(q||p)\) by proxy. Rewrite ELBO:</p><p>$$
\begin{equation}
\begin{aligned}
\texttt{ELBO}(q) &= \int_z{q(z)\log \frac{p(z, x)}{q(z)}} \\
&= \mathbb{E}_{z\sim q}[\log p(z, x)] - \mathbb{E}_{z\sim q}[\log q(z)]
\end{aligned}
\end{equation}
$$</p><h2 id=mean-field-variational-family>Mean field Variational Family<a hidden class=anchor aria-hidden=true href=#mean-field-variational-family>#</a></h2><p>Mean-field variational family made a strong assumption of independence between it&rsquo;s latent variable</p><p>$$
q(\mathbf{z}) = \prod_{j} {q_j(z_j)}
$$</p><p>Coordinate ascent variational inference is a common method to solve mean-field variational inference problem. Holding other latent variable fixed, the \(j^{th}\) latent variable is given by:</p><p>$$
q^*_{j}(z_j) = \text{exp}{\mathbb{E}_{-j}[\log p(z_j | z_{-j}, \mathbf{x})]} \propto \exp{\mathbb{E}_{-j} [\log p(z_j, z_{-j}, \mathbf{x})]}
$$</p><h3 id=proof>Proof<a hidden class=anchor aria-hidden=true href=#proof>#</a></h3><p>$$
\begin{equation}
\begin{aligned}
q^*_j(z_j) &= \texttt{arg}\max_{q_j(z_j)} \quad{\texttt{ELBO}(q)} \\
&= \texttt{arg}\max_{q_j(z_j)} \quad \mathbb{E}_q[\log p(z_j, z_{-j}, x)] - \mathbb{E}_q[\log q(z_j, z_{-j})] \\
&= \texttt{arg}\max_{q_j(z_j)} \quad \mathbb{E}_j[\mathbb{E}_{-j}[\log p(z_j, z_{-j}, x)]] - \mathbb{E}_j[\mathbb{E}_{-j}[\log q_j(z_j) + \log q_{-j}(z_{-j})]] \\
&= \texttt{arg}\max_{q_j(z_j)} \quad \mathbb{E}_j[\mathbb{E}_{-j}[\log p(z_j, z_{-j}, x)]] - \mathbb{E}_j[\log q_j(z_j)] + const \\
&= \texttt{arg}\max_{q_j(z_j)} \quad \mathbb{E}_j[\mathbb{E}_{-j}[\log p(z_j, z_{-j}, x)]] - \mathbb{E}_j[\log q_j(z_j)]
\end{aligned}
\end{equation}
$$</p><p>We need to find function \(q_j(z_j)\) that maximize \(\text{ELBO}(q)\)</p><p>Assuming \(q_j(z_j)= \epsilon \eta(z_j) + q^*_j(z_j)\)</p><p>$$
\begin{aligned}
K(\epsilon)
&= \mathbb{E}_j[\mathbb{E}_{-j}[\log p(z_j, z_{-j}, x)]] - \mathbb{E}_j[\log q_j(z_j)] \\
&= \int_{z_j} q_j(z_j) A d_{z_j} - \int_{z_j}q_j(z_j)\log q_z(z_j) d_{z_j} \\
&= \int_{z_j} [\epsilon \eta(z_j) + q^*_j(z_j)] A d_{z_j} - \int_{z_j}[\epsilon \eta(z_j) + q^*_j(z_j)] \log [\epsilon \eta(z_j) + q^*_j(z_j)] d_{z_j}
\end{aligned}
$$</p><p>Evaluate the partial derivative of \(K\) wrt \(\epsilon\) we have:</p><p>$$
\begin{aligned}
& \frac{\partial}{\partial \epsilon}K \bigg\vert_{\epsilon=0} = 0 \\
\iff & \int_{z_j} {\eta(z_j) A d_{z_j}} -
\int_{z_j} {
{\eta(z_j) \log [\epsilon \eta(z_j) + q^*_j(z_j)]}
+ [\epsilon \eta(z_j) + q^*_j(z_j)] \frac{\eta(z_j)}{\epsilon \eta(z_j) + q^*_j(z_j)}d_{z_j}
} = 0\\
\iff & \int_{z_j} {\eta(z_j) A d_{z_j}} - \int_{z_j}{[\eta(z_j)\log q^*_j(z_j) +\eta(z_j)]d_{z_j}} = 0; \quad \forall \eta(z_j) \\
\iff & \log q^*_j(z_j) = A-1 = \mathbb{E}_{-j}[\log p(z_j, z_{-j}, x)] - 1 \\
\iff & q^*_j(z_j) \propto \exp{\mathbb{E}_{-j}[\log p(z_j, z_{-j}, x)]}
\end{aligned}
$$</p><h2 id=complete-example-of-bayesian-gaussian-mixture>Complete example of Bayesian Gaussian Mixture<a hidden class=anchor aria-hidden=true href=#complete-example-of-bayesian-gaussian-mixture>#</a></h2><p>TDB</p></div><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://iamtu-dev.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><footer class=post-footer><ul class=post-tags><li><a href=https://iamtu.dev/tags/variational-inference/>Variational-Inference</a></li></ul><nav class=paginav><a class=prev href=https://iamtu.dev/posts/em/><span class=title>« Prev</span><br><span>Expectation Maximization - EM</span>
</a><a class=next href=https://iamtu.dev/posts/diff-under-integral-sign/><span class=title>Next »</span><br><span>Differentiation under integral sign</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://iamtu.dev/>iamtu</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>