<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Understanding Adjoint sensitivity method | iamtu</title>
<meta name=keywords content="learn"><meta name=description content="Problem
$$
\begin{equation}
\begin{aligned}
\text{ODE:}\quad &\begin{cases} \frac{du}{dt} &= f(u, t; \theta) &\text{(Dynamic function)}\\
u(t=0) &= u_0  & \text{(Initial value)}
\end{cases}
\end{aligned}
\end{equation}
$$
Minimize loss functional:
$$
\begin{equation}
\begin{aligned}
\mathcal{J}[\theta] = \int_{0}^T{g(u; \theta)} dt
\end{aligned}
\end{equation}
$$
Formulating as an optimization with ODE constraint:
$$
\begin{equation}
\begin{aligned}
\min_\theta \mathcal{J}(\theta) = \min_\theta \int_0^Tg(u; \theta) dt \\
\text{such that: } \
\frac{du}{dt} - f(u, t;\theta) = 0
\end{aligned}
\end{equation}
$$


Derivation of the adjoint-state
The Lagrangian of the optimization problem"><meta name=author content="Tu T. Do"><link rel=canonical href=https://iamtu.dev/posts/adjoint_state_method/><meta name=google-site-verification content="G-PWLR4FLELZ"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://iamtu.dev/posts/adjoint_state_method/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-PWLR4FLELZ"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PWLR4FLELZ")}</script><meta property="og:title" content="Understanding Adjoint sensitivity method"><meta property="og:description" content="Problem
$$
\begin{equation}
\begin{aligned}
\text{ODE:}\quad &\begin{cases} \frac{du}{dt} &= f(u, t; \theta) &\text{(Dynamic function)}\\
u(t=0) &= u_0  & \text{(Initial value)}
\end{cases}
\end{aligned}
\end{equation}
$$
Minimize loss functional:
$$
\begin{equation}
\begin{aligned}
\mathcal{J}[\theta] = \int_{0}^T{g(u; \theta)} dt
\end{aligned}
\end{equation}
$$
Formulating as an optimization with ODE constraint:
$$
\begin{equation}
\begin{aligned}
\min_\theta \mathcal{J}(\theta) = \min_\theta \int_0^Tg(u; \theta) dt \\
\text{such that: } \
\frac{du}{dt} - f(u, t;\theta) = 0
\end{aligned}
\end{equation}
$$


Derivation of the adjoint-state
The Lagrangian of the optimization problem"><meta property="og:type" content="article"><meta property="og:url" content="https://iamtu.dev/posts/adjoint_state_method/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-10-01T00:00:00+00:00"><meta property="article:modified_time" content="2024-10-01T00:00:00+00:00"><meta property="og:site_name" content="iamtu"><meta name=twitter:card content="summary"><meta name=twitter:title content="Understanding Adjoint sensitivity method"><meta name=twitter:description content="Problem
$$
\begin{equation}
\begin{aligned}
\text{ODE:}\quad &\begin{cases} \frac{du}{dt} &= f(u, t; \theta) &\text{(Dynamic function)}\\
u(t=0) &= u_0  & \text{(Initial value)}
\end{cases}
\end{aligned}
\end{equation}
$$
Minimize loss functional:
$$
\begin{equation}
\begin{aligned}
\mathcal{J}[\theta] = \int_{0}^T{g(u; \theta)} dt
\end{aligned}
\end{equation}
$$
Formulating as an optimization with ODE constraint:
$$
\begin{equation}
\begin{aligned}
\min_\theta \mathcal{J}(\theta) = \min_\theta \int_0^Tg(u; \theta) dt \\
\text{such that: } \
\frac{du}{dt} - f(u, t;\theta) = 0
\end{aligned}
\end{equation}
$$


Derivation of the adjoint-state
The Lagrangian of the optimization problem"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://iamtu.dev/posts/"},{"@type":"ListItem","position":2,"name":"Understanding Adjoint sensitivity method","item":"https://iamtu.dev/posts/adjoint_state_method/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Understanding Adjoint sensitivity method","name":"Understanding Adjoint sensitivity method","description":"Problem $$ \\begin{equation} \\begin{aligned} \\text{ODE:}\\quad \u0026amp;\\begin{cases} \\frac{du}{dt} \u0026amp;= f(u, t; \\theta) \u0026amp;\\text{(Dynamic function)}\\\\ u(t=0) \u0026amp;= u_0 \u0026amp; \\text{(Initial value)} \\end{cases} \\end{aligned} \\end{equation} $$\nMinimize loss functional:\n$$ \\begin{equation} \\begin{aligned} \\mathcal{J}[\\theta] = \\int_{0}^T{g(u; \\theta)} dt \\end{aligned} \\end{equation} $$\nFormulating as an optimization with ODE constraint: $$ \\begin{equation} \\begin{aligned} \\min_\\theta \\mathcal{J}(\\theta) = \\min_\\theta \\int_0^Tg(u; \\theta) dt \\\\ \\text{such that: } \\ \\frac{du}{dt} - f(u, t;\\theta) = 0 \\end{aligned} \\end{equation} $$\nDerivation of the adjoint-state The Lagrangian of the optimization problem\n","keywords":["learn"],"articleBody":"Problem $$ \\begin{equation} \\begin{aligned} \\text{ODE:}\\quad \u0026\\begin{cases} \\frac{du}{dt} \u0026= f(u, t; \\theta) \u0026\\text{(Dynamic function)}\\\\ u(t=0) \u0026= u_0 \u0026 \\text{(Initial value)} \\end{cases} \\end{aligned} \\end{equation} $$\nMinimize loss functional:\n$$ \\begin{equation} \\begin{aligned} \\mathcal{J}[\\theta] = \\int_{0}^T{g(u; \\theta)} dt \\end{aligned} \\end{equation} $$\nFormulating as an optimization with ODE constraint: $$ \\begin{equation} \\begin{aligned} \\min_\\theta \\mathcal{J}(\\theta) = \\min_\\theta \\int_0^Tg(u; \\theta) dt \\\\ \\text{such that: } \\ \\frac{du}{dt} - f(u, t;\\theta) = 0 \\end{aligned} \\end{equation} $$\nDerivation of the adjoint-state The Lagrangian of the optimization problem\n$$ \\begin{equation} \\begin{aligned} \\mathcal{L}(u, \\lambda;\\theta) \u0026 = \\mathcal{J}(\\theta) + \\underbrace{ \\int_0^T{\\lambda^\\top(t)(f - \\frac{du}{dt}) dt} }_{=0 \\text{ due to the ODE}} \\\\ \u0026 = \\int_0^T{ \\big[g(u;\\theta) + \\lambda^\\top(t)(f - \\frac{du}{dt})\\big]dt } \\end{aligned} \\end{equation} $$\nWe need to compute the the total derivative of loss functional with respect to (w.r.t) \\(\\theta\\), differentiating L.H.S and R.H.S w.r.t to \\(\\theta\\):\n$$ \\begin{equation} \\begin{aligned} \\frac{d\\mathcal{L}}{d\\theta} \u0026 = \\frac{d}{d\\theta}\\bigg(\\int_0^T{ \\big(g(u;\\theta) + \\lambda^\\top(t)(f - \\frac{du}{dt})\\big)dt }\\bigg)\\\\ \u0026 = \\int_0^T{ \\frac{d}{d\\theta} \\big(g(u;\\theta) + \\lambda^\\top(t)(f - \\frac{du}{dt})\\big)dt } \\\\ \u0026 = \\int_0^T {\\big( \\frac{\\partial g}{\\partial \\theta} + \\frac{\\partial g}{\\partial u}\\red{\\frac{du}{d\\theta}} + \\lambda^\\top(t)( \\frac{\\partial f}{\\partial \\theta} + \\frac{\\partial f}{\\partial u}\\red{\\frac{du}{d\\theta}} - \\frac{d}{d\\theta}\\frac{du}{dt} ) \\big)dt} \\\\ \u0026 = \\int_0^T{\\big( \\frac{\\partial g}{\\partial\\theta} + \\lambda^\\top(t) \\frac{\\partial f}{\\partial \\theta} + (\\frac{\\partial g}{\\partial u} + \\lambda^\\top(t)\\frac{\\partial f}{\\partial u})\\red{\\frac{du}{d\\theta}} - \\lambda^\\top(t) \\frac{d}{dt}\\red{\\frac{du}{d\\theta}} \\big)dt}\\\\ \u0026 = \\int_0^T{\\big( \\frac{\\partial g}{\\partial\\theta} + \\lambda^\\top(t) \\frac{\\partial f}{\\partial \\theta} + (\\frac{\\partial g}{\\partial u} + \\lambda^\\top(t)\\frac{\\partial f}{\\partial u})\\red{\\frac{du}{d\\theta}} \\underbrace{ - \\lambda^\\top(t) \\frac{d}{dt}\\red{\\frac{du}{d\\theta} }}_{A} \\big)dt} \\end{aligned} \\end{equation} $$\nNote: Initially I understood that bringing the differentiation into the integral is result of Leibniz Integral Rule. However, in the Leibniz Rule \\( dJ/dx = d/dx \\int_a^b{f(x, t)} dt = \\int_a^b{\\partial/\\partial x f(x, t) dt}\\), the total deriviative is changed to partial derivative because \\(x\\) is not a function of \\(t\\) in constrast to our case where \\(u\\) is a function of \\(\\theta\\). Refer to this paper for derivation.\nConsider the integration of term \\(A\\):\n$$ \\begin{equation} \\begin{aligned} \\int_0^T { -\\lambda^\\top(t) \\frac{d}{dt}\\red{\\frac{du}{d\\theta}} dt } \u0026= \\bigg[-\\lambda^\\top(t)\\red{\\frac{du}{d\\theta}}\\bigg]_0^T + \\int_0^T{\\frac{d\\lambda}{dt}^\\top \\red{\\frac{du}{d\\theta}}dt} \\\\ \u0026= \\lambda^\\top(0) \\red{\\frac{du}{d\\theta}}\\big\\vert_{t=0} - \\lambda^\\top(T)\\red{\\frac{du}{d\\theta}}\\big\\vert_{t=T} + \\int_0^T{\\frac{d\\lambda}{dt}^\\top \\red{\\frac{du}{d\\theta}}dt} \\\\ \\end{aligned} \\end{equation} $$\nReplacing result from eq. (6) into eq. (5):\n$$ \\begin{equation} \\begin{aligned} \\frac{d\\mathcal{L}}{d\\theta} \u0026= \\int_0^T {\\big( \\frac{\\partial g}{\\partial\\theta} + \\lambda^\\top(t)\\frac{\\partial f}{\\partial \\theta} + \\underbrace{ (\\frac{\\partial g}{\\partial u} + \\lambda^\\top(t) \\frac{\\partial f}{\\partial u} - \\frac{d\\lambda}{dt}^\\top)\\red{\\frac{du}{d\\theta}}}_{B} \\big)dt} \\\\ \u0026 + \\lambda^\\top(0)\\red{\\frac{du}{d\\theta}}\\big\\vert_{t=0} - \\underbrace{\\lambda^\\top(T)\\red{\\frac{du}{d\\theta}}\\big\\vert_{t=T}}_{C} \\end{aligned} \\end{equation} $$\nBecause the Jacobian \\(\\frac{du}{d\\theta}\\) is computationally expensive, we can choose \\(\\lambda(t)\\) such that \\(A\\) and \\(B\\) vanish from equation (7) and compute \\(\\lambda^\\top(0)\\) by solving terminal value ODE:\nSee section Forward sensitivity method bellow for directly computing \\(\\red{\\frac{du}{d\\theta}}\\)\n$$ \\begin{equation} \\begin{aligned} \u0026 \\begin{cases} \\frac{\\partial g}{\\partial u} + \\lambda^\\top(t) \\frac{\\partial f}{\\partial u} - \\frac{d\\lambda}{dt}^\\top = 0 \\\\ \\lambda^\\top(T) = 0 \\end{cases} \\\\ \\implies \u0026 \\begin{cases} \\frac{d\\lambda}{dt}^\\top = \\frac{\\partial g}{\\partial u} + \\lambda^\\top(t) \\frac{\\partial f}{\\partial u} \\\\ \\lambda^\\top(T) = 0 \\end{cases} \u0026 \\text{\\small(Rearrange)}\\\\ \\implies \u0026 \\begin{cases} \\frac{d\\lambda}{dt} = \\frac{\\partial g}{\\partial u}^\\top + \\frac{\\partial f}{\\partial u}^\\top\\lambda \\\\ \\lambda(T) = 0 \\end{cases} \u0026 \\text{\\small(Transposition both L.H.S and R.H.S)} \\end{aligned} \\end{equation} $$\nThen the gradient of loss functional w.r.t network parameters becames:\n$$ \\begin{equation} \\begin{aligned} \\frac{d\\mathcal{L}}{d\\theta} \u0026= \\int_0^T {\\big( \\frac{\\partial g}{\\partial_\\theta} + \\lambda^\\top(t)\\frac{\\partial f}{\\partial \\theta} \\big)dt} + \\lambda^\\top(0) \\frac{du}{d\\theta}\\big\\vert_{t=0} \\end{aligned} \\end{equation} $$\n\\(\\lambda(t)\\) is called the adjoint-state.\nComputing \\(\\frac{du}{d\\theta}\\big\\vert_{t=0}\\) Computing \\(\\frac{du}{d\\theta}\\big\\vert_{t=0}\\) Forward sensitivity method TBD\nSome examples Wolf and Bunny population dynamic Let \\(b(t)\\) and \\(w(t)\\) be population function of time for bunnies and wolves respectively. The change in population can be described by a coupled ODE\n$$ \\begin{equation} \\begin{aligned} \\text{ODEs: } \u0026 \\begin{cases} \\frac{dr}{dt} = 4 r(t) - 2w(t) \\\\ \\frac{dw}{dt} = r(t) + w(t) \\end{cases} \\\\ \\text{Initial values: } \u0026 r(0) = 100,\\quad t(0) = 2 \\end{aligned} \\end{equation} $$\nLet \\(S(t) = \\begin{bmatrix}b(t)\\\\s(t)\\end{bmatrix}\\) be a vector-valued function of time describe wolves and bunnies population. Eq. (10) can be rewritten as:\n$$ \\begin{equation} \\begin{aligned} \\text{ODE:} \u0026 \\frac{dS}{dt} = \\begin{bmatrix} 4 \u0026 -2 \\\\ 1 \u0026 1 \\end{bmatrix} S(t)\\\\ \\text{Initial value: } \u0026 S(0) = \\begin{bmatrix} 100\\\\ 2 \\end{bmatrix} \\end{aligned} \\end{equation} $$\nAppendix Table of notations\n\\(\\vec{u}(t) \\in \\mathbb{R}^N \\) Solution function to initial value ODE (1) \\(\\theta \\in \\mathbb{R}^P \\) Collection of parameters \\(g(t): \\mathbb{R}^N \\rightarrow \\mathbb{R}\\) Some loss function (i.e, MSE) \\(f(u, t; \\theta)\\\\ f : \\mathbb{R}^{N \\times P} \\rightarrow \\mathbb{R}^N \\) Dynamic function parameterized by \\(\\theta\\), describes the gradient field of state \\(u\\) given its current location \\(\\mathcal{J}: \\mathcal{F}\\rightarrow \\mathbb{R}\\) Loss functional, mapping from loss function onto real number line. References Efficient gradient computation for dynamical models ","wordCount":"722","inLanguage":"en","datePublished":"2024-10-01T00:00:00Z","dateModified":"2024-10-01T00:00:00Z","author":{"@type":"Person","name":"Tu T. Do"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://iamtu.dev/posts/adjoint_state_method/"},"publisher":{"@type":"Organization","name":"iamtu","logo":{"@type":"ImageObject","url":"https://iamtu.dev/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://iamtu.dev/ accesskey=h title="iamtu (Alt + H)">iamtu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=https://iamtu.dev/ title=home><span>home</span></a></li><li><a href=https://iamtu.dev/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://iamtu.dev/categories/ title=categories><span>categories</span></a></li><li><a href=https://iamtu.dev/about/ title=about><span>about</span></a></li><li><a href=https://iamtu.dev/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://iamtu.dev/>Home</a>&nbsp;»&nbsp;<a href=https://iamtu.dev/posts/>Posts</a></div><h1 class=post-title>Understanding Adjoint sensitivity method<sup><span class=entry-isdraft>&nbsp;&nbsp;[draft]</span></sup></h1><div class=post-meta><span title='2024-10-01 00:00:00 +0000 UTC'>October 1, 2024</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;722 words&nbsp;·&nbsp;Tu T. Do</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#problem aria-label=Problem>Problem</a><ul><li><a href=#formulating-as-an-optimization-with-ode-constraint aria-label="Formulating as an optimization with ODE constraint:">Formulating as an optimization with ODE constraint:</a></li></ul></li><li><a href=#derivation-of-the-adjoint-state aria-label="Derivation of the adjoint-state">Derivation of the adjoint-state</a><ul><ul><li><a href=#computing-fracdudthetabigvert_t0 aria-label="Computing \(\frac{du}{d\theta}\big\vert_{t=0}\)">Computing \(\frac{du}{d\theta}\big\vert_{t=0}\)</a></li><li><a href=#computing-fracdudthetabigvert_t0-1 aria-label="Computing \(\frac{du}{d\theta}\big\vert_{t=0}\)">Computing \(\frac{du}{d\theta}\big\vert_{t=0}\)</a></li></ul><li><a href=#forward-sensitivity-method aria-label="Forward sensitivity method">Forward sensitivity method</a></li></ul></li><li><a href=#some-examples aria-label="Some examples">Some examples</a><ul><li><a href=#wolf-and-bunny-population-dynamic aria-label="Wolf and Bunny population dynamic">Wolf and Bunny population dynamic</a></li></ul></li><li><a href=#appendix aria-label=Appendix>Appendix</a></li><li><a href=#references aria-label=References>References</a></li></ul></div></details></div><div class=post-content><h1 id=problem>Problem<a hidden class=anchor aria-hidden=true href=#problem>#</a></h1><p>$$
\begin{equation}
\begin{aligned}
\text{ODE:}\quad &\begin{cases} \frac{du}{dt} &= f(u, t; \theta) &\text{(Dynamic function)}\\
u(t=0) &= u_0 & \text{(Initial value)}
\end{cases}
\end{aligned}
\end{equation}
$$</p><p>Minimize loss functional:</p><p>$$
\begin{equation}
\begin{aligned}
\mathcal{J}[\theta] = \int_{0}^T{g(u; \theta)} dt
\end{aligned}
\end{equation}
$$</p><h2 id=formulating-as-an-optimization-with-ode-constraint>Formulating as an optimization with ODE constraint:<a hidden class=anchor aria-hidden=true href=#formulating-as-an-optimization-with-ode-constraint>#</a></h2><p>$$
\begin{equation}
\begin{aligned}
\min_\theta \mathcal{J}(\theta) = \min_\theta \int_0^Tg(u; \theta) dt \\
\text{such that: } \
\frac{du}{dt} - f(u, t;\theta) = 0
\end{aligned}
\end{equation}
$$</p><h1 id=derivation-of-the-adjoint-state>Derivation of the adjoint-state<a hidden class=anchor aria-hidden=true href=#derivation-of-the-adjoint-state>#</a></h1><p>The Lagrangian of the optimization problem</p><p>$$
\begin{equation}
\begin{aligned}
\mathcal{L}(u, \lambda;\theta) & =
\mathcal{J}(\theta) +
\underbrace{
\int_0^T{\lambda^\top(t)(f - \frac{du}{dt}) dt}
}_{=0 \text{ due to the ODE}} \\
& = \int_0^T{
\big[g(u;\theta) + \lambda^\top(t)(f - \frac{du}{dt})\big]dt
}
\end{aligned}
\end{equation}
$$</p><p>We need to compute the the total derivative of loss functional with respect to (w.r.t) \(\theta\), differentiating <em>L.H.S</em> and <em>R.H.S</em> w.r.t to \(\theta\):</p><p>$$
\begin{equation}
\begin{aligned}
\frac{d\mathcal{L}}{d\theta} & = \frac{d}{d\theta}\bigg(\int_0^T{
\big(g(u;\theta) + \lambda^\top(t)(f - \frac{du}{dt})\big)dt
}\bigg)\\
& = \int_0^T{
\frac{d}{d\theta}
\big(g(u;\theta) + \lambda^\top(t)(f - \frac{du}{dt})\big)dt
} \\
& = \int_0^T {\big(
\frac{\partial g}{\partial \theta} + \frac{\partial g}{\partial u}\red{\frac{du}{d\theta}}
+ \lambda^\top(t)(
\frac{\partial f}{\partial \theta}
+ \frac{\partial f}{\partial u}\red{\frac{du}{d\theta}}
- \frac{d}{d\theta}\frac{du}{dt}
)
\big)dt} \\
& = \int_0^T{\big(
\frac{\partial g}{\partial\theta} + \lambda^\top(t) \frac{\partial f}{\partial \theta}
+ (\frac{\partial g}{\partial u} + \lambda^\top(t)\frac{\partial f}{\partial u})\red{\frac{du}{d\theta}}
- \lambda^\top(t) \frac{d}{dt}\red{\frac{du}{d\theta}}
\big)dt}\\
& = \int_0^T{\big(
\frac{\partial g}{\partial\theta} + \lambda^\top(t) \frac{\partial f}{\partial \theta}
+ (\frac{\partial g}{\partial u} + \lambda^\top(t)\frac{\partial f}{\partial u})\red{\frac{du}{d\theta}}
\underbrace{
- \lambda^\top(t) \frac{d}{dt}\red{\frac{du}{d\theta}
}}_{A}
\big)dt}
\end{aligned}
\end{equation}
$$</p><blockquote><p><strong>Note</strong>: Initially I understood that bringing the differentiation into the integral is result of Leibniz Integral Rule. However, in the Leibniz Rule \( dJ/dx = d/dx \int_a^b{f(x, t)} dt = \int_a^b{\partial/\partial x f(x, t) dt}\), the total deriviative is changed to partial derivative because \(x\) is not a function of \(t\) in constrast to our case where \(u\) is a function of \(\theta\). Refer to <a href=https://www.sciencedirect.com/science/article/pii/S1053811914003097>this paper</a> for derivation.</p></blockquote><p>Consider the integration of term \(A\):</p><p>$$
\begin{equation}
\begin{aligned}
\int_0^T {
-\lambda^\top(t) \frac{d}{dt}\red{\frac{du}{d\theta}} dt
} &= \bigg[-\lambda^\top(t)\red{\frac{du}{d\theta}}\bigg]_0^T + \int_0^T{\frac{d\lambda}{dt}^\top \red{\frac{du}{d\theta}}dt} \\
&= \lambda^\top(0) \red{\frac{du}{d\theta}}\big\vert_{t=0} - \lambda^\top(T)\red{\frac{du}{d\theta}}\big\vert_{t=T} + \int_0^T{\frac{d\lambda}{dt}^\top \red{\frac{du}{d\theta}}dt} \\
\end{aligned}
\end{equation}
$$</p><p>Replacing result from eq. (6) into eq. (5):</p><p>$$
\begin{equation}
\begin{aligned}
\frac{d\mathcal{L}}{d\theta}
&= \int_0^T {\big(
\frac{\partial g}{\partial\theta} + \lambda^\top(t)\frac{\partial f}{\partial \theta}
+ \underbrace{
(\frac{\partial g}{\partial u} + \lambda^\top(t) \frac{\partial f}{\partial u} - \frac{d\lambda}{dt}^\top)\red{\frac{du}{d\theta}}}_{B}
\big)dt} \\
& + \lambda^\top(0)\red{\frac{du}{d\theta}}\big\vert_{t=0} -
\underbrace{\lambda^\top(T)\red{\frac{du}{d\theta}}\big\vert_{t=T}}_{C}
\end{aligned}
\end{equation}
$$</p><p>Because the Jacobian \(\frac{du}{d\theta}\) is computationally expensive, we can choose \(\lambda(t)\) such that \(A\) and \(B\) vanish from equation (7) and compute \(\lambda^\top(0)\) by solving terminal value ODE:</p><blockquote><p>See section <strong>Forward sensitivity method</strong> bellow for directly computing \(\red{\frac{du}{d\theta}}\)</p></blockquote><p>$$
\begin{equation}
\begin{aligned}
& \begin{cases}
\frac{\partial g}{\partial u} + \lambda^\top(t) \frac{\partial f}{\partial u} - \frac{d\lambda}{dt}^\top = 0 \\
\lambda^\top(T) = 0
\end{cases} \\
\implies & \begin{cases}
\frac{d\lambda}{dt}^\top = \frac{\partial g}{\partial u} + \lambda^\top(t) \frac{\partial f}{\partial u} \\
\lambda^\top(T) = 0
\end{cases} & \text{\small(Rearrange)}\\
\implies & \begin{cases}
\frac{d\lambda}{dt} = \frac{\partial g}{\partial u}^\top + \frac{\partial f}{\partial u}^\top\lambda \\
\lambda(T) = 0
\end{cases} & \text{\small(Transposition both L.H.S and R.H.S)}
\end{aligned}
\end{equation}
$$</p><p>Then the gradient of loss functional w.r.t network parameters becames:</p><p>$$
\begin{equation}
\begin{aligned}
\frac{d\mathcal{L}}{d\theta} &= \int_0^T {\big(
\frac{\partial g}{\partial_\theta} + \lambda^\top(t)\frac{\partial f}{\partial \theta}
\big)dt} + \lambda^\top(0) \frac{du}{d\theta}\big\vert_{t=0}
\end{aligned}
\end{equation}
$$</p><p>\(\lambda(t)\) is called the adjoint-state.</p><h3 id=computing-fracdudthetabigvert_t0>Computing \(\frac{du}{d\theta}\big\vert_{t=0}\)<a hidden class=anchor aria-hidden=true href=#computing-fracdudthetabigvert_t0>#</a></h3><h3 id=computing-fracdudthetabigvert_t0-1>Computing \(\frac{du}{d\theta}\big\vert_{t=0}\)<a hidden class=anchor aria-hidden=true href=#computing-fracdudthetabigvert_t0-1>#</a></h3><h2 id=forward-sensitivity-method>Forward sensitivity method<a hidden class=anchor aria-hidden=true href=#forward-sensitivity-method>#</a></h2><p>TBD</p><h1 id=some-examples>Some examples<a hidden class=anchor aria-hidden=true href=#some-examples>#</a></h1><h2 id=wolf-and-bunny-population-dynamic>Wolf and Bunny population dynamic<a hidden class=anchor aria-hidden=true href=#wolf-and-bunny-population-dynamic>#</a></h2><p>Let \(b(t)\) and \(w(t)\) be population function of time for bunnies and wolves respectively. The change in population can be described by a coupled ODE</p><p>$$
\begin{equation}
\begin{aligned}
\text{ODEs: } & \begin{cases}
\frac{dr}{dt} = 4 r(t) - 2w(t) \\
\frac{dw}{dt} = r(t) + w(t)
\end{cases} \\
\text{Initial values: } & r(0) = 100,\quad t(0) = 2
\end{aligned}
\end{equation}
$$</p><p>Let \(S(t) = \begin{bmatrix}b(t)\\s(t)\end{bmatrix}\) be a vector-valued function of time describe wolves and bunnies population. Eq. (10) can be rewritten as:</p><p>$$
\begin{equation}
\begin{aligned}
\text{ODE:} & \frac{dS}{dt} = \begin{bmatrix}
4 & -2 \\
1 & 1
\end{bmatrix} S(t)\\
\text{Initial value: } & S(0) = \begin{bmatrix}
100\\
2
\end{bmatrix}
\end{aligned}
\end{equation}
$$</p><h1 id=appendix>Appendix<a hidden class=anchor aria-hidden=true href=#appendix>#</a></h1><p><strong>Table of notations</strong></p><table><thead><tr><th style=text-align:left></th><th style=text-align:left></th></tr></thead><tbody><tr><td style=text-align:center>\(\vec{u}(t) \in \mathbb{R}^N \)</td><td style=text-align:left>Solution function to initial value ODE (1)</td></tr><tr><td style=text-align:center>\(\theta \in \mathbb{R}^P \)</td><td style=text-align:left>Collection of parameters</td></tr><tr><td style=text-align:center>\(g(t): \mathbb{R}^N \rightarrow \mathbb{R}\)</td><td style=text-align:left>Some loss function (i.e, MSE)</td></tr><tr><td style=text-align:center>\(f(u, t; \theta)\\ f : \mathbb{R}^{N \times P} \rightarrow \mathbb{R}^N \)</td><td style=text-align:left>Dynamic function parameterized by \(\theta\), describes the gradient field of state \(u\) given its current location</td></tr><tr><td style=text-align:center>\(\mathcal{J}: \mathcal{F}\rightarrow \mathbb{R}\)</td><td style=text-align:left>Loss functional, mapping from loss function onto real number line.</td></tr></tbody></table><h1 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h1><ul><li><a href=https://www.sciencedirect.com/science/article/pii/S1053811914003097>Efficient gradient computation for dynamical models</a></li></ul></div><div id=disqus_thread></div><script>(function(){var e=document,t=e.createElement("script");t.src="https://iamtu-dev.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)})()</script><noscript>Please enable JavaScript to view the <a href=https://disqus.com/?ref_noscript>comments powered by Disqus.</a></noscript><footer class=post-footer><ul class=post-tags><li><a href=https://iamtu.dev/tags/learn/>Learn</a></li></ul><nav class=paginav><a class=next href=https://iamtu.dev/posts/ode_solver/><span class=title>Next »</span><br><span>Numerical Integrations</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://iamtu.dev/>iamtu</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>