<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>Noise constrastive estimation | iamtu</title>
<meta name=keywords content="learning,probabilistic-ml">
<meta name=description content="TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \(T\) observations \((x_1, &mldr; x_T)\) drawn from a true distribution \(p_d(.)\). We then try to approximate \(p_d\) by a parameterized function \(p_m(.;\theta)\). The estimator \(\hat{\theta}_T\) is defined to be the \(\theta\) that maximize function
$$ J_T(\theta) = \frac{1}{2T}\sum_t{\log[h(x_t; 0)]} + \log[1-h(y_t; \theta)] $$">
<meta name=author content="Me">
<link rel=canonical href=https://iamtu.dev/posts/noise-contrastive-estimation/>
<meta name=google-site-verification content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<noscript>
<style>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PWLR4FLELZ"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-PWLR4FLELZ',{anonymize_ip:!1})}</script>
<meta property="og:title" content="Noise constrastive estimation">
<meta property="og:description" content="TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \(T\) observations \((x_1, &mldr; x_T)\) drawn from a true distribution \(p_d(.)\). We then try to approximate \(p_d\) by a parameterized function \(p_m(.;\theta)\). The estimator \(\hat{\theta}_T\) is defined to be the \(\theta\) that maximize function
$$ J_T(\theta) = \frac{1}{2T}\sum_t{\log[h(x_t; 0)]} + \log[1-h(y_t; \theta)] $$">
<meta property="og:type" content="article">
<meta property="og:url" content="https://iamtu.dev/posts/noise-contrastive-estimation/"><meta property="article:section" content="posts">
<meta property="article:published_time" content="2023-09-23T00:00:00+00:00">
<meta property="article:modified_time" content="2023-09-23T00:00:00+00:00"><meta property="og:site_name" content="iamtu">
<meta name=twitter:card content="summary">
<meta name=twitter:title content="Noise constrastive estimation">
<meta name=twitter:description content="TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \(T\) observations \((x_1, &mldr; x_T)\) drawn from a true distribution \(p_d(.)\). We then try to approximate \(p_d\) by a parameterized function \(p_m(.;\theta)\). The estimator \(\hat{\theta}_T\) is defined to be the \(\theta\) that maximize function
$$ J_T(\theta) = \frac{1}{2T}\sum_t{\log[h(x_t; 0)]} + \log[1-h(y_t; \theta)] $$">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://iamtu.dev/posts/"},{"@type":"ListItem","position":2,"name":"Noise constrastive estimation","item":"https://iamtu.dev/posts/noise-contrastive-estimation/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Noise constrastive estimation","name":"Noise constrastive estimation","description":"TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \\(T\\) observations \\((x_1, \u0026hellip; x_T)\\) drawn from a true distribution \\(p_d(.)\\). We then try to approximate \\(p_d\\) by a parameterized function \\(p_m(.;\\theta)\\). The estimator \\(\\hat{\\theta}_T\\) is defined to be the \\(\\theta\\) that maximize function\n$$ J_T(\\theta) = \\frac{1}{2T}\\sum_t{\\log[h(x_t; 0)]} + \\log[1-h(y_t; \\theta)] $$","keywords":["learning","probabilistic-ml"],"articleBody":"TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \\(T\\) observations \\((x_1, … x_T)\\) drawn from a true distribution \\(p_d(.)\\). We then try to approximate \\(p_d\\) by a parameterized function \\(p_m(.;\\theta)\\). The estimator \\(\\hat{\\theta}_T\\) is defined to be the \\(\\theta\\) that maximize function\n$$ J_T(\\theta) = \\frac{1}{2T}\\sum_t{\\log[h(x_t; 0)]} + \\log[1-h(y_t; \\theta)] $$\nIn which:\n \\(y=(y_1, …, y_T)\\) be a dataset of \\(T\\) observations draw from a noise density function $p_n(.)$. \\(h(u; \\theta) = 1/(1 + e^{-G(u;\\theta)})\\) \\(G(u; \\theta) = \\log p_m(u; \\theta) - \\log p_n(u)\\)  For \\(p_m(.; \\theta)\\) to be a valid p.d.f, we also need to include unit integral constraint into the optimization problem, that is \\(\\int_x{p_m(x; \\theta)dx} = 1\\). However, this integral is often intractable in most cases, for example, when we use a neural network to parameterize \\(p_m(.;\\theta)\\).\nInterestingly, the paper claims that maximizing the objective function gives a valid p.d.f without placing the unit integral constraint on the optimization (Theorem 1). In this post, I’ll attempt to prove the theorem as an exercise. Note that, I made an assumption that support of $x$ and $y$ are equal (in eq.7); which mean $p_n(.)$ is nonzero whenever $p_d(.)$ is nonzero and $p_n(.)$ is zero everywhere else.\nProof of threorem I When the sample size $T$ becomes arbitrarily large, the objective function $J_T(\\theta)$ converges in probability (this is a new word for me) to $\\tilde{J}$\n\\begin{equation} \\begin{aligned} \\tilde{J}(\\theta) = \\frac{1}{2}\\mathbb{E}_{x, y} { \\log{r\\big(f(x) - \\log{p_n(x)}\\big)} + \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} } \\end{aligned} \\end{equation}\nIn which \\(f(x) = \\log p_m(x; \\theta)\\) is the function approximating log-likelihood the true distribution $p_d(.)$.\nNotation  \\(p_d(x)\\) true probability density function (p.d.f) of data. \\(p_n(x)\\) p.d.f of noise generating distribution. \\(r(x) = \\frac{1}{1+\\exp(-x)}\\) sigmoid function. \\(X = (x_1, … x_T); x \\sim p_d(x)\\) be the dataset of T observations. \\(Y = (y_1, … y_T); y \\sim p_n(y)\\) be the dataset of T artificially generated noise. \\(p_m(.; \\theta)\\) is estimation of \\(p_d(.)\\) parameterized by \\(\\theta\\).  Theorem  \\(\\tilde{J}\\) attains a maximum at \\(f(.) = \\log p_d(.)\\). There are no other extrema if the noise density \\(p_n(.)\\) is chosen such it is nonzero whenever \\(p_d(.)\\) is nonzero.\n Proof Let \\(\\hat{f}(x)\\) be the optimal function that maximizes \\(\\tilde{J}\\), and \\(f(x)=\\hat{f}(x) + \\epsilon\\eta(x)\\).\n\\begin{equation} \\begin{aligned} \\tilde{J}(\\theta) \u0026= K(\\epsilon) \\\\ \u0026= \\frac{1}{2}\\mathbb{E}_{x, y} { \\log{ r\\big(f(x) - \\log{p_n(x)}\\big) } + \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} } \\\\ \u0026= \\frac{1}{2}\\underbrace{ \\mathbb{E}_x { \\log r\\big( f(x) - \\log p_n(x) \\big) } }_A + \\frac{1}{2} \\underbrace{\\mathbb{E}_y { \\log{\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big]} }}_B \\\\ \\implies \\frac{dK}{d\\epsilon} \u0026= \\frac{dA}{d\\epsilon} + \\frac{dB}{d\\epsilon} \\end{aligned} \\end{equation}\nExpand the first term of $K(\\epsilon)$\n\\begin{equation} \\begin{aligned} A(\\epsilon) \u0026= \\mathbb{E}_x { \\log r\\big( f(x) - \\log p_n(x) \\big) } \\\\ \u0026 = \\int_x { p_d(x) \\log{ r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) \\big) } dx } \\end{aligned} \\end{equation}\nTaking derivative of $A(\\epsilon)$\n\\begin{equation} \\begin{aligned} \\frac{dA}{d\\epsilon} \u0026= \\frac{1}{d\\epsilon} \\int_x { p_d(x) \\log{ r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) \\big) } dx } \\\\ \u0026 = \\int_x { p_d(x) \\big[ \\frac{1}{d\\epsilon}\\log{ r \\big( \\underbrace{ \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x) }_{g(\\epsilon)} \\big) } \\big]dx } \\\\ \u0026 = \\int_x{ p_d(x) \\frac{d\\log{r}}{dr} \\frac{dr}{dg} \\frac{dg}{d\\epsilon} dx } \\\\ \u0026 = \\int_x{ p_d(x) \\frac{1}{r} r(1-r) \\eta(x) dx } \\\\ \u0026 = \\int_x{ p_d(x) \\big[1 - r\\big( \\hat{f}(x) + \\epsilon \\eta(x) - \\log p_n(x)\\big) \\big] \\eta(x) dx } \\end{aligned} \\end{equation}\nNow let’s turn our attention to the second term of $K(\\epsilon)$\n\\begin{equation} \\begin{aligned} B(\\epsilon) \u0026= \\mathbb{E}_y { \\log\\big[ 1 - r\\big(f(y) - \\log{p_n(y)}\\big) \\big] } \\\\ \u0026 = \\int_y { p_n(y) \\log \\big[ 1 - r \\big( \\underbrace{ \\hat{f}(y) + \\epsilon \\eta(y) - \\log p_n(y) }_h \\big) \\big]dy } \\end{aligned} \\end{equation}\nTaking derivative of $B$ w.r.t $\\epsilon$\n\\begin{equation} \\begin{aligned} \\frac{dB}{d\\epsilon} \u0026= \\frac{1}{d\\epsilon} \\int_y{ p_n(y)\\log{ \\big[ 1 - r\\big( h(\\epsilon)\\big) \\big] }dy } \\\\ \u0026= \\int_y { p_n(y) \\frac{d\\log(1-r)}{d(1-r)} \\frac{d(1-r)}{dr} \\frac{dr}{dh} \\frac{dh}{d\\epsilon} dy } \\\\ \u0026 = \\int_y { p_n(y) \\frac{1}{1-r} (-1) r(1-r) \\eta(y) } \\\\ \u0026 = -\\int_y{ p_n(y) r\\big( \\hat{f}(y) + \\epsilon \\eta(y) - \\log p_n(y) \\big) \\eta(y) dy } \\end{aligned} \\end{equation}\nSubstitute result from eq(4) and eq(6) to eq(2), $\\frac{dK}{d\\epsilon}$ is evaluated to $0$ at $\\epsilon = 0$.\n\\begin{equation} \\begin{aligned} \\frac{dK}{d\\epsilon}\\big\\vert_{\\epsilon=0} \u0026= \\frac{dA}{d\\epsilon}\\big\\vert_{\\epsilon=0} + \\frac{dB}{d\\epsilon}\\big\\vert_{\\epsilon=0} \\\\ \u0026= \\int_x { p_d(x) \\big[1 - r\\big( \\hat{f}(x) - \\log p_n(x)\\big) \\big] \\eta(x) dx } \\\\ \u0026 - \\int_y{ p_n(y) r \\big( \\hat{f}(y) - \\log p_n(y) \\big) \\eta(y) dy } \\\\ \u0026 = 0 \\end{aligned} \\end{equation}\nConsider eq. (7), if the support for $x$ and $y$ are equal, which mean we integrate $x$ and $y$ over a same region, we can change $y$ to $x$ and rewrite eq.(7) as\n\\begin{equation} \\begin{aligned} \\frac{dK}{d\\epsilon} \\big\\vert_{\\epsilon = 0} \u0026= \\int_x { \\underbrace{ p_d(x) \\big[1 - r\\big( \\hat{f}(x) - \\log p_n(x)\\big) \\big] }_C \\eta(x) dx } \\\\ \u0026 - \\int_x{ \\underbrace{ p_n(x) r \\big( \\hat{f}(x) - \\log p_n(x) \\big) }_D \\eta(x) dx } \\\\ \u0026 = \\int_x{(C-D)\\eta(x)dx} = 0 \\quad \\forall \\eta(x) \\end{aligned} \\end{equation}\nThe equality in eq.(8) happend if and only if $C=D$. This result easily leads to $\\hat{f}(x) = \\log p_d(x)$.\nReferences  Noise-contrastive estimation: A new estimation principle for unnormalized statistical models  ","wordCount":"868","inLanguage":"en","datePublished":"2023-09-23T00:00:00Z","dateModified":"2023-09-23T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://iamtu.dev/posts/noise-contrastive-estimation/"},"publisher":{"@type":"Organization","name":"iamtu","logo":{"@type":"ImageObject","url":"https://iamtu.dev/%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://iamtu.dev/ accesskey=h title="iamtu (Alt + H)">iamtu</a>
<div class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</div>
</div>
<ul id=menu>
<li>
<a href=https://iamtu.dev/ title=home>
<span>home</span>
</a>
</li>
<li>
<a href=https://iamtu.dev/search/ title="search (Alt + /)" accesskey=/>
<span>search</span>
</a>
</li>
<li>
<a href=https://iamtu.dev/categories/ title=categories>
<span>categories</span>
</a>
</li>
<li>
<a href=https://iamtu.dev/tags/ title=tags>
<span>tags</span>
</a>
</li>
<li>
<a href=https://iamtu.dev/archives/ title=archives>
<span>archives</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://iamtu.dev/>Home</a>&nbsp;»&nbsp;<a href=https://iamtu.dev/posts/>Posts</a></div>
<h1 class=post-title>
Noise constrastive estimation
</h1>
</header> <div class=toc>
<details>
<summary accesskey=c title="(Alt + C)">
<span class=details>Table of Contents</span>
</summary>
<div class=inner><ul>
<li>
<a href=#tldr aria-label=TLDR>TLDR</a></li>
<li>
<a href=#proof-of-threorem-i aria-label="Proof of threorem I">Proof of threorem I</a><ul>
<li>
<a href=#notation aria-label=Notation>Notation</a></li>
<li>
<a href=#theorem aria-label=Theorem>Theorem</a></li>
<li>
<a href=#proof aria-label=Proof>Proof</a></li></ul>
</li>
<li>
<a href=#references aria-label=References>References</a>
</li>
</ul>
</div>
</details>
</div>
<div class=post-content><h2 id=tldr>TLDR<a hidden class=anchor aria-hidden=true href=#tldr>#</a></h2>
<p>The <a href=https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf>paper</a> proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \(T\) observations \((x_1, &mldr; x_T)\) drawn from a true distribution \(p_d(.)\). We then try to approximate \(p_d\) by a parameterized function \(p_m(.;\theta)\). The estimator \(\hat{\theta}_T\) is defined to be the \(\theta\) that maximize function</p>
<p>$$
J_T(\theta) = \frac{1}{2T}\sum_t{\log[h(x_t; 0)]} + \log[1-h(y_t; \theta)]
$$</p>
<p>In which:</p>
<ul>
<li>\(y=(y_1, &mldr;, y_T)\) be a dataset of \(T\) observations draw from a noise density function $p_n(.)$.</li>
<li>\(h(u; \theta) = 1/(1 + e^{-G(u;\theta)})\)</li>
<li>\(G(u; \theta) = \log p_m(u; \theta) - \log p_n(u)\)</li>
</ul>
<p>For \(p_m(.; \theta)\) to be a valid p.d.f, we also need to include unit integral constraint into the optimization problem, that is \(\int_x{p_m(x; \theta)dx} = 1\). However, this integral is often intractable in most cases, for example, when we use a neural network to parameterize \(p_m(.;\theta)\).</p>
<p>Interestingly, the paper claims that maximizing the objective function gives a valid p.d.f without placing the unit integral constraint on the optimization (Theorem 1). In this post, I&rsquo;ll attempt to prove the theorem as an exercise. Note that, I made an assumption that support of $x$ and $y$ are equal (in eq.7); which mean $p_n(.)$ is nonzero whenever $p_d(.)$ is nonzero and $p_n(.)$ is zero everywhere else.</p>
<h2 id=proof-of-threorem-i>Proof of threorem I<a hidden class=anchor aria-hidden=true href=#proof-of-threorem-i>#</a></h2>
<p>When the sample size $T$ becomes arbitrarily large, the objective function $J_T(\theta)$ converges in probability (this is a new word for me) to $\tilde{J}$</p>
<p>\begin{equation}
\begin{aligned}
\tilde{J}(\theta) = \frac{1}{2}\mathbb{E}_{x, y} {
\log{r\big(f(x) - \log{p_n(x)}\big)}
+ \log{\big[
1 - r\big(f(y) - \log{p_n(y)}\big)
\big]}
}
\end{aligned}
\end{equation}</p>
<p>In which \(f(x) = \log p_m(x; \theta)\) is the function approximating log-likelihood the true distribution $p_d(.)$.</p>
<h3 id=notation>Notation<a hidden class=anchor aria-hidden=true href=#notation>#</a></h3>
<ul>
<li>\(p_d(x)\) true probability density function (p.d.f) of data.</li>
<li>\(p_n(x)\) p.d.f of noise generating distribution.</li>
<li>\(r(x) = \frac{1}{1+\exp(-x)}\) sigmoid function.</li>
<li>\(X = (x_1, &mldr; x_T); x \sim p_d(x)\) be the dataset of T observations.</li>
<li>\(Y = (y_1, &mldr; y_T); y \sim p_n(y)\) be the dataset of T artificially generated noise.</li>
<li>\(p_m(.; \theta)\) is estimation of \(p_d(.)\) parameterized by \(\theta\).</li>
</ul>
<h3 id=theorem>Theorem<a hidden class=anchor aria-hidden=true href=#theorem>#</a></h3>
<blockquote>
<p>\(\tilde{J}\) attains a maximum at \(f(.) = \log p_d(.)\). There are no other extrema if the noise density \(p_n(.)\) is chosen such it is nonzero whenever \(p_d(.)\) is nonzero.</p>
</blockquote>
<h3 id=proof>Proof<a hidden class=anchor aria-hidden=true href=#proof>#</a></h3>
<p>Let \(\hat{f}(x)\) be the optimal function that maximizes \(\tilde{J}\), and \(f(x)=\hat{f}(x) + \epsilon\eta(x)\).</p>
<p>\begin{equation}
\begin{aligned}
\tilde{J}(\theta) &= K(\epsilon) \\
&= \frac{1}{2}\mathbb{E}_{x, y} {
\log{
r\big(f(x) - \log{p_n(x)}\big)
} + \log{\big[
1 - r\big(f(y) - \log{p_n(y)}\big)
\big]}
} \\
&= \frac{1}{2}\underbrace{
\mathbb{E}_x {
\log r\big(
f(x) - \log p_n(x)
\big)
}
}_A +
\frac{1}{2} \underbrace{\mathbb{E}_y {
\log{\big[
1 - r\big(f(y) - \log{p_n(y)}\big)
\big]}
}}_B \\
\implies \frac{dK}{d\epsilon} &= \frac{dA}{d\epsilon} + \frac{dB}{d\epsilon}
\end{aligned}
\end{equation}</p>
<p>Expand the first term of $K(\epsilon)$</p>
<p>\begin{equation}
\begin{aligned}
A(\epsilon) &= \mathbb{E}_x {
\log r\big(
f(x) - \log p_n(x)
\big)
} \\
& = \int_x {
p_d(x) \log{
r\big(
\hat{f}(x) + \epsilon \eta(x) - \log p_n(x)
\big)
} dx
}
\end{aligned}
\end{equation}</p>
<p>Taking derivative of $A(\epsilon)$</p>
<p>\begin{equation}
\begin{aligned}
\frac{dA}{d\epsilon} &= \frac{1}{d\epsilon} \int_x {
p_d(x) \log{
r\big(
\hat{f}(x) + \epsilon \eta(x) - \log p_n(x)
\big)
} dx
} \\
& = \int_x {
p_d(x) \big[
\frac{1}{d\epsilon}\log{
r \big(
\underbrace{
\hat{f}(x) + \epsilon \eta(x) - \log p_n(x)
}_{g(\epsilon)}
\big)
}
\big]dx
} \\
& = \int_x{
p_d(x)
\frac{d\log{r}}{dr}
\frac{dr}{dg}
\frac{dg}{d\epsilon}
dx
} \\
& = \int_x{
p_d(x)
\frac{1}{r}
r(1-r)
\eta(x)
dx
} \\
& = \int_x{
p_d(x)
\big[1 - r\big( \hat{f}(x) + \epsilon \eta(x) - \log p_n(x)\big) \big]
\eta(x) dx
}
\end{aligned}
\end{equation}</p>
<p>Now let&rsquo;s turn our attention to the second term of $K(\epsilon)$</p>
<p>\begin{equation}
\begin{aligned}
B(\epsilon) &= \mathbb{E}_y {
\log\big[
1 - r\big(f(y) - \log{p_n(y)}\big)
\big]
} \\
& = \int_y {
p_n(y)
\log \big[
1 - r \big(
\underbrace{
\hat{f}(y) + \epsilon \eta(y) - \log p_n(y)
}_h
\big)
\big]dy
}
\end{aligned}
\end{equation}</p>
<p>Taking derivative of $B$ w.r.t $\epsilon$</p>
<p>\begin{equation}
\begin{aligned}
\frac{dB}{d\epsilon} &= \frac{1}{d\epsilon} \int_y{
p_n(y)\log{
\big[
1 - r\big( h(\epsilon)\big)
\big]
}dy
} \\
&= \int_y {
p_n(y)
\frac{d\log(1-r)}{d(1-r)}
\frac{d(1-r)}{dr}
\frac{dr}{dh}
\frac{dh}{d\epsilon}
dy
} \\
& = \int_y {
p_n(y)
\frac{1}{1-r}
(-1)
r(1-r)
\eta(y)
} \\
& = -\int_y{
p_n(y)
r\big(
\hat{f}(y) + \epsilon \eta(y) - \log p_n(y)
\big)
\eta(y) dy
}
\end{aligned}
\end{equation}</p>
<p>Substitute result from eq(4) and eq(6) to eq(2), $\frac{dK}{d\epsilon}$ is evaluated to $0$ at $\epsilon = 0$.</p>
<p>\begin{equation}
\begin{aligned}
\frac{dK}{d\epsilon}\big\vert_{\epsilon=0}
&= \frac{dA}{d\epsilon}\big\vert_{\epsilon=0}
+ \frac{dB}{d\epsilon}\big\vert_{\epsilon=0} \\
&= \int_x {
p_d(x)
\big[1 - r\big( \hat{f}(x) - \log p_n(x)\big) \big]
\eta(x) dx
} \\
& - \int_y{
p_n(y)
r \big(
\hat{f}(y) - \log p_n(y)
\big)
\eta(y) dy
} \\
& = 0
\end{aligned}
\end{equation}</p>
<p>Consider eq. (7), if the support for $x$ and $y$ are equal, which mean we integrate $x$ and $y$ over a same region, we can change $y$ to $x$ and rewrite eq.(7) as</p>
<p>\begin{equation}
\begin{aligned}
\frac{dK}{d\epsilon} \big\vert_{\epsilon = 0}
&= \int_x {
\underbrace{
p_d(x)
\big[1 - r\big( \hat{f}(x) - \log p_n(x)\big) \big]
}_C
\eta(x) dx
} \\
& - \int_x{
\underbrace{
p_n(x)
r \big(
\hat{f}(x) - \log p_n(x)
\big)
}_D
\eta(x) dx
} \\
& = \int_x{(C-D)\eta(x)dx} = 0 \quad \forall \eta(x)
\end{aligned}
\end{equation}</p>
<p>The equality in eq.(8) happend if and only if $C=D$. This result easily leads to $\hat{f}(x) = \log p_d(x)$.</p>
<h2 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h2>
<ol>
<li><a href=https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf>Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</a></li>
</ol>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://iamtu.dev/tags/learning/>learning</a></li>
<li><a href=https://iamtu.dev/tags/probabilistic-ml/>probabilistic-ml</a></li>
</ul>
<nav class=paginav>
<a class=prev href=https://iamtu.dev/posts/sbi/>
<span class=title>« Prev</span>
<br>
<span>Likelihood-free MCMC with Amortized Ratio Estimator</span>
</a>
</nav>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2023 <a href=https://iamtu.dev/>iamtu</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a>
<script>let menu=document.getElementById('menu');menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
<script>document.querySelectorAll('pre > code').forEach(b=>{const c=b.parentNode.parentNode,a=document.createElement('button');a.classList.add('copy-code'),a.innerHTML='copy';function d(){a.innerHTML='copied!',setTimeout(()=>{a.innerHTML='copy'},2e3)}a.addEventListener('click',e=>{if('clipboard'in navigator){navigator.clipboard.writeText(b.textContent),d();return}const a=document.createRange();a.selectNodeContents(b);const c=window.getSelection();c.removeAllRanges(),c.addRange(a);try{document.execCommand('copy'),d()}catch(a){}c.removeRange(a)}),c.classList.contains("highlight")?c.appendChild(a):c.parentNode.firstChild==c||(b.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?b.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(a):b.parentNode.appendChild(a))})</script>
</body>
</html>