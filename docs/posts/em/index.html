<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Expectation Maximization - EM | iamtu</title><meta name=keywords content="learn,ml"><meta name=description content="As of May'24, I am looking for a doctoral training opportunity. Please check out my resume if you think I am a good fit for your program.
Problem Given a statistical model \(P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})\), which generate set of observations \(\boldsymbol{X}\), where \(\boldsymbol{Z}\) is a latent variable and unknow parameter vector \(\boldsymbol{\theta}\). The goal is to find \(\boldsymbol{\theta}\) that maximize the marginal likelihood:
$$ \mathcal{L}(\boldsymbol{\theta}; \boldsymbol{X}) = P(\boldsymbol{X} | \boldsymbol{\theta}) = \int_{\boldsymbol{Z}}P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})d\boldsymbol{Z} $$"><meta name=author content="Me"><link rel=canonical href=https://iamtu.dev/posts/em/><meta name=google-site-verification content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.5cfc680b1eeaeef9efbced92d46c2a9e876b72ee14fba85846afc4cff9e6e6f8.css integrity="sha256-XPxoCx7q7vnvvO2S1Gwqnodrcu4U+6hYRq/Ez/nm5vg=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=16x16 href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://iamtu.dev/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css integrity=sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js integrity=sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8 crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js integrity=sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05 crossorigin=anonymous onload=renderMathInElement(document.body)></script>
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PWLR4FLELZ"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-PWLR4FLELZ",{anonymize_ip:!1})}</script><meta property="og:title" content="Expectation Maximization - EM"><meta property="og:description" content="As of May'24, I am looking for a doctoral training opportunity. Please check out my resume if you think I am a good fit for your program.
Problem Given a statistical model \(P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})\), which generate set of observations \(\boldsymbol{X}\), where \(\boldsymbol{Z}\) is a latent variable and unknow parameter vector \(\boldsymbol{\theta}\). The goal is to find \(\boldsymbol{\theta}\) that maximize the marginal likelihood:
$$ \mathcal{L}(\boldsymbol{\theta}; \boldsymbol{X}) = P(\boldsymbol{X} | \boldsymbol{\theta}) = \int_{\boldsymbol{Z}}P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})d\boldsymbol{Z} $$"><meta property="og:type" content="article"><meta property="og:url" content="https://iamtu.dev/posts/em/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2024-04-15T00:00:00+00:00"><meta property="article:modified_time" content="2024-04-15T00:00:00+00:00"><meta property="og:site_name" content="iamtu"><meta name=twitter:card content="summary"><meta name=twitter:title content="Expectation Maximization - EM"><meta name=twitter:description content="As of May'24, I am looking for a doctoral training opportunity. Please check out my resume if you think I am a good fit for your program.
Problem Given a statistical model \(P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})\), which generate set of observations \(\boldsymbol{X}\), where \(\boldsymbol{Z}\) is a latent variable and unknow parameter vector \(\boldsymbol{\theta}\). The goal is to find \(\boldsymbol{\theta}\) that maximize the marginal likelihood:
$$ \mathcal{L}(\boldsymbol{\theta}; \boldsymbol{X}) = P(\boldsymbol{X} | \boldsymbol{\theta}) = \int_{\boldsymbol{Z}}P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})d\boldsymbol{Z} $$"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://iamtu.dev/posts/"},{"@type":"ListItem","position":2,"name":"Expectation Maximization - EM","item":"https://iamtu.dev/posts/em/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Expectation Maximization - EM","name":"Expectation Maximization - EM","description":"As of May'24, I am looking for a doctoral training opportunity. Please check out my resume if you think I am a good fit for your program.\nProblem Given a statistical model \\(P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})\\), which generate set of observations \\(\\boldsymbol{X}\\), where \\(\\boldsymbol{Z}\\) is a latent variable and unknow parameter vector \\(\\boldsymbol{\\theta}\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that maximize the marginal likelihood:\n$$ \\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{X}) = P(\\boldsymbol{X} | \\boldsymbol{\\theta}) = \\int_{\\boldsymbol{Z}}P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})d\\boldsymbol{Z} $$","keywords":["learn","ml"],"articleBody":" As of May'24, I am looking for a doctoral training opportunity. Please check out my resume if you think I am a good fit for your program.\nProblem Given a statistical model \\(P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})\\), which generate set of observations \\(\\boldsymbol{X}\\), where \\(\\boldsymbol{Z}\\) is a latent variable and unknow parameter vector \\(\\boldsymbol{\\theta}\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that maximize the marginal likelihood:\n$$ \\mathcal{L}(\\boldsymbol{\\theta}; \\boldsymbol{X}) = P(\\boldsymbol{X} | \\boldsymbol{\\theta}) = \\int_{\\boldsymbol{Z}}P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta})d\\boldsymbol{Z} $$\nAs an example for this type of problem, there are two (unfair) coin A and B with probability of head for each coin is \\(p_A(H) = p \\text{ and } p_B(H) = q\\). For each trial, we select coin A with probability \\(p(A) = \\tau\\) and coin B with probability \\(p(B) = 1 -\\tau\\), toss the coin and record the observation. The set of observations \\(\\boldsymbol{X}\\) is the record of head or tail \\(\\{H, T, H, H, \\cdots\\}\\), the latent variable which is unobserved is which coint is selected for each trail \\(\\{A, B, B, A, \\cdots\\}\\), and the unknown parameter vector \\(\\boldsymbol{\\theta} = [p, q, \\tau]\\). The goal is to find \\(\\boldsymbol{\\theta}\\) that best fit observations; EM is an instance of Maximum Likelihood Estimation (MLE).\nThe EM algorithm The algorithm The EM algorithm seeks for \\(\\boldsymbol{\\theta}\\) by first initiates a random parameter vector \\(\\boldsymbol{\\theta}^{(0)}\\) and then iteratively performs two steps, namely the expectation step (E step) and the maximization step (M step):\n(The E step) the expected loglikelihood of \\(\\boldsymbol{\\theta}\\), with respect to the current conditional distribution of \\(\\boldsymbol{Z}\\) given observations \\(\\boldsymbol{X}\\) and current estimation of \\(\\boldsymbol{\\theta}^{(t)}\\) $$ Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) = \\mathbb{E}_{\\boldsymbol{Z} \\sim P(. | \\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)})} {[ \\log P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) ]} $$\n(The M step) update parameter vector \\(\\boldsymbol{\\theta}\\) $$ \\boldsymbol{\\theta}^{(t+1)} = \\arg\\max_{\\boldsymbol{\\theta}} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) $$\nEM for the coin example Setup\nParameter vector \\(\\boldsymbol{\\theta} = [p, q, \\tau]\\), and its estimation at step (t) is \\(\\boldsymbol{\\theta}^{(t)} = [p_t, q_t, \\tau_t]\\)\nThe \\(i^{th}\\) observation \\(x^{(i)}\\) is either head (H) or tail (T).\nThe coin selected for the \\(i^{th}\\) trail \\(z^{(i)}\\) is either A or B:\n\\(p(z^{(i)} = A) = \\tau\\) \\(p(z^{(i)} = B) = 1 -\\tau\\). For both cases, $$ \\begin{equation} p(z^{(i)}) = \\tau^{\\mathbb{I}(z^{(i)}=A)}(1-\\tau)^{\\mathbb{I}(z^{(i)}=B)} \\end{equation} $$\nWhen selected the coin A,\nProbability that we get a head (H): \\(p(x^{(i)}=H | z^{(i)} = A) = p\\) Probability that we get a head (T): \\(p(x^{(i)}=T | z^{(i)} = A) = 1 - p\\) For both cases, $$ \\begin{equation} p(x^{(i)} | z^{(i)}=A) = p^{\\mathbb{I}(x^{(i)}=H)}(1 - p)^{\\mathbb{I}(x^{(i)}=T)} \\end{equation} $$\nSimilarly, when B is selected $$ \\begin{equation} p(x^{(i)} | z^{(i)}=B) = q^{\\mathbb{I}(x^{(i)}=H)}(1 - q)^{\\mathbb{I}(x^{(i)}=T)} \\end{equation} $$\nWhere \\(\\mathbb{I}(\\cdot)\\) is an indicator function on a predicate $$ \\mathbb{I}(p) = \\begin{cases} 1 \\quad \\text{if } p \\text{ is True}\\\\ 0 \\quad \\text{otherwise} \\end{cases} $$\nOnce again, we generalize for both cases of \\(z^{(i)}\\)\n$$ \\begin{equation} \\begin{aligned} p(x^{(i)} | z^{(i)}) = [p^{\\mathbb{I}(x^{(i)}=H)}(1 - p)^{\\mathbb{I}(x^{(i)}=T)}]^{\\mathbb{I}(z^{(i)}=A)}\\\\ \\times [q^{\\mathbb{I}(x^{(i)}=H)}(1 - q)^{\\mathbb{I}(x^{(i)}=T)}]^{\\mathbb{I}(z^{(i)}=B)} \\end{aligned} \\end{equation} $$\nThe equation looks rather ugly, we can simplify this by encoding head as 1 and tail as 0; coin A as 1 and coin B as 0. The equation above can be written as\n$$ \\begin{equation} p(x^{(i)} | z^{(i)}) = [p^{x^{(i)}}(1-p)^{1 - x^{(i)}}]^{z^{(i)}} [q^{x^{(i)}}(1-q)^{1 - x^{(i)}}]^{1-z^{(i)}} \\end{equation} $$\nSimilarly for \\(p(z^{(i)})\\) $$ \\begin{equation} p(z^{(i)}) = \\tau^{z^{(i)}}(1-\\tau)^{1-z^{(i)}} \\end{equation} $$\nApplying EM algorithm\nThe (E step):\nConstruct the joint likelihood of a single pair of observation and latent variable \\(p(x^{(i)}, z^{(i)})\\ | \\boldsymbol{\\theta})\\). For the conciseness, we drop the \\((i)\\) superscript from the equation. $$ \\begin{equation} \\begin{aligned} p(x, z | \\boldsymbol{\\theta}) = \u0026 p(x | z, \\boldsymbol{\\theta})p(z | \\boldsymbol{\\theta})\\\\ = \u0026 [p^{x}(1-p)^{1 - x}]^{z} [q^{x}(1-q)^{1 - x}]^{1-z} \\tau^{z}(1-\\tau)^{1-z} \u0026 \\text{\\tiny(from eq. 5 and 6)} \\end{aligned} \\end{equation} $$\nLikelihood over entire observations \\(\\boldsymbol{X}\\) and latent \\(\\boldsymbol{Z}\\):\n$$\\boldsymbol{X}\\odot\\boldsymbol{Z} := \\{(x^{(i)}, z^{(i)})\\}_{i=1\\cdots N}$$\nA side note is that I am not entirely sure that \\(\\odot\\) operator is appropriate in this situation.\n$$ \\begin{equation} \\begin{aligned} P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) =\u0026 \\prod_{(x, z) \\in \\boldsymbol{X}\\odot\\boldsymbol{Z}} { p(x, z | \\boldsymbol{\\theta}) } \\end{aligned} \\end{equation} $$\nLog likelihood of the joint probability\n$$ \\begin{equation} \\begin{aligned} \\log P(\\boldsymbol{X}, \\boldsymbol{Z} | \\boldsymbol{\\theta}) \u0026 = \\sum_{(x, z)} \\log p(x, z | \\boldsymbol{\\theta}) \\end{aligned} \\end{equation} $$\nTaking a log always seem to make thing to be better.\nFinally, we need to take the expectation of the log likelihood w.r.t conditional probability of \\(\\boldsymbol{Z}|\\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)}\\)\nPosterior for a single latent \\(z\\)\n$$ \\begin{equation} \\begin{aligned} p(z | x, \\boldsymbol{\\theta}^{(t)}) \u0026 = \\frac{p(x, z | \\boldsymbol{\\theta}^{(t)})} {p(x | \\boldsymbol{\\theta}^{(t)})} \u0026 \\text{\\tiny(Bayes Theorem)}\\\\ \u0026 = \\frac{p(x, z | \\boldsymbol{\\theta}^{(t)})} { p(x, z = 0| \\boldsymbol{\\theta}^{(t)}) + p(x, z = 1| \\boldsymbol{\\theta}^{(t)}) } \u0026 \\text{\\tiny(Marginal likelihood over z in denominator)}\\\\ \u0026 = \\frac{ [p_t^{x}(1-p_t)^{1 - x}]^{z} [q_t^{x}(1-q_t)^{1 - x}]^{1-z} \\tau_t^{z}(1-\\tau_t)^{1-z} }{ q_t^{x}(1-q_t)^{1 - x} (1-\\tau_t) + p_t^{x}(1-p_t)^{1 - x}\\tau_t } \u0026 \\text{\\tiny(from eq. 7)} \\end{aligned} \\end{equation} $$\nTaking the expectation\n$$ \\begin{equation} \\begin{aligned} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) \u0026= \\mathbb{E}_{\\boldsymbol{Z} | \\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)}}{\\bigg[ \\sum_{(x, z)}{\\log p(x, z | \\boldsymbol{\\theta})} \\bigg]} \\\\ \u0026= \\sum_{(x, z)} { \\mathbb{E}_{\\boldsymbol{Z} | \\boldsymbol{X}, \\boldsymbol{\\theta}^{(t)}}{[ \\log p(x, z | \\boldsymbol{\\theta}) ]} } \\\\ \u0026= \\sum_{(x, z)} { \\mathbb{E}_{z | x, \\boldsymbol{\\theta}^{(t)}}{[ \\log p(x, z | \\boldsymbol{\\theta}) ]} } \\\\ \\end{aligned} \\end{equation} $$\nIt is always bothering for me that in literature, the posterior, of which to be taken expectation over, for the entire set latent variables \\(\\boldsymbol{Z} = \\{ z^{(1)}, \\cdots z^{(n)}\\}\\) can be replaced by the posterior for a single latent \\(z\\) in (eq. 11) without explanation. So in order to understand this, consider the equation.\n$$ \\begin{aligned} \\mathbb{E}_{\\boldsymbol{Z}}{\\bigg[\\sum_{z\\in \\boldsymbol{Z}}{f(z)}\\bigg]} \u0026= \\int_{\\boldsymbol{Z}}{ \\bigg[\\sum_{z\\in\\boldsymbol{Z}} f(z)\\bigg] p(\\boldsymbol{Z}) d\\boldsymbol{Z} } \\\\ \u0026 = \\sum_{z\\in\\boldsymbol{Z}}{\\int_{\\boldsymbol{Z}}{f(z)}} p(\\boldsymbol{Z})d\\boldsymbol{Z} \\\\ \u0026 = \\sum_{z\\in\\boldsymbol{Z}}{ \\int_{\\boldsymbol{Z}\\text{/}z} \\underbrace{\\bigg[\\int_{z}f(z)p(z)dz\\bigg]}_{A=\\mathbb{E}_z[f(z)]} p(\\boldsymbol{Z}\\text{/}z)d(\\boldsymbol{Z}/z) } \\\\ \u0026 = \\sum_{z\\in\\boldsymbol{Z}} A \\int_{\\boldsymbol{Z}\\text{/}z} p(\\boldsymbol{Z}\\text{/}z)d(\\boldsymbol{Z}/z) \u0026 \\text{\\tiny(A is constant w.r.t variable being integrated over)} \\\\ \u0026 = \\sum_{z\\in\\boldsymbol{Z}} \\mathbb{E}_z[f(z)] \u0026 \\text{\\tiny(Integeral overal a p.d.f evalulated to 1)} \\end{aligned} $$\nWhere \\(\\boldsymbol{Z} = \\{z^i\\}_{i=1\\cdots N}; z \\sim p(Z)\\); \\(\\boldsymbol{Z}/z\\) denotes set all variables within \\(\\boldsymbol{Z}\\) except \\(z\\).\nHaving clear that up, we are able to resume from (eq. 11) $$ \\begin{aligned} Q(\\boldsymbol{\\theta} | \\boldsymbol{\\theta}^{(t)}) \u0026= \\sum_{(x, z)} { \\mathbb{E}_{z | x, \\boldsymbol{\\theta}^{(t)}}{[ \\log p(x, z | \\boldsymbol{\\theta}) ]} } \\\\ \u0026= \\sum_{(x, z)} {\\bigg[ p(z = 0 | x, \\boldsymbol{\\theta}^{(t)}) \\log p(x, z = 0 | \\boldsymbol{\\theta}) \\ + p(z = 1 | x, \\boldsymbol{\\theta}^{(t)}) \\log p(x, z = 1 | \\boldsymbol{\\theta}) \\bigg]} \\end{aligned} $$\nFrom (eq. 7)\n$$ \\begin{aligned} p(x, z = 0 |\\boldsymbol{\\theta}) = q^x(1-q)^{1-x}(1-\\tau) \\end{aligned} $$ $$ \\begin{aligned} p(x, z = 1 |\\boldsymbol{\\theta}) = p^x(1-p)^{1-x}\\tau \\end{aligned} $$ From (eq. 10)\n$$ \\begin{aligned} p(z = 0 | x, \\boldsymbol{\\theta}^{(t)}) \u0026 = \\frac{ q_t^{x}(1-q_t)^{1-x}(1-\\tau_t) }{ q_t^{x}(1-q_t)^{1 - x} (1-\\tau_t) + p_t^{x}(1-p_t)^{1 - x}\\tau_t } \\end{aligned} $$\n$$ \\begin{aligned} p(z = 1 | x, \\boldsymbol{\\theta}^{(t)}) \u0026 = \\frac{ p_t^{x}(1-p_t)^{1-x}\\tau_t }{ q_t^{x}(1-q_t)^{1 - x} (1-\\tau_t) + p_t^{x}(1-p_t)^{1 - x}\\tau_t } \\end{aligned} $$\nProof of correctness T.B.D\nEM for Gaussian Mixture Model EM for GMM’s python implementation Foot note\nI am preparing for my graduate school application, this post is written in preparation for the application and interview. I suppose to be preparing a slide for my supervisor, but sometime you can’t help when the mood strikes. So sorry in advance to my supervisor. Lately I haven’t been myself due to the stress of application process, so writing this helps keeping me on track, somehow. ","wordCount":"1205","inLanguage":"en","datePublished":"2024-04-15T00:00:00Z","dateModified":"2024-04-15T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://iamtu.dev/posts/em/"},"publisher":{"@type":"Organization","name":"iamtu","logo":{"@type":"ImageObject","url":"https://iamtu.dev/%3Clink%20/%20abs%20url%3E"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://iamtu.dev/ accesskey=h title="iamtu (Alt + H)">iamtu</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://iamtu.dev/ title=home><span>home</span></a></li><li><a href=https://iamtu.dev/search/ title="search (Alt + /)" accesskey=/><span>search</span></a></li><li><a href=https://iamtu.dev/categories/ title=categories><span>categories</span></a></li><li><a href=https://iamtu.dev/about/ title=about><span>about</span></a></li><li><a href=https://iamtu.dev/archives/ title=archives><span>archives</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://iamtu.dev/>Home</a>&nbsp;»&nbsp;<a href=https://iamtu.dev/posts/>Posts</a></div><h1 class=post-title>Expectation Maximization - EM</h1></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#problem aria-label=Problem>Problem</a></li><li><a href=#the-em-algorithm aria-label="The EM algorithm">The EM algorithm</a><ul><li><a href=#the-algorithm aria-label="The algorithm">The algorithm</a></li><li><a href=#em-for-the-coin-example aria-label="EM for the coin example">EM for the coin example</a></li></ul></li><li><a href=#proof-of-correctness aria-label="Proof of correctness">Proof of correctness</a></li><li><a href=#em-for-gaussian-mixture-model aria-label="EM for Gaussian Mixture Model">EM for Gaussian Mixture Model</a></li></ul></div></details></div><div class=post-content><blockquote><p>As of May'24, I am looking for a doctoral training opportunity. Please check out <a href=/about>my resume</a> if you think I am a good fit for your program.</p></blockquote><h2 id=problem>Problem<a hidden class=anchor aria-hidden=true href=#problem>#</a></h2><p>Given a statistical model \(P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})\), which generate set of observations \(\boldsymbol{X}\), where \(\boldsymbol{Z}\) is a latent variable and unknow parameter vector \(\boldsymbol{\theta}\). The goal is to find \(\boldsymbol{\theta}\) that maximize the marginal likelihood:</p><p>$$
\mathcal{L}(\boldsymbol{\theta}; \boldsymbol{X}) = P(\boldsymbol{X} | \boldsymbol{\theta})
= \int_{\boldsymbol{Z}}P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})d\boldsymbol{Z}
$$</p><p>As an example for this type of problem, there are two (unfair) coin A and B with probability of head for each coin is \(p_A(H) = p \text{ and } p_B(H) = q\). For each trial, we select coin A with probability \(p(A) = \tau\) and coin B with probability \(p(B) = 1 -\tau\), toss the coin and record the observation. The set of observations \(\boldsymbol{X}\) is the record of head or tail \(\{H, T, H, H, \cdots\}\), the latent variable which is unobserved is which coint is selected for each trail \(\{A, B, B, A, \cdots\}\), and the unknown parameter vector \(\boldsymbol{\theta} = [p, q, \tau]\). The goal is to find \(\boldsymbol{\theta}\) that best fit observations; EM is an instance of Maximum Likelihood Estimation (MLE).</p><h2 id=the-em-algorithm>The EM algorithm<a hidden class=anchor aria-hidden=true href=#the-em-algorithm>#</a></h2><h3 id=the-algorithm>The algorithm<a hidden class=anchor aria-hidden=true href=#the-algorithm>#</a></h3><p>The EM algorithm seeks for \(\boldsymbol{\theta}\) by first initiates a random parameter vector \(\boldsymbol{\theta}^{(0)}\) and then iteratively performs two steps, namely the expectation step (E step) and the maximization step (M step):</p><ul><li>(The E step) the expected loglikelihood of \(\boldsymbol{\theta}\), with respect to the current conditional distribution of \(\boldsymbol{Z}\) given observations \(\boldsymbol{X}\) and current estimation of \(\boldsymbol{\theta}^{(t)}\)</li></ul><p>$$
Q(\boldsymbol{\theta} | \boldsymbol{\theta}^{(t)}) = \mathbb{E}_{\boldsymbol{Z} \sim P(. | \boldsymbol{X}, \boldsymbol{\theta}^{(t)})} {[
\log P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})
]}
$$</p><ul><li>(The M step) update parameter vector \(\boldsymbol{\theta}\)</li></ul><p>$$
\boldsymbol{\theta}^{(t+1)} = \arg\max_{\boldsymbol{\theta}} Q(\boldsymbol{\theta} | \boldsymbol{\theta}^{(t)})
$$</p><h3 id=em-for-the-coin-example>EM for the coin example<a hidden class=anchor aria-hidden=true href=#em-for-the-coin-example>#</a></h3><p><strong>Setup</strong></p><ul><li><p>Parameter vector \(\boldsymbol{\theta} = [p, q, \tau]\), and its estimation at step (t) is \(\boldsymbol{\theta}^{(t)} = [p_t, q_t, \tau_t]\)</p></li><li><p>The \(i^{th}\) observation \(x^{(i)}\) is either head (H) or tail (T).</p></li><li><p>The coin selected for the \(i^{th}\) trail \(z^{(i)}\) is either A or B:</p><ul><li>\(p(z^{(i)} = A) = \tau\)</li><li>\(p(z^{(i)} = B) = 1 -\tau\).</li></ul><p>For both cases,
$$
\begin{equation}
p(z^{(i)}) = \tau^{\mathbb{I}(z^{(i)}=A)}(1-\tau)^{\mathbb{I}(z^{(i)}=B)}
\end{equation}
$$</p></li><li><p>When selected the coin A,</p><ul><li>Probability that we get a head (H): \(p(x^{(i)}=H | z^{(i)} = A) = p\)</li><li>Probability that we get a head (T): \(p(x^{(i)}=T | z^{(i)} = A) = 1 - p\)</li></ul><p>For both cases,
$$
\begin{equation}
p(x^{(i)} | z^{(i)}=A) = p^{\mathbb{I}(x^{(i)}=H)}(1 - p)^{\mathbb{I}(x^{(i)}=T)}
\end{equation}
$$</p></li><li><p>Similarly, when B is selected
$$
\begin{equation}
p(x^{(i)} | z^{(i)}=B) = q^{\mathbb{I}(x^{(i)}=H)}(1 - q)^{\mathbb{I}(x^{(i)}=T)}
\end{equation}
$$</p></li></ul><p>Where \(\mathbb{I}(\cdot)\) is an indicator function on a predicate
$$
\mathbb{I}(p) = \begin{cases}
1 \quad \text{if } p \text{ is True}\\
0 \quad \text{otherwise}
\end{cases}
$$</p><p>Once again, we generalize for both cases of \(z^{(i)}\)</p><p>$$
\begin{equation}
\begin{aligned}
p(x^{(i)} | z^{(i)}) =
[p^{\mathbb{I}(x^{(i)}=H)}(1 - p)^{\mathbb{I}(x^{(i)}=T)}]^{\mathbb{I}(z^{(i)}=A)}\\
\times [q^{\mathbb{I}(x^{(i)}=H)}(1 - q)^{\mathbb{I}(x^{(i)}=T)}]^{\mathbb{I}(z^{(i)}=B)}
\end{aligned}
\end{equation}
$$</p><p>The equation looks rather ugly, we can simplify this by encoding head as 1 and tail as 0; coin A as 1 and coin B as 0. The equation above can be written as</p><p>$$
\begin{equation}
p(x^{(i)} | z^{(i)}) = [p^{x^{(i)}}(1-p)^{1 - x^{(i)}}]^{z^{(i)}}
[q^{x^{(i)}}(1-q)^{1 - x^{(i)}}]^{1-z^{(i)}}
\end{equation}
$$</p><p>Similarly for \(p(z^{(i)})\)
$$
\begin{equation}
p(z^{(i)}) = \tau^{z^{(i)}}(1-\tau)^{1-z^{(i)}}
\end{equation}
$$</p><p><strong>Applying EM algorithm</strong></p><ul><li><p><strong>The (E step)</strong>:</p><ul><li>Construct the joint likelihood of a single pair of observation and latent variable \(p(x^{(i)}, z^{(i)})\ | \boldsymbol{\theta})\). For the conciseness, we drop the \((i)\) superscript from the equation.</li></ul><p>$$
\begin{equation}
\begin{aligned}
p(x, z | \boldsymbol{\theta}) = & p(x | z, \boldsymbol{\theta})p(z | \boldsymbol{\theta})\\
= & [p^{x}(1-p)^{1 - x}]^{z} [q^{x}(1-q)^{1 - x}]^{1-z} \tau^{z}(1-\tau)^{1-z}
& \text{\tiny(from eq. 5 and 6)}
\end{aligned}
\end{equation}
$$</p><ul><li><p>Likelihood over entire observations \(\boldsymbol{X}\) and latent \(\boldsymbol{Z}\):</p><p>$$\boldsymbol{X}\odot\boldsymbol{Z} := \{(x^{(i)}, z^{(i)})\}_{i=1\cdots N}$$</p><blockquote><p>A side note is that I am not entirely sure that \(\odot\) operator is appropriate in this situation.</p></blockquote><p>$$
\begin{equation}
\begin{aligned}
P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta}) =& \prod_{(x, z) \in \boldsymbol{X}\odot\boldsymbol{Z}} {
p(x, z | \boldsymbol{\theta})
}
\end{aligned}
\end{equation}
$$</p></li><li><p>Log likelihood of the joint probability</p><p>$$
\begin{equation}
\begin{aligned}
\log P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta}) & = \sum_{(x, z)} \log p(x, z | \boldsymbol{\theta})
\end{aligned}
\end{equation}
$$</p><blockquote><p>Taking a log always seem to make thing to be better.</p></blockquote></li><li><p>Finally, we need to take the expectation of the log likelihood w.r.t conditional probability of \(\boldsymbol{Z}|\boldsymbol{X}, \boldsymbol{\theta}^{(t)}\)</p><ul><li><p>Posterior for a single latent \(z\)</p><p>$$
\begin{equation}
\begin{aligned}
p(z | x, \boldsymbol{\theta}^{(t)})
& = \frac{p(x, z | \boldsymbol{\theta}^{(t)})}
{p(x | \boldsymbol{\theta}^{(t)})} & \text{\tiny(Bayes Theorem)}\\
& = \frac{p(x, z | \boldsymbol{\theta}^{(t)})}
{
p(x, z = 0| \boldsymbol{\theta}^{(t)}) +
p(x, z = 1| \boldsymbol{\theta}^{(t)})
} & \text{\tiny(Marginal likelihood over z in denominator)}\\
& = \frac{
[p_t^{x}(1-p_t)^{1 - x}]^{z} [q_t^{x}(1-q_t)^{1 - x}]^{1-z} \tau_t^{z}(1-\tau_t)^{1-z}
}{
q_t^{x}(1-q_t)^{1 - x} (1-\tau_t) + p_t^{x}(1-p_t)^{1 - x}\tau_t
} & \text{\tiny(from eq. 7)}
\end{aligned}
\end{equation}
$$</p></li><li><p>Taking the expectation</p><p>$$
\begin{equation}
\begin{aligned}
Q(\boldsymbol{\theta} | \boldsymbol{\theta}^{(t)})
&= \mathbb{E}_{\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta}^{(t)}}{\bigg[
\sum_{(x, z)}{\log p(x, z | \boldsymbol{\theta})}
\bigg]} \\
&= \sum_{(x, z)} {
\mathbb{E}_{\boldsymbol{Z} | \boldsymbol{X}, \boldsymbol{\theta}^{(t)}}{[
\log p(x, z | \boldsymbol{\theta})
]}
} \\
&= \sum_{(x, z)} {
\mathbb{E}_{z | x, \boldsymbol{\theta}^{(t)}}{[
\log p(x, z | \boldsymbol{\theta})
]}
} \\
\end{aligned}
\end{equation}
$$</p><blockquote><p>It is always bothering for me that in literature, the posterior, of which to be taken expectation over, for the entire set latent variables \(\boldsymbol{Z} = \{ z^{(1)}, \cdots z^{(n)}\}\) can be replaced by the posterior for a single latent \(z\) in (eq. 11) without explanation. So in order to understand this, consider the equation.</p></blockquote><p>$$
\begin{aligned}
\mathbb{E}_{\boldsymbol{Z}}{\bigg[\sum_{z\in \boldsymbol{Z}}{f(z)}\bigg]} &= \int_{\boldsymbol{Z}}{
\bigg[\sum_{z\in\boldsymbol{Z}} f(z)\bigg] p(\boldsymbol{Z}) d\boldsymbol{Z}
} \\
& = \sum_{z\in\boldsymbol{Z}}{\int_{\boldsymbol{Z}}{f(z)}} p(\boldsymbol{Z})d\boldsymbol{Z} \\
& = \sum_{z\in\boldsymbol{Z}}{
\int_{\boldsymbol{Z}\text{/}z}
\underbrace{\bigg[\int_{z}f(z)p(z)dz\bigg]}_{A=\mathbb{E}_z[f(z)]}
p(\boldsymbol{Z}\text{/}z)d(\boldsymbol{Z}/z)
} \\
& = \sum_{z\in\boldsymbol{Z}} A
\int_{\boldsymbol{Z}\text{/}z} p(\boldsymbol{Z}\text{/}z)d(\boldsymbol{Z}/z) & \text{\tiny(A is constant w.r.t variable being integrated over)} \\
& = \sum_{z\in\boldsymbol{Z}} \mathbb{E}_z[f(z)] & \text{\tiny(Integeral overal a p.d.f evalulated to 1)}
\end{aligned}
$$</p><p>Where \(\boldsymbol{Z} = \{z^i\}_{i=1\cdots N}; z \sim p(Z)\); \(\boldsymbol{Z}/z\) denotes set all variables within \(\boldsymbol{Z}\) except \(z\).</p></li><li><p>Having clear that up, we are able to resume from (eq. 11)
$$
\begin{aligned}
Q(\boldsymbol{\theta} | \boldsymbol{\theta}^{(t)})
&= \sum_{(x, z)} {
\mathbb{E}_{z | x, \boldsymbol{\theta}^{(t)}}{[
\log p(x, z | \boldsymbol{\theta})
]}
} \\
&= \sum_{(x, z)} {\bigg[
p(z = 0 | x, \boldsymbol{\theta}^{(t)}) \log p(x, z = 0 | \boldsymbol{\theta}) \
+ p(z = 1 | x, \boldsymbol{\theta}^{(t)}) \log p(x, z = 1 | \boldsymbol{\theta})
\bigg]}
\end{aligned}
$$</p><p>From (eq. 7)</p><ul><li>$$
\begin{aligned}
p(x, z = 0 |\boldsymbol{\theta}) = q^x(1-q)^{1-x}(1-\tau)
\end{aligned}
$$</li><li>$$
\begin{aligned}
p(x, z = 1 |\boldsymbol{\theta}) = p^x(1-p)^{1-x}\tau
\end{aligned}
$$</li></ul><p>From (eq. 10)</p><ul><li><p>$$
\begin{aligned}
p(z = 0 | x, \boldsymbol{\theta}^{(t)})
& = \frac{
q_t^{x}(1-q_t)^{1-x}(1-\tau_t)
}{
q_t^{x}(1-q_t)^{1 - x} (1-\tau_t) + p_t^{x}(1-p_t)^{1 - x}\tau_t
}
\end{aligned}
$$</p></li><li><p>$$
\begin{aligned}
p(z = 1 | x, \boldsymbol{\theta}^{(t)})
& = \frac{
p_t^{x}(1-p_t)^{1-x}\tau_t
}{
q_t^{x}(1-q_t)^{1 - x} (1-\tau_t) + p_t^{x}(1-p_t)^{1 - x}\tau_t
}
\end{aligned}
$$</p></li></ul></li></ul></li></ul></li></ul><h2 id=proof-of-correctness>Proof of correctness<a hidden class=anchor aria-hidden=true href=#proof-of-correctness>#</a></h2><p>T.B.D</p><h2 id=em-for-gaussian-mixture-model>EM for Gaussian Mixture Model<a hidden class=anchor aria-hidden=true href=#em-for-gaussian-mixture-model>#</a></h2><ul><li><a href=https://github.com/young1906/em>EM for GMM&rsquo;s python implementation</a></li></ul><hr><p><strong>Foot note</strong></p><ul><li>I am preparing for my graduate school application, this post is written in preparation for the application and interview.</li><li>I suppose to be preparing a slide for my supervisor, but sometime you can&rsquo;t help when the mood strikes. So sorry in advance to my supervisor.</li><li>Lately I haven&rsquo;t been myself due to the stress of application process, so writing this helps keeping me on track, somehow.</li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=https://iamtu.dev/tags/learn/>learn</a></li><li><a href=https://iamtu.dev/tags/ml/>ml</a></li></ul><nav class=paginav><a class=next href=https://iamtu.dev/posts/variational_inference/><span class=title>Next »</span><br><span>Understanding Variational Inference</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2024 <a href=https://iamtu.dev/>iamtu</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>