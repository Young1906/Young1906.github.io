<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>iamtu</title>
    <link>https://iamtu.dev/</link>
    <description>Recent content on iamtu</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 22 Jul 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://iamtu.dev/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Learning to solve heat equation</title>
      <link>https://iamtu.dev/posts/heat/</link>
      <pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate>
      
      <guid>https://iamtu.dev/posts/heat/</guid>
      <description>(note) Editting note TODO:
Compare FTCS with close-form solution Motivation section Introduction to heat equation BTCS scheme PINN Theory? Coding Citation? TLDR Heat equations $$ \begin{equation} \begin{aligned} PDE: &amp;amp; &amp;amp; u_t = \alpha^2 u_{xx} &amp;amp; &amp;amp; 0 &amp;lt; x&amp;lt; 1 &amp;amp; &amp;amp; 0 &amp;lt; t &amp;lt; \infty \\ BCs: &amp;amp; &amp;amp; \begin{cases} u(0, t) = 0\\ u(1, t) = 0 \end{cases} &amp;amp; &amp;amp; 0 &amp;lt; t &amp;lt; \infty \\ ICs: &amp;amp; &amp;amp; u(x, 0) = \sin(2\pi x) &amp;amp; &amp;amp; 0 \leq x \leq 1 \end{aligned} \end{equation} $$</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://iamtu.dev/about/</link>
      <pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://iamtu.dev/about/</guid>
      <description>PAGE UNDER CONSTRUCTION
Resumes / Portfolio Academic Resume Research portfolio Professional Resume Researchs / Publications Accepted (BME 2020) Tu, Do Thanh, Thuong Nguyen, Anh Tho Le, Sinh Nguyen, Huong Ha. &amp;ldquo;Automated EOG removal from EEG signal using Independent Component Analysis and Machine Learning Algorithms&amp;rdquo; at The 8th International Conference in Vietnam on the Development of Biomedical Engineering. (link)
(ICHST 2023) Tu, Do Thanh, Luan Van Tran, Tho Anh Le, Thao Mai Thi Le, Lanâ€‘Anh Hoang Duong, Thuong Hoai Nguyen, Anh Minh Hoang An, Duy The Phan, Khiet Thu Thi Dang, Quyen Hoang Quoc Vo, Nam Phuong Nguyen, Huong Thanh Thi Ha.</description>
    </item>
    
    <item>
      <title>Expectation Maximization - EM</title>
      <link>https://iamtu.dev/posts/em/</link>
      <pubDate>Wed, 08 May 2024 00:00:00 +0000</pubDate>
      
      <guid>https://iamtu.dev/posts/em/</guid>
      <description>Problem Given a statistical model \(P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})\), which generate set of observations \(\boldsymbol{X}\), where \(\boldsymbol{Z}\) is a latent variable and unknow parameter vector \(\boldsymbol{\theta}\). The goal is to find \(\boldsymbol{\theta}\) that maximize the marginal likelihood:
$$ \mathcal{L}(\boldsymbol{\theta}; \boldsymbol{X}) = P(\boldsymbol{X} | \boldsymbol{\theta}) = \int_{\boldsymbol{Z}}P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})d\boldsymbol{Z} $$
As an example for this type of problem, there are two (unfair) coin A and B with probability of head for each coin is \(p_A(H) = p \text{ and } p_B(H) = q\).</description>
    </item>
    
    <item>
      <title>Understanding Variational Inference</title>
      <link>https://iamtu.dev/posts/variational_inference/</link>
      <pubDate>Thu, 29 Feb 2024 00:00:00 +0000</pubDate>
      
      <guid>https://iamtu.dev/posts/variational_inference/</guid>
      <description>This post is a note I take from while reading Blei et al 2018.
Goal:
Motivation of variational inference Understand the derivation of ELBO and its intiution Walk through the derivation, some of which was skip the in original paper Implementation of CAVI ELBO Goal is to find \(q(z)\) to approximate \(p(z|x)\)
The KL-divergence
$$ \begin{equation} \begin{aligned} KL[q(z)||p(z | x)] &amp;amp;= \int_z{q(z)\log{\frac{p(z|x)}{q(z)}} dz} \end{aligned} \end{equation} $$
However, this quantity is intractable to compute hence, we&amp;rsquo;re unable to optimize this quantity directly.</description>
    </item>
    
    <item>
      <title>Differentiation under integral sign</title>
      <link>https://iamtu.dev/posts/diff-under-integral-sign/</link>
      <pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://iamtu.dev/posts/diff-under-integral-sign/</guid>
      <description>Motivating example Evaluating following integral
$$ I = \int_0^1{\frac{1 - x^2}{\ln{x}}dx} $$
Closed-form results $$ \begin{equation} \begin{aligned} F(t) &amp;amp;= \int_0^1{\frac{1-x^t}{\ln(x)}dx} \\ \implies \frac{d}{dt}F &amp;amp;= \frac{d}{dt}\int_0^1{\frac{1-x^t}{\ln(x)}dx}\\ &amp;amp;= \int_0^1{ \frac{\partial}{\partial t} \frac{1-x^t}{\ln(x)}dx }\\ &amp;amp;= \int_0^1{ \frac{-\ln(x)x^t}{ln(x)} dx} \\ &amp;amp;= \bigg[-\frac{x^{t+1}}{t+1}\bigg]_0^1\\ &amp;amp;= -\frac{1}{t+1}\\ \implies F(t) &amp;amp;= -\ln({t+1}) \\ \implies I &amp;amp;= f(2) = -\ln3 \end{aligned} \end{equation} $$
Numerical approximation Code to produce the figure 1 2 3 4 5 6 7 8 import numpy as np from matplotlib import pyplot as plt def I(): g = lambda x: (1 - x**2)/np.</description>
    </item>
    
    <item>
      <title>Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution</title>
      <link>https://iamtu.dev/posts/closed-form-kl-gaussian/</link>
      <pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://iamtu.dev/posts/closed-form-kl-gaussian/</guid>
      <description>The closed form of KL divergence used in Variational Auto Encoder.
Univariate case Let
\(p(x) = \mathcal{N}(\mu_1, \sigma_1) = (2\pi\sigma_1^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_1^2}(x-\mu_1)^2]\) \(q(x) = \mathcal{N}(\mu_1, \sigma_2) = (2\pi\sigma_2^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_2^2}(x-\mu_2)^2]\) KL divergence between \(p\) and \(q\) is defined as:
$$ \begin{aligned} \text{KL}(p\parallel q) &amp;amp;= -\int_{x}{p(x)\log{\frac{q(x)}{p(x)}}dx} \\ &amp;amp;= -\int_x p(x) [\log{q(x)} - \log{p(x)}]dx \\ &amp;amp;= \underbrace{ \int_x{p(x)\log p(x) dx}}_A - \underbrace{ \int_x{p(x)\log q(x) dx}}_B \end{aligned} $$
First quantity \(A\):
$$ \begin{aligned} A &amp;amp;= \int_x{p(x)\log p(x) dx} \\ &amp;amp;= \int_x{p(x)\big[ -\frac{1}{2}\log{2\pi\sigma_1^2 - \frac{1}{2\sigma_1^2}(x - \mu_1)^2} \big]dx}\\ &amp;amp;= -\frac{1}{2}\log{2\pi\sigma_1^2}\int_x{p(x)dx} - \frac{1}{2\sigma_1^2} \underbrace{\int_x{p(x)(x-\mu_1)^2dx}}_{\text{var(x)}}\\ &amp;amp;= -\frac{1}{2}\log{2\pi} - \log\sigma_1-\frac{1}{2} \end{aligned} $$</description>
    </item>
    
    <item>
      <title>Likelihood-free MCMC with Amortized Ratio Estimator</title>
      <link>https://iamtu.dev/posts/sbi/</link>
      <pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://iamtu.dev/posts/sbi/</guid>
      <description>Simulation Based Inference Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see this paper.</description>
    </item>
    
    <item>
      <title>Noise constrastive estimation</title>
      <link>https://iamtu.dev/posts/noise-contrastive-estimation/</link>
      <pubDate>Sat, 23 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://iamtu.dev/posts/noise-contrastive-estimation/</guid>
      <description>TLDR The paper proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \(T\) observations \((x_1, &amp;hellip; x_T)\) drawn from a true distribution \(p_d(.)\). We then try to approximate \(p_d\) by a parameterized function \(p_m(.;\theta)\). The estimator \(\hat{\theta}_T\) is defined to be the \(\theta\) that maximize function
$$ J_T(\theta) = \frac{1}{2T}\sum_t{\log[h(x_t; 0)]} + \log[1-h(y_t; \theta)] $$</description>
    </item>
    
    
    <item>
      <title>Real Analysis - Lecture notes</title>
      <link>https://iamtu.dev/posts/real-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://iamtu.dev/posts/real-analysis/</guid>
      <description>Notes I took during studying MIT OCW Real Analysis. The class taught by Professor Casey Rodriguez, he also taught Functional analysis.
Resources (Useful link) Video lecture Course&amp;rsquo;s homepage Lecture notes Goal of the course - Gain experience with proofs - Prove statements about the real numbers, function and limits
Lecture 1: Sets, Set operations, and Mathematical Induction Definition (Sets) A sets is a collection of objects called elements/members.
Definition (Empty set) A set with no elements, denoted as \(\emptyset\)</description>
    </item>
    
    
  </channel>
</rss>
