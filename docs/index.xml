<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>iamtu</title><link>https://iamtu.dev/</link><description>Recent content on iamtu</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 01 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://iamtu.dev/index.xml" rel="self" type="application/rss+xml"/><item><title>Understanding Adjoint sensitivity method</title><link>https://iamtu.dev/posts/adjoint_state_method/</link><pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/adjoint_state_method/</guid><description>&lt;p>Suppose we have a dataset \({(t_i, u_{t_i})}_{i=0\cdots N-1}, \quad u_{t_i} \in \mathbb{R}^N\) is the observed state of a dynamical system given by ODE&lt;/p>
&lt;p>\begin{equation}
\begin{aligned}
\begin{cases}
u(t=t_i) = u_{t_i} \
\frac{du}{dt} = f(u, t, \theta)
\end{cases}
\end{aligned}
\end{equation}&lt;/p>
&lt;p>For simplicity, assume we only have 2 observed states \((t_0, u_0), (t_1, u_1)\). So that we can write \(u_1\) in term of \(u_0\) and the dynamic \(f\)&lt;/p>
&lt;p>$$
\begin{equation}
\begin{aligned}
u(t_1) = u_0 + \int_{t_0}^{t_1}{f(u, t, \theta) dt}
\end{aligned}
\end{equation}
$$&lt;/p></description></item><item><title>Numerical Integrations</title><link>https://iamtu.dev/posts/ode_solver/</link><pubDate>Thu, 26 Sep 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/ode_solver/</guid><description>&lt;p>&lt;strong>todo&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>&lt;input disabled="" type="checkbox"> Derivation of second and forth order Runge-Kutta methods&lt;/li>
&lt;li>&lt;input disabled="" type="checkbox"> Comparison of truncation error with different step-size&lt;/li>
&lt;/ul>
&lt;h2 id="ordinary-differential-equation-ode-initial-value-problem">Ordinary Differential Equation (ODE) Initial Value Problem&lt;/h2>
&lt;p>A &lt;strong>differential equation&lt;/strong> is differential equation is a relationship between function \(f(x)\), its independent variable \(x\), and any number of its derivative. An &lt;strong>ODE&lt;/strong> is a differential equation where the independent variable and its derivatives are in one dimension.&lt;/p>
&lt;p>$$
\begin{equation}
F(x, f(x), f^{(1)}(x), f^{(2)}, \cdots f^{(n-1)}(x)) = f^{(n)}(x)
\end{equation}
$$&lt;/p></description></item><item><title>My calculus of variations crash course</title><link>https://iamtu.dev/posts/variational_calculus/</link><pubDate>Tue, 24 Sep 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/variational_calculus/</guid><description>&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>My goal for this post is to have a basic understanding of Calculus of Variations, so that I can be more comfortable with mathematics in &lt;a href="https://arxiv.org/abs/1806.07366">NeuralODE paper&lt;/a>, where the problem can be formulated as a optimization of a functional with ODE constraint (&lt;a href="https://www.youtube.com/watch?v=k6s2G5MZv-I&amp;amp;t=556s">Adjoint State Method for an ODE&lt;/a>).&lt;/p>
&lt;p>My first encounter with Calculus of Variation is one of my homework where we try to derive probablity density function of some distribution by the principle of maximum entropy. This is my note of a more thorough investigation of the topic and it is heavily based on the content of &lt;a href="https://www.open.edu/openlearn/8b/91/8b919cebaa524d141308761405438be36126c07d?response-content-disposition=inline%3Bfilename%3D%22Introduction%20to%20the%20calculus%20of%20variations_ms327.pdf%22&amp;amp;response-content-type=application%2Fpdf&amp;amp;Expires=1727281020&amp;amp;Signature=Y5qPgt8T-FL6MrViTgisHOaDvOZrnyRTlog5CKdcx3O3IaYDepGQ43r2ZdUf2rX3tGPulZYzYbf1H1RhB6Xv2V80TZtQThFlBZLMgHWWNDR3AZKxqL8FD3Xq2vlRwft~grD5fGpioT6G57~zDUKl0TQ7bR3PDv4DzUakrkhDg-NWehqo4dIk7v5PnyFEeki7qsvbz6Dtkqo9MwEn8Z-CDWg2IUwZ5ocNFSts7Hj8ABQpljFITa8C36XvZBgy8lcmPCAhAr4gFi26Y5PXnytW7~3FCqnlLJd6pA7yK-t9A5UMW7dooi6el4bvbuMVrbrZ~sLTNXeM9SXOryke07jTBw__&amp;amp;Key-Pair-Id=K87HJKWMK329B">this tutorial&lt;/a>.&lt;/p></description></item><item><title>Learning to solve heat equation</title><link>https://iamtu.dev/posts/heat/</link><pubDate>Mon, 22 Jul 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/heat/</guid><description>&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>Surveying numerical methods (finite difference methods) and physics-informed neural networks to solve a 1D heat equation. This post was heavily inspired by:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>(Book) &lt;a href="https://www.amazon.com/Differential-Equations-Scientists-Engineers-Mathematics/dp/048667620X">Partial Differential Equations for Scientists and Engineers - Standley J. Farlow&lt;/a>&lt;/strong> for deriving closed-form solution.&lt;/li>
&lt;li>&lt;strong>(Article) &lt;a href="http://dma.dima.uniroma1.it/users/lsa_adn/MATERIALE/FDheat.pdf">Finite-Difference Approximations to the Heat Equation&lt;/a>&lt;/strong>&lt;/li>
&lt;li>&lt;strong>(Course) &lt;a href="https://www.youtube.com/watch?v=IDIv92Z6Qvc&amp;amp;list=PLJkYEExhe7rYY5HjpIJbgo-tDZ3bIAqAm&amp;amp;index=5">ETH Zurich | Deep Learning for Scientific Computing 2023&lt;/a>&lt;/strong> for Theory and Implementation of Physics-Informed Neural Network.&lt;/li>
&lt;/ul>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>&lt;em>Physics-Informed Machine Learning&lt;/em> (PIML) is an exciting subfield of Machine Learning that aims to incorporate physical laws and/or constraints into statistical machine learning. The representations of the laws and constraints can be categorized into three groups (with decreasing strength of inductive bias):&lt;/p></description></item><item><title>Numerical methods for PDE</title><link>https://iamtu.dev/posts/pde/</link><pubDate>Thu, 23 May 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/pde/</guid><description>&lt;p>Outline&lt;/p>
&lt;ul>
&lt;li>ODE with initial value problem&lt;/li>
&lt;li>Reduction of orders&lt;/li>
&lt;li>Euler method&lt;/li>
&lt;/ul></description></item><item><title>About</title><link>https://iamtu.dev/about/</link><pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/about/</guid><description>&lt;h2 id="update">Update&lt;/h2>
&lt;h3 id="2024">2024&lt;/h3>
&lt;h4 id="may">May&lt;/h4>
&lt;ul>
&lt;li>Accepted into a graduate program.&lt;/li>
&lt;/ul>
&lt;h4 id="apr">Apr&lt;/h4>
&lt;ul>
&lt;li>Currently looking for PhD Opportunity in the field of AI/ML. My research interest lies in Reinforcement Learning and Physics-Informed Machine Learning.&lt;/li>
&lt;/ul>
&lt;h3 id="2022">2022&lt;/h3>
&lt;h4 id="dec">Dec&lt;/h4>
&lt;ul>
&lt;li>I am 29. My undergrad was Economics at a local university, and I am going back to school so I can pursue higher education in the field of Machine Learning &amp;amp; Artificial Intelligence.&lt;/li>
&lt;/ul>
&lt;hr>
&lt;h2 id="resumes--portfolio">Resumes / Portfolio&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://iamtu.dev/pdf/01_cv.pdf">Academic Resume&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://iamtu.dev/pdf/interview-tudo.pdf">Research portfolio&lt;/a>&lt;/li>
&lt;li>Professional Resume&lt;/li>
&lt;/ul>
&lt;h2 id="researchs--publications">Researchs / Publications&lt;/h2>
&lt;h3 id="accepted">Accepted&lt;/h3>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>(BME 2020)&lt;/strong> Tu, Do Thanh, Thuong Nguyen, Anh Tho Le, Sinh Nguyen, Huong Ha. &lt;em>&amp;ldquo;Automated EOG removal from EEG signal using Independent Component Analysis and Machine Learning Algorithms&amp;rdquo;&lt;/em> at The 8th International Conference in Vietnam on the Development of Biomedical Engineering. (&lt;a href="">link&lt;/a>)&lt;/p></description></item><item><title>Learning physics-informed Neural Networks (PINN)</title><link>https://iamtu.dev/posts/pinn/</link><pubDate>Thu, 09 May 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/pinn/</guid><description>&lt;p>My goal is to familiarize myself with physics-informed neural network(PINN), consist of &amp;hellip; Having no formal training on solving PDE, the first section will outline basic concepts in solving partial differential equations (PDEs), an example of a PDE, and its closed-form solution.&lt;/p>
&lt;p>Outline:&lt;/p>
&lt;ul>
&lt;li>PDE&lt;/li>
&lt;li>Closed-formed solution&lt;/li>
&lt;li>Generating data&lt;/li>
&lt;li>PINN&lt;/li>
&lt;/ul>
&lt;h2 id="pde-and-the-legendre-differential-equation">PDE and the Legendre Differential Equation&lt;/h2>
&lt;p>This section is written with instruction from &lt;a href="https://web.math.ucsb.edu/~moore/pde.pdf">this introduction&lt;/a> by JD Moore.&lt;/p>
&lt;!-- raw HTML omitted -->
&lt;!-- raw HTML omitted -->
&lt;p>&lt;strong>Definition&lt;/strong> (Power series)&lt;/p></description></item><item><title>Expectation Maximization - EM</title><link>https://iamtu.dev/posts/em/</link><pubDate>Wed, 08 May 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/em/</guid><description>&lt;h2 id="problem">Problem&lt;/h2>
&lt;p>Given a statistical model \(P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})\), which generate set of observations \(\boldsymbol{X}\), where \(\boldsymbol{Z}\) is a latent variable and unknow parameter vector \(\boldsymbol{\theta}\). The goal is to find \(\boldsymbol{\theta}\) that maximize the marginal likelihood:&lt;/p>
&lt;p>$$
\mathcal{L}(\boldsymbol{\theta}; \boldsymbol{X}) = P(\boldsymbol{X} | \boldsymbol{\theta})
= \int_{\boldsymbol{Z}}P(\boldsymbol{X}, \boldsymbol{Z} | \boldsymbol{\theta})d\boldsymbol{Z}
$$&lt;/p>
&lt;p>As an example for this type of problem, there are two (unfair) coin A and B with probability of head for each coin is \(p_A(H) = p \text{ and } p_B(H) = q\). For each trial, we select coin A with probability \(p(A) = \tau\) and coin B with probability \(p(B) = 1 -\tau\), toss the coin and record the observation. The set of observations \(\boldsymbol{X}\) is the record of head or tail \(\{H, T, H, H, \cdots\}\), the latent variable which is unobserved is which coint is selected for each trail \(\{A, B, B, A, \cdots\}\), and the unknown parameter vector \(\boldsymbol{\theta} = [p, q, \tau]\). The goal is to find \(\boldsymbol{\theta}\) that best fit observations; EM is an instance of Maximum Likelihood Estimation (MLE).&lt;/p></description></item><item><title>Implementing Automatic Differentiation</title><link>https://iamtu.dev/posts/auto-grad/</link><pubDate>Mon, 15 Apr 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/auto-grad/</guid><description/></item><item><title>Understanding Variational Inference</title><link>https://iamtu.dev/posts/variational_inference/</link><pubDate>Thu, 29 Feb 2024 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/variational_inference/</guid><description>&lt;p>This post is a note I take from while reading &lt;a href="">Blei et al 2018&lt;/a>.&lt;/p>
&lt;p>Goal:&lt;/p>
&lt;ul>
&lt;li>Motivation of variational inference&lt;/li>
&lt;li>Understand the derivation of ELBO and its intiution&lt;/li>
&lt;li>Walk through the derivation, some of which was skip the in original paper&lt;/li>
&lt;li>Implementation of CAVI&lt;/li>
&lt;/ul>
&lt;h2 id="elbo">ELBO&lt;/h2>
&lt;p>Goal is to find \(q(z)\) to approximate \(p(z|x)\)&lt;/p>
&lt;p>The KL-divergence&lt;/p>
&lt;p>$$
\begin{equation}
\begin{aligned}
KL[q(z)||p(z | x)] &amp;amp;=
\int_z{q(z)\log{\frac{p(z|x)}{q(z)}} dz}
\end{aligned}
\end{equation}
$$&lt;/p>
&lt;p>However, this quantity is intractable to compute hence, we&amp;rsquo;re unable to optimize this quantity directly.&lt;/p></description></item><item><title>Differentiation under integral sign</title><link>https://iamtu.dev/posts/diff-under-integral-sign/</link><pubDate>Thu, 16 Nov 2023 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/diff-under-integral-sign/</guid><description>&lt;h2 id="motivating-example">Motivating example&lt;/h2>
&lt;p>Evaluating following integral&lt;/p>
&lt;p>$$
I = \int_0^1{\frac{1 - x^2}{\ln{x}}dx}
$$&lt;/p>
&lt;h3 id="closed-form-results">Closed-form results&lt;/h3>
&lt;p>$$
\begin{equation}
\begin{aligned}
F(t) &amp;amp;= \int_0^1{\frac{1-x^t}{\ln(x)}dx} \\
\implies \frac{d}{dt}F &amp;amp;= \frac{d}{dt}\int_0^1{\frac{1-x^t}{\ln(x)}dx}\\
&amp;amp;= \int_0^1{
\frac{\partial}{\partial t}
\frac{1-x^t}{\ln(x)}dx
}\\
&amp;amp;= \int_0^1{
\frac{-\ln(x)x^t}{ln(x)}
dx} \\
&amp;amp;= \bigg[-\frac{x^{t+1}}{t+1}\bigg]_0^1\\
&amp;amp;= -\frac{1}{t+1}\\
\implies F(t) &amp;amp;= -\ln({t+1}) \\
\implies I &amp;amp;= f(2) = -\ln3
\end{aligned}
\end{equation}
$$&lt;/p>
&lt;h3 id="numerical-approximation">Numerical approximation&lt;/h3>
&lt;p>&lt;img loading="lazy" src="https://iamtu.dev/images/mcmc_integral.png" alt="image" />
&lt;/p>
&lt;p>&lt;details >
&lt;summary markdown="span">Code to produce the figure&lt;/summary>
&lt;div class="highlight">&lt;div class="chroma">
&lt;table class="lntable">&lt;tr>&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code>&lt;span class="lnt">1
&lt;/span>&lt;span class="lnt">2
&lt;/span>&lt;span class="lnt">3
&lt;/span>&lt;span class="lnt">4
&lt;/span>&lt;span class="lnt">5
&lt;/span>&lt;span class="lnt">6
&lt;/span>&lt;span class="lnt">7
&lt;/span>&lt;span class="lnt">8
&lt;/span>&lt;/code>&lt;/pre>&lt;/td>
&lt;td class="lntd">
&lt;pre tabindex="0" class="chroma">&lt;code class="language-python" data-lang="python">&lt;span class="line">&lt;span class="cl">&lt;span class="kn">import&lt;/span> &lt;span class="nn">numpy&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="nn">np&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="kn">from&lt;/span> &lt;span class="nn">matplotlib&lt;/span> &lt;span class="kn">import&lt;/span> &lt;span class="n">pyplot&lt;/span> &lt;span class="k">as&lt;/span> &lt;span class="n">plt&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl">&lt;span class="k">def&lt;/span> &lt;span class="nf">I&lt;/span>&lt;span class="p">():&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">g&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="k">lambda&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="p">:&lt;/span> &lt;span class="p">(&lt;/span>&lt;span class="mi">1&lt;/span> &lt;span class="o">-&lt;/span> &lt;span class="n">x&lt;/span>&lt;span class="o">**&lt;/span>&lt;span class="mi">2&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">/&lt;/span>&lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">log&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">vG&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">vectorize&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">g&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="n">x&lt;/span> &lt;span class="o">=&lt;/span> &lt;span class="n">np&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">random&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">uniform&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="mi">0&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">1&lt;/span>&lt;span class="p">,&lt;/span> &lt;span class="mi">10000&lt;/span>&lt;span class="p">)&lt;/span>
&lt;/span>&lt;/span>&lt;span class="line">&lt;span class="cl"> &lt;span class="k">return&lt;/span> &lt;span class="n">vG&lt;/span>&lt;span class="p">(&lt;/span>&lt;span class="n">x&lt;/span>&lt;span class="p">)&lt;/span>&lt;span class="o">.&lt;/span>&lt;span class="n">mean&lt;/span>&lt;span class="p">()&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/td>&lt;/tr>&lt;/table>
&lt;/div>
&lt;/div>
&lt;/details>&lt;/p></description></item><item><title>Deriving closed-form Kullback-Leibler divergence for Gaussian Distribution</title><link>https://iamtu.dev/posts/closed-form-kl-gaussian/</link><pubDate>Sun, 12 Nov 2023 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/closed-form-kl-gaussian/</guid><description>&lt;p>The closed form of KL divergence used in Variational Auto Encoder.&lt;/p>
&lt;h2 id="univariate-case">Univariate case&lt;/h2>
&lt;p>Let&lt;/p>
&lt;ul>
&lt;li>\(p(x) = \mathcal{N}(\mu_1, \sigma_1) = (2\pi\sigma_1^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_1^2}(x-\mu_1)^2]\)&lt;/li>
&lt;li>\(q(x) = \mathcal{N}(\mu_1, \sigma_2) = (2\pi\sigma_2^2)^{-\frac{1}{2}}\exp[-\frac{1}{2\sigma_2^2}(x-\mu_2)^2]\)&lt;/li>
&lt;/ul>
&lt;p>KL divergence between \(p\) and \(q\) is defined as:&lt;/p>
&lt;p>$$
\begin{aligned}
\text{KL}(p\parallel q) &amp;amp;= -\int_{x}{p(x)\log{\frac{q(x)}{p(x)}}dx} \\
&amp;amp;= -\int_x p(x) [\log{q(x)} - \log{p(x)}]dx \\
&amp;amp;=
\underbrace{
\int_x{p(x)\log p(x) dx}}_A
- \underbrace{
\int_x{p(x)\log q(x) dx}}_B
\end{aligned}
$$&lt;/p>
&lt;p>First quantity \(A\):&lt;/p>
&lt;p>$$
\begin{aligned}
A &amp;amp;= \int_x{p(x)\log p(x) dx} \\
&amp;amp;= \int_x{p(x)\big[
-\frac{1}{2}\log{2\pi\sigma_1^2
- \frac{1}{2\sigma_1^2}(x - \mu_1)^2}
\big]dx}\\
&amp;amp;= -\frac{1}{2}\log{2\pi\sigma_1^2}\int_x{p(x)dx}
- \frac{1}{2\sigma_1^2}
\underbrace{\int_x{p(x)(x-\mu_1)^2dx}}_{\text{var(x)}}\\
&amp;amp;= -\frac{1}{2}\log{2\pi} - \log\sigma_1-\frac{1}{2}
\end{aligned}
$$&lt;/p></description></item><item><title>Likelihood-free MCMC with Amortized Ratio Estimator</title><link>https://iamtu.dev/posts/sbi/</link><pubDate>Tue, 31 Oct 2023 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/sbi/</guid><description>&lt;h2 id="simulation-based-inference">Simulation Based Inference&lt;/h2>
&lt;p>Imagine we have some black-box machine; such a machine has some knobs and levels so we can change its inner configurations. The machine churns out some data for each configuration. The Simulation-based inference (SBI) solves the inverse problem that is given some data, estimating the configuration (Frequentist approach) or sampling the configuration from the posterior distribution (for Bayesian approach). For a formal definition and review of current methods for SBI, see &lt;a href="https://www.pnas.org/doi/full/10.1073/pnas.1912789117">this paper&lt;/a>. In the analogy above, the black box represents the simulator, and the configurations are the simulatorâ€™s parameters.&lt;/p></description></item><item><title>Miscellanous</title><link>https://iamtu.dev/posts/miscellanous/</link><pubDate>Thu, 28 Sep 2023 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/miscellanous/</guid><description>&lt;h2 id="convergence-in-probability">Convergence in probability&lt;/h2>
&lt;p>&lt;strong>Definition&lt;/strong> a sequence $\{X_n\}$ of random variable converges in probability towards the random variable $X$ if for all $\epsilon &amp;gt; 0$&lt;/p>
&lt;p>$$
\lim_{n \leftarrow \infty} \mathbb{P}(|X_n - X| &amp;gt; \epsilon) = 0
$$&lt;/p>
&lt;h2 id="consistent-estimator">Consistent estimator&lt;/h2>
&lt;h2 id="a-statistic-singular">A statistic (singular)&lt;/h2>
&lt;p>is any quantity computed from values in a sample which is considered fro a statistical purpose (estimating population parameter, describing sample, evaluating hypothesis).&lt;/p>
&lt;h2 id="sufficient-statistics">Sufficient statistics&lt;/h2>
&lt;p>A statistic is sufficient with respect to a statisticcal model and its associated unknown parameter if no other statistic can be&lt;/p></description></item><item><title>Noise constrastive estimation</title><link>https://iamtu.dev/posts/noise-contrastive-estimation/</link><pubDate>Sat, 23 Sep 2023 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/noise-contrastive-estimation/</guid><description>&lt;h2 id="tldr">TLDR&lt;/h2>
&lt;p>The &lt;a href="https://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf">paper&lt;/a> proposed a method to estimate the probability density function of a dataset by discriminating observed data and noise drawn from a distribution. The paper setups the problem into a dataset of \(T\) observations \((x_1, &amp;hellip; x_T)\) drawn from a true distribution \(p_d(.)\). We then try to approximate \(p_d\) by a parameterized function \(p_m(.;\theta)\). The estimator \(\hat{\theta}_T\) is defined to be the \(\theta\) that maximize function&lt;/p>
&lt;p>$$
J_T(\theta) = \frac{1}{2T}\sum_t{\log[h(x_t; 0)]} + \log[1-h(y_t; \theta)]
$$&lt;/p></description></item><item><title>Real Analysis - Lecture notes</title><link>https://iamtu.dev/posts/real-analysis/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://iamtu.dev/posts/real-analysis/</guid><description>&lt;p>Notes I took during studying MIT OCW &lt;a href="https://ocw.mit.edu/courses/18-100a-real-analysis-fall-2020/">Real Analysis&lt;/a>. The class taught by Professor Casey Rodriguez, he also taught &lt;a href="https://tarheels.live/crodriguez/168-2/">Functional analysis&lt;/a>.&lt;/p>
&lt;h2 id="resources-useful-link">Resources (Useful link)&lt;/h2>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=LY7YmuDbuW0&amp;amp;list=PLUl4u3cNGP61O7HkcF7UImpM0cR_L2gSw">Video lecture&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://ocw.mit.edu/courses/18-100a-real-analysis-fall-2020/">Course&amp;rsquo;s homepage&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="lecture-notes">Lecture notes&lt;/h2>
&lt;p>Goal of the course
- Gain experience with proofs
- Prove statements about the real numbers, function and limits&lt;/p>
&lt;h3 id="lecture-1-sets-set-operations-and-mathematical-induction">Lecture 1: Sets, Set operations, and Mathematical Induction&lt;/h3>
&lt;p>&lt;strong>Definition&lt;/strong> (Sets) A sets is a collection of objects called elements/members.&lt;/p>
&lt;p>&lt;strong>Definition&lt;/strong> (Empty set) A set with no elements, denoted as \(\emptyset\)&lt;/p></description></item></channel></rss>